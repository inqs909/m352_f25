{
  "hash": "7b8a96325b93ca4573f04ad7c555b69e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 3: Linear Regression\"\neditor: source\ndate: 5-7-2022\ndraft: true\n---\n\n\n\n## Introduction\n\nThis tutorial focuses on simple linear regression. This tutorial will focus on different methods in conducting a simple linear regression in R. **You only need to do section 1 and 2 to learn how to do SLR in R.**\n\nFor this tutorial, we will use the `mtcars` data set and comparing the the `mpg` (dependent) variable with the `hp` (independent) variable.\n\nThrough out this tutorial, we use certain notations for different components in R. To begin, when something is in a gray block, `_`, this indicates that R code is being used. When I am talking about an R Object, it will be displayed as a word. For example, we will be using the R object `mtcars`. When I am talking about an R function, it will be displayed as a word followed by an open and close parentheses. For example, we will use the mean function denoted as `mean()` (read this as \"mean function\"). When I am talking about an R argument for a function, it will be displayed as a word following by an equal sign. For example, we will use the data argument denoted as `data=` (read this as \"data argument\"). When I am referencing an R package, I will use `::` (two colons) after the name. For example, in this tutorial, I will use the `ggplot2::` (read this as \"ggplot2 package\") Lastly, if I am displaying R code for your reference or to run, it will be displayed on its own line. There are many components in R, and my hope is that this will help you understand what components am I talking about.\n\n### Contents\n\n1.  Linear Relationship\n\n2.  SLR via `lm()`\n\n3.  Multivariable Linear Regression\n\n4.  MLR via `lm()`\n\n5.  Questions\n\n## Linear Relationship\n\nWhen we are comparing two continuous variables, we are measuring the association between them. We want to know how does one variable change when the other variable increases or decreases. There are two methods to measure this association: a correlation or linear regression.\n\n### Correlation\n\nThe correlation coefficient is a statistic used to measure the association between 2 variables. The value range from -1 to 1. To compute the correlation coefficient, use the `cor()`. You will only need to specify the `x=` and `y=` which indicate the vectors of data to compute the correlation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(mtcars$mpg, mtcars$hp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.7761684\n```\n\n\n:::\n:::\n\n\n### Linear Regression\n\nWith linear regression, we are trying to fit a line such that all the points are closest to the line as possible. Below is the scatter plot of `hp` vs `mpg`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars, aes(hp, mpg)) + geom_point() + theme_bw()\n```\n\n::: {.cell-output-display}\n![](r_lab_3_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nAs you can see, there is a negative association. As `hp` goes up, `mpg` goes down. What we want to do is fit a line where all the points are closest to the line. The next four plots demonstrates how a line is fitted. Linear regression searches for a line until the distance between all the points and the line are smallest possible. The fourth plot with the blue line indicates the best fit line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars, aes(hp, mpg)) + geom_point() + theme_bw() +\n  geom_function(fun=yy, args = list(m = -.205, b = 50)) \n```\n\n::: {.cell-output-display}\n![](r_lab_3_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nNotice how the points at the end of the plot are further away from the line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars, aes(hp, mpg)) + geom_point() + theme_bw() +\n  geom_function(fun=yy, args = list(m = -.013, b = 20))\n```\n\n::: {.cell-output-display}\n![](r_lab_3_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThis is much better, but the points at the beginning are further away.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars, aes(hp, mpg)) + geom_point() + theme_bw() +\n  geom_function(fun=yy, args = list(m = -.10, b = 35)) \n```\n\n::: {.cell-output-display}\n![](r_lab_3_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThis plot provides a better fit of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars, aes(hp, mpg)) + geom_point() + theme_bw() +\n  geom_smooth(method = \"lm\", se = F)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](r_lab_3_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis contains the fitted line from the linear regression.\n\nMathematically, we are try trying to find the values for an equation of a line that fits the data. The equation of the line for the $i^{th}$ point is\n\n$Y_i= \\beta_0 + \\beta_1 X_i + \\varepsilon_i,$\n\nwhere $Y_i$ is the $i^{th}$ outcome, $X_i$ is the $i^{th}$ predictor, $\\epsilon_i$ is the $i^{th}$ error term, $\\beta_0$ is the intercept, and $\\beta_1$ is the slope. The error term is the distance between the line and the point. Therefore, we want to minimize the error term:\n\n$\\varepsilon_i = Y_i-\\beta_0 -\\beta_1 X_i$\n\nBecause some of the error terms are negative (points are below the line), we will need to square it. Lastly, we want to minimize all the points, so we add all observations together and minimize the following least-squares formula:\n\n$\\sum_{i=1}^n(Y_i-\\beta_0 -\\beta_1 X_i)^2$\n\nWe will need to find the values of $\\beta_0$ and $\\beta_1$ that minimizes the least-squares formula. We can use the Ordinary least-squares (OLS) or the Maximum Likelihood Estimation (MLE) estimators to obtain the these estimates.\n\n## SLR via `lm()`\n\n### Fitting the model\n\nFitting the model means finding the estimates of $\\beta_0$ and $\\beta_1$ that minimizes the least-squares formula, which is denoted with hat: $\\hat \\beta_0$ and $\\hat \\beta_1$. In R, we can obtain these estimates using the `lm()`. The `lm()` function only needs the `formula=` and the `data=` as inputs. The output of the `lm()` is an `lm` R class that contains information about the model.\n\nTo conduct a linear regression where `mpg` is the dependent variable and `hp` is the independent variable. Then, we will store the output from the `lm()` in the R object `xlm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxlm <- lm(mpg~hp, data = mtcars)\n```\n:::\n\n\nTo view the summary table of coefficients from the `lm()`, you will need to use the `summary()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(xlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,\tAdjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(xlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,\tAdjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n```\n\n\n:::\n:::\n\n\nThe results from the `summary()` provide statistics on the residuals, a coefficient table with corresponding hypothesis test, and other relevant statistics for model fit.\n\n### Hypothesis Testing\n\nFocusing on the coefficient table, it provides information for each variable (labeled in the rows section). For each regression coefficient, the table provides an \"Estimate\", \"Std. Error\", \"t value\", and \"Pr(\\>\\|t\\|)\". The p-value corresponds to the following hypothesis test:\n\n$H_0: \\beta = 0$,\n\nand\n\n$H_1: \\beta \\neq 0$.\n\n## Multivariable Linear Regression\n\nSimple linear regression uses a model with one independent variable:\n\n$$\nY = \\beta_0 + \\beta_1 X_1.\n$$\n\nThe multivariable linear regression extends the model to include more than independent variable:\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\n$$\n\nand in matrix form:\n\n$$\nY = \\boldsymbol X ^ \\mathrm T \\boldsymbol \\beta,\n$$\n\nwhere\n\n$$\n\\boldsymbol \\beta = \\left(\n\\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{array}\n\\right)\n$$\n\nand\n\n$$\n\\boldsymbol X = \\left(\\begin{array}{cc}\n1 & X_{11} & X_{21} & \\cdots & X_{p1}\\\\\n1 & X_{12} & X_{23} & \\cdots & X_{p2}\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & X_{1n} & X_{2n} & \\cdots & X_{pn}\\\\\n\\end{array}\n\\right).\n$$\n\nTo fit the data to the model, we search for values of $\\hat \\beta$ that minimizes the least-squares formula:\n\n$$\n\\sum^n_{i=1} \\left( Y_i - \\boldsymbol X_i ^ \\mathrm T \\boldsymbol \\beta \\right)^2\n$$\n\nFor the rest of the tutorial, we will go over how to find the estimates of $\\beta$\n\n## MLR with `lm()`\n\nFirst, we are interested in fitting the model below:\n\n$$\nmpg = \\beta_0 + \\beta_1 \\cdot hp +\\beta_2 \\cdot cyl + \\beta_3 \\cdot wt + \\beta_4 \\cdot disp.\n$$\n\nThe outcome variable is `mpg`, and the independent variables are `hp`, `cyl`, `wt`, and `disp`. Fitting a multivariable linear regression in R is done with the `lm()`. We just need to specify the `formula=` and the `data=`. Similar to the simple linear regression model, the `formula=` contains the outcome on side of the `~` and the independent variables are on the other side. The independent variable are added to each other using the `+` operator. For example, the `formula=` is coded as\n\n```         \nmpg ~ hp + cyl + wt + disp\n```\n\nUsing the formula above, fit a model with the outcome variable is `mpg` and the independent variables are `hp`, `cyl`, `wt`, and `disp` from the `mtcars` data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxlm <- lm(mpg ~ hp + cyl + wt + disp, data = mtcars)\n```\n:::\n\n\nLastly, we can obtain organized output using the `summary()`. Obtain the summary for `xlm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(xlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp + cyl + wt + disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0562 -1.4636 -0.4281  1.2854  5.8269 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 40.82854    2.75747  14.807 1.76e-14 ***\nhp          -0.02054    0.01215  -1.691 0.102379    \ncyl         -1.29332    0.65588  -1.972 0.058947 .  \nwt          -3.85390    1.01547  -3.795 0.000759 ***\ndisp         0.01160    0.01173   0.989 0.331386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.513 on 27 degrees of freedom\nMultiple R-squared:  0.8486,\tAdjusted R-squared:  0.8262 \nF-statistic: 37.84 on 4 and 27 DF,  p-value: 1.061e-10\n```\n\n\n:::\n:::\n\n\n## MLR Statistics\n\n### $\\widehat{cov}(\\hat \\beta)$\n\nThe estimated covariance matrix of $\\hat \\beta$ can be obtained with the `vcov()` on an R object containing the output from the `lm()`. To obtain the covariance matrix of $\\hat \\beta$ from `xlm`, use the `vcov()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvcov(xlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             (Intercept)            hp          cyl           wt          disp\n(Intercept)  7.603629375  2.005298e-03 -1.238326838 -1.777628876  2.462149e-02\nhp           0.002005298  1.475440e-04 -0.003531738  0.001567700 -2.964271e-05\ncyl         -1.238326838 -3.531738e-03  0.430174318  0.017632767 -4.169713e-03\nwt          -1.777628876  1.567700e-03  0.017632767  1.031186716 -8.144097e-03\ndisp         0.024621488 -2.964271e-05 -0.004169713 -0.008144097  1.375181e-04\n```\n\n\n:::\n:::\n\n\n### Hypothesis Testing\n\nFocusing on the coefficient table from the `summary()`, it provides information for each variable (labeled in the rows section). For each regression coefficient, the table provides an \"Estimate\", \"Std. Error\", \"t value\", and \"Pr(\\>\\|t\\|)\". The p-value corresponds to the following hypothesis test:\n\n$H_0: \\beta_j = 0$,\n\nand\n\n$H_1: \\beta_j \\neq 0$.\n\n## Questions\n\n1.  The `faithful` data set in R contains information on `eruptions` time and `waiting` time. Describe the relationship between `waiting` (independent) and `eruptions` (dependent).\n\n2.  The `cats` data set from the `MASS` package contains information of cat body weight (kg; `Bwt`) and heart weight (g; `Hwt`). Fit a model to see if there is a significant association between body weight (predictor) and heart weight (outcome).\n\n**Due May 14 \\@ 11:59 PM**\n",
    "supporting": [
      "r_lab_3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}