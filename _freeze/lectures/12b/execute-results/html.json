{
  "hash": "9ee7b94ab04c780f8d2e96939685897a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"GLM\"\nsubtitle: \"Generalized Linear Models\"\nformat:\n  revealjs:\n    width: 1200\n    scrollable: true\n    theme: [default, styles.scss]\n    controls-tutorial: true\n    navigation-mode: vertical\n    incremental: false \n    touch: false\n    controls: true\n    pointer:\n      pointerSize: 32\n    slide-number: true\n    sc-sb-title: h1\n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: false\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \n\neditor: source\n\nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n  \nauto-agenda:\n  bullets: numbered\n  clickable: true\n---\n\n## Learning Outcomes\n\n-   Exponential Family of Distributions\n\n-   Generalized Linear Models\n\n-   Regression Models\n\n# Exponential Family of Distributions\n\n## Exponential Family of Distributions\n\nAn exponential family of distributions are random variables that allow their probability density function to have the following form:\n\n$$\nf(x; \\theta,\\phi) = a(x,\\phi)\\exp\\left\\{\\frac{x\\theta-\\kappa(\\theta)}{\\phi}\\right\\}\n$$\n\n-   $\\theta$: is the canonical parameter (also a function of other parameters)\n\n-   $\\kappa(\\theta)$: is a known cumulant function\n\n-   $\\phi>0$: dispersion parameter function\n\n-   $a(y,\\phi)$: normalizing constant\n\n## Canonical Parameter\n\nThe canonical parameter represents the relationship between the random variable and the $E(Y)=\\mu$\n\n## Normal Distribution\n\n$$\nf(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n\n::: fragmet\n$$\nf(x;\\mu,\\sigma^2)= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{\\frac{x\\mu-\\mu^2/2}{\\sigma^2}-\\frac{x^2}{2\\sigma^2}\\right\\}\n$$\n:::\n\n## Binomial Distribution\n\n$$\nf(x;n,p) = \\left(\\begin{array}{c}n\\\\x\\end{array}\\right) p^x(1-p)^{n-p}\n$$\n\n::: fragment\n$$\nf(x;n,p) = \\left(\\begin{array}{c}n\\\\x\\end{array}\\right) \\exp\\left\\{x\\log\\left(\\frac{p}{1-p}\\right) + n \\log(1-p)\\right\\}\n$$\n:::\n\n## Distributions and Canonical Parameters\n\n| Random Variable   | Canonical Parameter                  |\n|-------------------|--------------------------------------|\n| Normal            | $\\mu$                                |\n| Binomial          | $\\log\\left(\\frac{\\mu}{1-\\mu}\\right)$ |\n| Negative Binomial | $\\log\\left(\\frac{\\mu}{\\mu+\\phi^{-1}}\\right)$ |\n| Poisson           | $\\log(\\mu)$                          |\n| Gamma             | $-\\frac{1}{\\mu}$                     |\n| Inverse Gaussian  | $-\\frac{1}{2\\mu^2}$                  |\n\n# Generalized Linear Models\n\n## Generalized Linear Models\n\nA generalized linear model (GLM) is used to model the association between an outcome variable (of any data type) and a set of predictor values. We estimate a set of regression coefficients $\\boldsymbol \\beta$ to explain how each predictor is related to the expected value of the outcome.\n\n## Generalized Linear Models\n\nA GLM is composed of a systematic and random component.\n\n## Random Component\n\nThe random component is the random variable that defines the randomness and variation of the outcome variable.\n\n## Systematic Component\n\nThe systematic component is the linear model that models the association between a set of predictors and the expected value of Y:\n\n$$\ng(\\mu_i)=\\eta_i=\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta\n$$\n\n-   $\\boldsymbol\\beta$: regression coefficients\n\n-   $\\boldsymbol X_i=(1, X_{i1}, \\ldots, X_{ik})^\\mathrm T$: design vector\n\n-   $\\eta$: linear model\n\n-   $\\mu_i=E(Y_i)$\n\n-   $g(\\cdot)$: link function\n\n# Regression Models\n\n## Logistic Regression\n\n- **Logistic Regression** models the probability that a binary outcome equals 1.\n- The model assumes a linear relationship between predictors and the log-odds of the outcome.\n- To map probabilities (0â€“1) to the real line, we use a link function. Usually the logit function.\n- Utilizes a Bernoulli Model\n\n## The logit Function\n\n$$\ng(p_i) = \\eta_i = X_i^\\mathrm T \\boldsymbol \\beta\n$$\n\n$$\ng(p_i) = \\log\\left(\\frac{p_i}{1 - p_i}\\right)\n$$\n\n$$\np_i = g^{-1}(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\n$$\n\n\n## The logit Function\n\n- The logit link ensures predicted probabilities stay between 0 and 1.\n\n- Small changes in predictors can have nonlinear effects on probability.\n\n\n## Likelihood Function\n\n$$\nL(\\boldsymbol \\beta) = \\prod^n_{i=1} p_i^{Y_i}(1-p_i)^{1-Y_i}\n$$\n\n$$\nL(\\boldsymbol \\beta) = \\prod^n_{i=1} \\left(\\frac{1}{1 + e^{-\\eta_i}}\\right)^{Y_i}\\left\\{1-\\frac{1}{1 + e^{-\\eta_i}}\\right\\}^{1-Y_i}\n$$\n\n$$\n\\eta_i = X_i^\\mathrm T \\boldsymbol \\beta\n$$\n\n\n## Poisson Regression\n\n- **Poisson regression** models are used for modeling **count data** (e.g., number of events per unit time or space).  \n- It assumes that the response variable $Y$ follows a **Poisson distribution**.\n- It is recommended not to use the regression model since the assumption is that $E(Y)=Var(y)$, which is unrealistic.\n    - It is recommended to use a negative binomial regression instead.\n\n## The Log Link Function\n\nFor the Poisson model, the canonical link is the log link:\n\n$$\ng(\\lambda_i) = \\log(\\lambda_i) = \\eta_i\n$$\n\nThis ensures that the predicted mean $\\lambda_i$ is always **positive**, since:\n\n$$\n\\lambda_i = e^{\\eta_i}\n$$\n\n## Likelihood Function\n\n$$\nL(\\boldsymbol \\beta) = \\prod^n_{i=1} \\frac{e^{-\\lambda_i}(\\lambda_i)^{Y_i}}{Y_i!}\n$$\n\n$$\nL(\\boldsymbol \\beta) = \\prod^n_{i=1} \\frac{e^{-\\exp(\\eta_i)}(\\exp(\\eta_i))^{Y_i}}{Y_i!}\n$$\n\n$$\n\\eta_i = X_i^\\mathrm T \\boldsymbol \\beta\n$$\n\n## Negative Binomial Regression\n\n- **Negative Binomial Regression ** is used for **overdispersed count data**,  \n  where the variance exceeds the mean.\n\n$$\n\\text{Var}(Y_i) > E[Y_i]\n$$\n\n- It generalizes Poisson regression by introducing a **dispersion parameter**.\n\n- In real data, we often observe **overdispersion** (variance > mean).  \n  This leads to:\n  - Underestimated standard errors  \n\n## Negative Binomial Model\n\nThe Negative Binomial can be derived as a **Poisson-Gamma mixture**:\n\n$$\nY_i \\mid \\lambda_i \\sim \\text{Poisson}(\\lambda_i), \\quad\n\\lambda_i \\sim \\text{Gamma}(\\mu_i, \\phi)\n$$\n\nResulting in:\n\n$$\nE[Y_i] = \\mu_i, \\quad\n\\text{Var}(Y_i) = \\mu_i + \\phi\\mu_i^2\n$$\n\nwhere $\\phi$ controls overdispersion.\n\n## Negative Binomial Model\n\n$$\nf(y) = \\frac{\\Gamma(y + \\phi^{-1})}{\\Gamma(\\phi^{-1})\\Gamma(y + 1)} \\left( \\frac{\\mu}{\\mu + \\phi^{-1}} \\right)^y\\left(1- \\frac{\\mu}{\\mu + \\phi^{-1}} \\right)^{\\phi^{-1}}\n$$\n\n## The log link function\n\nJust like in Poisson regression, we use the **log link function**:\n\n$$\ng(\\mu_i) = \\log(\\mu_i) = \\eta_i = X_i^\\mathrm T \\boldsymbol \\beta\n$$\n\nand the inverse link:\n\n$$\n\\mu_i = e^{\\eta_i}\n$$\n\n:::: fragment\n::: callout-note\n\nUsing the log-link function is different from our canonical parameter $\\log\\left(\\frac{\\mu}{\\mu + \\phi^{-1}}\\right)$. This leads to more interpretable results.\n\n:::\n::::\n\n## Likelihood Function\n\n$$\nL(\\boldsymbol \\beta, \\phi) = \\prod^n_{i=1} \\frac{\\Gamma(y_i + \\phi^{-1})}{\\Gamma(\\phi^{-1})\\Gamma(y_i + 1)} \\left( \\frac{\\mu_i}{\\mu_i + \\phi^{-1}} \\right)^{y_i}\\left(1- \\frac{\\mu}{\\mu_i + \\phi^{-1}} \\right)^{\\phi^{-1}}\n$$\n\n$$\n\\mu_i = e^{\\eta_i}\n$$\n\n$$\n\\mu_i = e^{\\eta_i}\n$$\n\n$$\n\\eta_i = X_i^\\mathrm T \\boldsymbol \\beta\n$$\n\n## Gamma Regression\n\n- **Gamma regression** models positive continuous responses that are right-skewed.  \n- Examples:\n  - Waiting times\n  - Insurance claims\n  - Reaction times\n\n\n## Gamma Distribution\n\n$$\nf(y) = \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}y^{\\alpha-1}e^{-y/\\beta}\n$$\n\n::: fragment\n\nLet $\\alpha = 1/\\psi$ and $\\beta=\\mu\\psi$\n$$\nf(y) = \\frac{1}{\\Gamma(1/\\psi)}\\left(\\frac{1}{y}\\right)\\left(\\frac{y}{\\psi\\mu}\\right)^{1/\\psi}e^{-\\frac{y}{\\psi\\mu}}\n$$\n\n:::\n\n::: fragment\n$$\n\\text{E}(Y) = \\mu \\quad \\text{Var}(Y) = \\psi \\mu_i^2\n$$\n:::\n\n\n\n\n## The inverse-link function\n\nWe relate the mean to predictors through a canonical link function:\n\n$$\ng(\\mu_i) = \\eta_i = X_i^\\mathrm T \\boldsymbol \\beta\n$$\n\n$$\ng(\\mu_i) = \\frac{1}{\\mu_i}\n$$\n\n## The log-link function\n\nBut the **log link** is often used for interpretability as well maintian a positive expected value:\n\n$$\ng(\\mu) = \\log(\\mu)\n$$\n\n## Likelihood Function\n\n$$\nL(\\boldsymbol \\beta, \\psi) = \\prod^n_{i=1} \\frac{1}{\\Gamma(1/\\psi)}\\left(\\frac{1}{y_i}\\right)\\left(\\frac{y_i}{\\psi\\mu_i}\\right)^{1/\\psi}e^{-\\frac{y_i}{\\psi\\mu_i}}\n$$\n\n$$\n\\mu_i = \\exp(\\eta_i)\n$$\n\n$$\n\\eta_i X_i^\\mathrm T \\boldsymbol \\beta\n$$\n\n## Other regression Models\n\n:::: {.columns}\n::: {.column}\n- Binomial Models\n- Beta Models\n- Tweedie Models\n- Cox Models\n- Zero-Inflated Models\n    - Poisson\n    - Negative Binomial\n:::\n::: {.column}\n- Log-Normal Models\n- Beta-Binomial Models\n- Multinomial Models\n- Student Models\n- Hurdle Models\n    - Gamma\n    - Log-Normal\n:::\n::::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}