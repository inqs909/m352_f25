{
  "hash": "0300b7ade1cccf3bad46bb8928d9a885",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Joint Distribution Functions\"\nformat:\n  revealjs:\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      theme: whiteboard\n      chalk-width: 4\neditor: visual\n---\n\n\n\n## Learning Outcomes\n\n-   Joint Distributions\n\n-   Marginal Distributions\n\n-   Conditional Distributions\n\n-   Independence\n\n-   Expectations\n\n-   Covariance\n\n# Joint Distributions\n\n## Partial Derivatives\n\nFor a function $f(x,y)$, the partial derivative with respect to $x$ is taken by differentiating $f(x,y)$ with respect to $x$ while treating $y$ as a constant. For example:\n\n$f(x,y) = x^2 + \\ln(y)$\n\n## Multiple Integration\n\nMultiple integration is when you integrate a multivariate function by multiple variables. This is done by integrating the function by an individual variable at a time. For example:\n\n$f(x,y)=x^2 + y^2$ which can be integrated as:\n\n## Joint Distributions\n\nA joint distribution is a process where more than one random variable is generated; for example, collecting biomedical data, such as multiple biomarkers, are considered to follow a joint distribution. In mathematical terms, instead of dealing with a random variable, we are dealing with a random vector. Observing a particular random vector will have a probability attached to it.\n\n## Bivariate Discrete Distributions\n\nLet $X_1$ and $X_2$ be 2 discrete random variables, the joint distribution function of $(X_1, X_2)$ is defined as\n\n$$\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n$$\n\nThe properties of a bivariate discrete distribution are\n\n-   $p_{X_1,X_2}(x_1,x_2)\\ge 0$ for all $x_1,\\ x_2$\n\n-   $\\sum_{x_1}\\sum_{x2}p(x_1,x_2)=1$\n\n## Bivariate Continuous Distribution\n\nLet $X_1$ and $X_2$ be 2 continuous random variables, the joint distribution function of $(X_1, X_2)$ is defined as\n\n$$\nF_{X_1,X_2}(x_1, x_2) = P(X_1\\le x_1, X_2 \\le x_2).\n$$\n\nThe properties of a bivariate continuous distribution are\n\n-   $f_{X_1,X_2}(x_1,x_2)=\\frac{\\partial^2F(x_1,x_2)}{\\partial x_1\\partial x_2}$\n\n-   $f_{X_1,X_2}(x_1, x_2)\\ge 0$\n\n-   $\\int_{x_1}\\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2dx_1=1$\n\n## Example\n\n$$\nf(x,y) \\left\\{\\begin{array}{cc}\n3x & 0\\le y\\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n$$\n\nFind $P(0\\le X\\le 0.5,0.25\\le Y)$\n\n# Marginal Density Function\n\n## Marginal Density Functions\n\nA Marginal Density Function is density function of one random variable from a random vector.\n\n## Marginal Discrete Probability Mass Function\n\nLet $X_1$ and $X_2$ be 2 discrete random variables, with a joint distribution function of\n\n$$\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n$$\n\nThe marginal distribution of $X_1$ is defined as\n\n$$\np_{X_1}(x_1) = \\sum_{x_2}p_{X_1,X_2}(x_1,x_2)\n$$\n\n## Marginal Continuous Density Function\n\nLet $X_1$ and $X_2$ be 2 continuous random variables, with a joint density function of $f_{X_1,X_2}(x_1,x_2)$. The marginal distribution of $X_1$ is defined as\n\n$$\nf_{X_1}(x_1) = \\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2\n$$\n\n## Example\n\n$$\nf_{X,Y}(x,y) \\left\\{\\begin{array}{cc}\n2x & 0\\le y \\le 1;\\ 0 \\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n$$\n\nFind $f_X(x)$\n\n# Conditional Distributions\n\n## Conditional Distributions\n\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable.\n\n## Discrete Conditional Distributions\n\nLet $X_1$ and $X_2$ be 2 discrete random variables, with a joint distribution function of\n\n$$\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n$$\n\nThe conditional distribution of $X_1|X_2=x_2$ is defined as\n\n$$\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n$$\n\n## Continuous Conditional Distributions\n\nLet $X_1$ and $X_2$ be 2 continuous random variables, with a joint density function of $f_{X_1,X_2}(x_1,x_2)$. The conditional distribution of $X_1|X_2=_2$ is defined as\n\n$$\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n$$\n\n## Example\n\nLet the joint density function of $X_1$ and $X_2$ be defined as\n\n$$\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2Â² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n$$\n\nFind the conditional density function of $X_2|X_1=x_1$.\n\n# Independence\n\n## Independent Random Variables\n\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable.\n\n## Discrete Independent Random Variables\n\nLet $X_1$ and $X_2$ be 2 discrete random variables, with a joint density function of $p_{X_1,X_2}(x_1,x_2)$. $X_1$ is independent of $X_2$ if and only if\n\n$$\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n$$\n\n## Continuous Independent Random Variables\n\nLet $X_1$ and $X_2$ be 2 continuous random variables, with a joint density function of $f_{X_1,X_2}(x_1,x_2)$. $X_1$ is independent of $X_2$ if and only if\n\n$$\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n$$\n\n## Matrix Algebra\n\n$$\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n$$\n\n$$\n\\det(A) = a_1a_2\n$$\n\n$$\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n$$\n\n## Example\n\n$$\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n$$\n\nShow that $X\\perp Y$.\n\n$$\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n$$\n\nwhere $\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)$, $\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)$, and $\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)$\n\n# Expectations\n\n## Expectations\n\nLet $X_1, X_2, \\ldots,X_n$ be a set of random variables, the expectation of a function $g(X_1,\\ldots, X_n)$ is defined as\n\n$$\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n$$\n\nor\n\n$$\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n$$\n\n-   $\\boldsymbol X = (X_1,\\cdots, X_n)$\n\n## Expected Value and Variance of Linear Functions\n\nLet $X_1,\\ldots,X_n$ and $Y_1,\\ldots,Y_m$ be random variables with $E(X_i)=\\mu_i$ and $E(Y_j)=\\tau_j$. Furthermore, let $U = \\sum^n_{i=1}a_iX_i$ and $V=\\sum^m_{j=1}b_jY_j$ where $\\{a_i\\}^n_{i=1}$ and $\\{b_j\\}_{j=1}^m$ are constants. We have the following properties:\n\n-   $E(U)=\\sum_{i=1}^na_i\\mu_i$\n\n-   $Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i<j}{\\sum\\sum}a_ia_jCov(X_i,X_j)$\n\n-   $Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)$\n\n## Conditional Expectations\n\nLet $X_1$ and $X_2$ be two random variables, the conditional expectation of $g(X_1)$, given $X_2=x_2$, is defined as\n\n$$\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n$$\n\nor\n\n$$\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n$$\n\n## Conditional Expectations\n\nFurthermore,\n\n$$\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n$$\n\nand\n\n$$\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n$$\n\n# Covariance\n\n## Covariance\n\nLet $X_1$ and $X_2$ be 2 random variables with mean $E(X_1)=\\mu_1$ and $E(X_2)=\\mu_2$, respectively. The covariance of $X_1$ and $X_2$ is defined as\n\n$$\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n & =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n$$\n\nIf $X_1$ and $X_2$ are independent random variables, then\n\n$$\nCov(X_1,X_2)=0\n$$\n\n## Correlation\n\nThe correlation of $X_1$ and $X_2$ is defined as\n\n$$\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n$$\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}