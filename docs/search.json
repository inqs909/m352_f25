[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Fall 2025\nInstructor: Isaac Quintanilla Salinas\nEmail: isaac.qs@csuci.edu\nOffice Location: Marin Hall 2326\nWebsite: m352.inqs.info\nOffice Hours:\n\nTue/Thur 5-6 PM\nWed 2-4 PM\n\nOr by Zoom appointment.\nLecture:\n\nSec 01: T/TH 10:30-11:45 AM\nSec 02: T/TH 3-4:45 PM\n\nPre-Requisites: MATH 151"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nStatistics is the science of reasoning from data. It is both an exciting intellectual discipline and a powerful scientific tool. Statistics is a mathematical science, in the sense that it makes use of mathematics extensively, but it is not a branch of mathematics.\nThe practice of statistics involves collecting data, analyzing data, and drawing inferences from data. The mathematical foundations of statistical inference lie in probability, the study of randomness and uncertainty. This course introduces you to fundamental ideas and methods of probability and statistics."
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand and apply basic ideas and methods of probability, including conditional probability and random variables.\nApply and interpret the results of a variety of statistical techniques, including both descriptive and inferential methods;\nUnderstand many of the fundamental ideas of statistics, such as variability, distribution, sampling, confidence, and significance;\nUse statistical software to conduct simulations and analyze data;\nCommunicate your knowledge of probability and statistics effectively."
  },
  {
    "objectID": "syllabus.html#recommended-texts",
    "href": "syllabus.html#recommended-texts",
    "title": "Syllabus",
    "section": "Recommended Texts",
    "text": "Recommended Texts\nProbability for Data Science and Course Materials"
  },
  {
    "objectID": "syllabus.html#required-software",
    "href": "syllabus.html#required-software",
    "title": "Syllabus",
    "section": "Required Software",
    "text": "Required Software\nFor this course, we will use R, RMarkdown, and RStudio. Please download and install on your computer.\n\nR is a free statistical software program that is available for download at: https://www.r-project.org/.\nQuarto is a scientific documentation system used to write scientific documentation. Quarto is freely available at: https://quarto.org/\nRStudio provides free and open source tools for your data analysis in R: https://www.rstudio.com/"
  },
  {
    "objectID": "syllabus.html#course-grading",
    "href": "syllabus.html#course-grading",
    "title": "Syllabus",
    "section": "Course Grading",
    "text": "Course Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n20%\n\n\nR Assignments\n20%\n\n\nIn-Person Exam 1\n15%\n\n\nIn-Person Exam 2\n15%\n\n\nTake-Home Exam 1\n15%\n\n\nTake-Home Exam 2\n15%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\nHomework\nHomework will be assigned on a regular basis and posted on https://m352.inqs.info/hw.html and CANVAS. The homework is to help you practice the concepts learned in lecture and to help you study. You must turn in your own individual homework and show your understanding of the material. At the end of the semester, the two lowest homework grades will be dropped. Late work will be accepted, but with a 25% penalty. The last day late work will be accepted is on 12/5/2025 at 11:59 PM.\n\n\nR Labs\nThe objective of the labs are to develop both your statistical and programming skills. We will work on these labs in class. The lowest R Lab will be dropped.\n\n\nExams\nThere will be four exams this semester, two in-person exams and two take-home exams. While the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success. At the end of the semester, your lowest exam grade will be replaced by the median of all exam grades. Exam #2 (final exam) will be on December 11, 2025 from 8-10 AM for Sec 01 and December 11, 2025 from 1-3 PM for Sec 02.\nThis course will operate under a zero-tolerance policy. Talking during the time of the exam, sharing materials, looking at another students’ exam, or not following directions given will be subject to the University’s academic integrity policy.\n\n\nExtra Credit\nThere will be 4 extra credit opportunities worth a total of 10% of your overall grade. There are no make-ups for missed extra credit assignments!"
  },
  {
    "objectID": "syllabus.html#class-schedule",
    "href": "syllabus.html#class-schedule",
    "title": "Syllabus",
    "section": "Class Schedule",
    "text": "Class Schedule\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nAssignments/Exams\nReading\n\n\n\n\n8/25\nIntro to Course/Intro to R\n\nHandouts\n\n\n9/1\nIntroduction to Probability\nHW #1\n2.1 - 2.4\n\n\n9/8\nDiscrete Random Variables\nHW #2\n3.1 - 3.5\n\n\n9/15\nDiscrete Random Variables\nTake-Home Exam #1\n4.1 - 4.3, 4.5, 4.6\n\n\n9/22\nContinuous Random Variables\nHW #3\n5.1 - 5.4\n\n\n9/29\nContinuous Random Variables\n\n4.7, 5.5, 5.6, 6.1\n\n\n10/6\nJoint Distributions\n\n5.1 - 5.4\n\n\n10/13\nExam # 1/Sampling Statistics\n\n\n\n\n10/20\nSampling Statistics\n\n6.3 - 6.4\n\n\n10/27\nMaximum Likelihood Estimation\nHW #4\n8.1 - 8.2\n\n\n11/3\nRegression\nHW #5\n7.1\n\n\n11/10\nHoliday/Regression\nHW #6\n7.2\n\n\n11/17\nStandard Errors\nTake-Home Exam #2\nHandouts\n\n\n11/24\nHypothesis Testing\nHW #7\n9.3\n\n\n12/1\nConfidence Intervals\nHW #8\n9.2 - 9.4\n\n\n12/8\nExam #2\n\n\n\n\n\n\nGenerative Artificial Intelligence Policy\nThe use of generative artificial intelligence (AI) in an ethical manner is permitted for this course.\n\nPermitted Uses\nYou may use AI for:\n\nObtain clarification\nBrainstorming ideas, examples, outlines, and strategies\nGenerating questions for practice or exploration\nIdentifying keywords or phrasing to match professional goals\n\n\n\nProhibited Uses\nYou may not:\n\nSubmit AI-generated work\nUse AI to complete assignments, quizzes, exams, or other assessments meant to reflect only your own work\nUse AI to generate code\n\nAny AI-generated work will receive a 0 in the class. Severe cases will be reported to Academic Misconduct."
  },
  {
    "objectID": "syllabus.html#university-policies",
    "href": "syllabus.html#university-policies",
    "title": "Syllabus",
    "section": "University Policies",
    "text": "University Policies\n\nSyllabus Policies and Assistance\nCSUCI’s Syllabus Policies and Assistance Website provides important details about academic policies, campus expectations, and student support services that are all highly applicable to your success as a student both in and outside of the classroom. Ensure that you review this site on a regular basis to stay informed about the policies and resources that support your success, as campus resources or policies may change semester to semester.\n\n\nAcademic Honesty\nConduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\n\n\nCSUCI Basic Needs\nPlease use the link to the Basic Needs Program on the Syllabus Policies and Assistance website (&lt;go.csuci.edu/syllabuspolicies&gt;) for information on emergency food, housing accommodations, toiletries, and connections to critical resources.\n\n\nCSUCI Disability Statement\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\n\n\nDisruption\n\nIf I Am Out: I will communicate via email and will hold classes asynchronously.\nIf You Are Out: Contact me as soon as possible to talk about your options. Reasonable accommodations will be provided for a brief absence. With proper documentation, extended accommodations will be provided."
  },
  {
    "objectID": "posts/week_9.html",
    "href": "posts/week_9.html",
    "title": "Week 9",
    "section": "",
    "text": "Sampling Distributions\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\nTuesday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_9.html#learning-outcomes",
    "href": "posts/week_9.html#learning-outcomes",
    "title": "Week 9",
    "section": "",
    "text": "Sampling Distributions\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\nTuesday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_7.html",
    "href": "posts/week_7.html",
    "title": "Week 7",
    "section": "",
    "text": "Marginal Distributions\nConditional Density Functions\nIndependence\nExpected Value\nCovariance\n\n\n\n\n\nSampling Distributions\nCentral Limit Theorem\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_7.html#learning-outcomes",
    "href": "posts/week_7.html#learning-outcomes",
    "title": "Week 7",
    "section": "",
    "text": "Marginal Distributions\nConditional Density Functions\nIndependence\nExpected Value\nCovariance\n\n\n\n\n\nSampling Distributions\nCentral Limit Theorem\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_7.html#important-concepts",
    "href": "posts/week_7.html#important-concepts",
    "title": "Week 7",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nTuesday\n\nBivariate Distributions\n\nDiscrete Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, the joint distribution function of \\((X_1, X_2)\\) is defined as\n\\[\np_{X_1,X_2}(X_1, X_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe properties of a bivariate discrete distribution are\n\n\\(p_{X_1,X_2}(x_1,x_2)\\ge 0\\) for all \\(x_1,\\ x_2\\)\n\\(\\sum_{x_1,x_2}p(x_1,x_2)=1\\)\n\n\n\nContinuous Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, the joint distribution function of \\((X_1, X_2)\\) is defined as\n\\[\nF_{X_1,X_2}(x_1, x_2) = P(X_1\\le x_1, X_2 \\le x_2).\n\\]\nThe properties of a bivariate continuous distribution are\n\n\\(f_{X_1,X_2}(x_1,x_2)=\\frac{\\partial^2F(x_1,x_2)}{\\partial x_1\\partial x_2}\\)\n\\(f_{X_1,X_2}(x_1, x_2)\\ge 0\\)\n\\(\\int_{x_1}\\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2dx_1=1\\)\n\n\n\n\nMarginal Distributions\n\nDiscrete Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(X_1, X_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe marginal distribution of \\(X_1\\) is defined as\n\\[\np_{X_1}(x_1) = \\sum_{x_2}p_{X_1,X_2}(x_1,x_2)\n\\]\n\n\nContinuous Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The marginal distribution of \\(X_1\\) is defined as\n\\[\nf_{X_1}(x_1) = \\int_{x_1}f_{X_1,X_2}(x_1,x_2)dx_2\n\\]\n\n\n\nConditional Distributions\n\nDiscrete Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(X_1, X_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]\n\n\nContinuous Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]\n\n\n\nIndependence\n\nDiscrete Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]\n\n\nContinuous Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]\n\n\n\nExpected Value\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(X_1,\\ldots, X_n)f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n)dx_n \\cdots dx_1\n\\]\n\n\nCovariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(\\mu_1\\) and \\(\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]\n\n\nExpected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)\n\n\n\nConditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\n\n\n\nThursday\n\nSampling Distributions\n\nObserving Random Variables\nWhen collecting a sample of \\(n\\), we tend to observe individual random variables: \\(\\{X_1, X_2, \\cdots,X_n\\}\\).\n\n\nSum of Random Variables\nLet \\(X_i\\), for \\(i=1, \\cdots, n\\), be identically and independently distributed (iid) normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(T=\\sum_{i=1}^nX_i\\) follow an normal distribution with mean \\(\\mu\\) and variance \\(n\\sigma^2\\).\n\n\n\nCentral Limit Theorem\nLet \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define\n\\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\).\n\n\nOther Sampling Distributions\n\n\\(\\chi^2\\)-distribution\nLet \\(Z_1, Z_2,\\ldots,Z_n \\overset{iid}{\\sim}N(0,1)\\),\n\\[\n\\sum_{i=1}^nZ_i^2\\sim\\chi^2_n.\n\\]\nLet \\(X_1, X_2,\\ldots,X_n \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), \\(S^2 = \\frac{1}{n-1}\\sum^n_{i=1}(X_i-\\bar X)^2\\), and \\(\\bar X \\perp S^2\\); therefore:\n\\[\n\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}.\n\\]\n\n\nt-distribution\nLet \\(Z\\sim N(0,1)\\), \\(W\\sim \\chi^2_\\nu\\), \\(Z\\perp W\\); therefore:\n\\[\nT=\\frac{Z}{\\sqrt{W/\\nu}} \\sim t_\\nu\n\\]\n\n\nF-distribution\nLet \\(W_1\\sim\\chi^2_{\\nu_1}\\) \\(W_2\\sim\\chi^2_{\\nu_2}\\), and \\(W_1\\perp W_2\\); therefore:\n\\[\nF = \\frac{W_1/\\nu_1}{W_2/\\nu_2}\\sim F_{\\nu_1,\\nu_2}\n\\]"
  },
  {
    "objectID": "posts/week_7.html#resources",
    "href": "posts/week_7.html#resources",
    "title": "Week 7",
    "section": "Resources",
    "text": "Resources\n\nFirst Lecture\n\n\n\nSlides\nVideos\n\n\n\n\nSlides\nVideo 001 Video 002\n\n\n\n\n\nSecond Lecture\n\n\n\nSlides\nVideos\n\n\n\n\nSlides\nVideo 001 Video 002"
  },
  {
    "objectID": "posts/week_5.html",
    "href": "posts/week_5.html",
    "title": "Week 5",
    "section": "",
    "text": "Continuous Random Variables\n\n\n\n\n\nExpected Values\nMGF\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_5.html#learning-outcomes",
    "href": "posts/week_5.html#learning-outcomes",
    "title": "Week 5",
    "section": "",
    "text": "Continuous Random Variables\n\n\n\n\n\nExpected Values\nMGF\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_5.html#important-concepts",
    "href": "posts/week_5.html#important-concepts",
    "title": "Week 5",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nFirst Lecture\n\nContinuous Variables\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist.\n\nCDF\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function\n\n\n\nPDF\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)\n\n\n\nCommonly Used Distributions\n\n\n\n\n\n\n\n\nDistribution\nParameters\nPDF\n\n\n\n\nUniform\n\\(a\\) and \\(b\\)\n\\(\\frac{1}{b-a}\\)\n\n\nNormal\n\\(\\mu\\) and \\(\\sigma^2\\)\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)\n\n\nExponential\n\\(\\lambda\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\nGamma\n\\(\\alpha\\) and \\(\\beta\\)\n\\(\\frac{x^{\\alpha-1}e^{-x/\\beta}}{\\beta^\\alpha\\Gamma(\\alpha)}\\)\n\n\nBeta\n\\(\\alpha\\) and \\(\\beta\\)\n\\(\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\int^1_0x^{\\alpha-1}(1-x)^{\\beta-1}dx}\\)\n\n\n\n\n\n\n\nSecond Lecture\n\n\nExpected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of a discrete random variable is \\(Y\\) with PDF of \\(f(y)\\) is\n\\[\nE(Y)=\\int_y yf(y)dy\n\\]\n\n\nVariance\nThe variance represents the variation of a random variable. The variance for a random variable Y is\n\\[\nVar(Y) = E[\\{Y-E(Y)\\}^2]\n\\]"
  },
  {
    "objectID": "posts/week_3.html",
    "href": "posts/week_3.html",
    "title": "Week 3",
    "section": "",
    "text": "Describe Discrete Distributions\n\n\n\n\n\nExpected Values\nVariance\nProperties of Expected Values\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nUnavailable\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_3.html#learning-outcomes",
    "href": "posts/week_3.html#learning-outcomes",
    "title": "Week 3",
    "section": "",
    "text": "Describe Discrete Distributions\n\n\n\n\n\nExpected Values\nVariance\nProperties of Expected Values\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nUnavailable\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_3.html#important-concepts",
    "href": "posts/week_3.html#important-concepts",
    "title": "Week 3",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nFirst Lecture\n\nDiscrete Variables\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values.\n\nPMF\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\).\n\n\nCDF\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\).\n\n\nCommonly Used Distributions\n\n\n\n\n\n\n\n\nDistribution\nParameter(s)\nPMF \\(P(Y=y)\\)\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\n\nBinomial\n\\(n\\) and \\(p\\)\n\\((^n_y)p^y(1-p)^{n-y}\\)\n\n\nGeometric\n\\(p\\)\n\\((1-p)^{y-1}p\\)\n\n\nNegative Binomial\n\\(r\\) and \\(p\\)\n\\((^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}\\)\n\n\nHypergeometric\n\\(N\\), \\(n\\), and \\(r\\)\n\\(\\frac{(^r_y)(^{N-r}_{n-y})}{(^N_n)}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{\\lambda^y}{y!} e^{-\\lambda}\\)\n\n\n\n\n\n\n\nSecond Lecture\n\nExpected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of a discrete random variable is \\(Y\\) with PMF of \\(P(y)\\) is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]\n\n\nExpected Value Properties\nSpecial properties of the expected value:\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(Y)\\}=cE\\{g(Y)\\}\\)\n\\(E\\{g_1(Y)+\\cdots+g_n(Y)\\}=E\\{g_1(Y)\\}+\\cdots+E\\{g_n(Y)\\}\\)\n\n\n\nVariance\nThe variance represents the variation of a random variable. The variance for a random variable Y is\n\\[\nVar(Y) = E[\\{Y-E(Y)\\}^2]\n\\]"
  },
  {
    "objectID": "posts/week_1.html",
    "href": "posts/week_1.html",
    "title": "Week 1",
    "section": "",
    "text": "Define Population\nDefine Sample\nDefine Inference\nDefine Association\n\n\n\n\n\nInstalling R and RStudio\nScripts\nR Packages\nR Environment"
  },
  {
    "objectID": "posts/week_1.html#learning-outcomes",
    "href": "posts/week_1.html#learning-outcomes",
    "title": "Week 1",
    "section": "",
    "text": "Define Population\nDefine Sample\nDefine Inference\nDefine Association\n\n\n\n\n\nInstalling R and RStudio\nScripts\nR Packages\nR Environment"
  },
  {
    "objectID": "posts/week_1.html#resources",
    "href": "posts/week_1.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\n\nLecture\nTuesday Slides | Thursday Slides\n\n\nVideos\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\n\nVideo\n\n\n002\n\nVideo"
  },
  {
    "objectID": "posts/week_1.html#important-concepts",
    "href": "posts/week_1.html#important-concepts",
    "title": "Week 1",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nFirst Lecture\n\nPopulation\nA set of all measurements of interest to the sample collector.\n\n\nSample\nA sample is any subset of measurements selected from the population.\n\n\nInference\n\nParameter: a measurement describing the population\nStatistic: a measurement describing the sample\n\n\n\nAssociation\nAn association describes the relationship between two characteristics of a population.\n\n\n\nSecond Lecture\n\nAccessing R & RStudio\nIf you are on a tablet or Chromebook, you can access R & RStudio via rstudio.cloud for free. However, they have limited computing resources. Be mindful of your experimentation. You may also be able to use Quarto in Rstudio cloud.\nYou can install R via their website: https://www.r-project.org/.\nYou can install RStudio for free from their website: https://www.rstudio.com/products/rstudio/download/\n\n\nUsing R\nR can be used as a calculator; below are a few examples:\n\n1+2\n\n[1] 3\n\n3/4\n\n[1] 0.75\n\n9*8\n\n[1] 72\n\nexp(4)\n\n[1] 54.59815\n\n\n\n\nR Functions\nR has specialized functions that can compute specific values. R functions require inputs, known as arguments, to produce a specific output.\nFor example, the log() function can be used to compute the natural logarithm of a specified input:\n\nlog(34)\n\n[1] 3.526361\n\n\nIf you want to know information about a specific function, you can use the ? operator:\n\n?log\n\nwhich will open the help tab. Notice there are 2 arguments: x and base. This means that the log() function can be extended to other base. To use common log1, specify the arguments:\n\nlog(x=34, base=10)\n\n[1] 1.531479\n\n\nNotice that I specified the arguments. You can also type this:\n\nlog(34, 10)\n\n[1] 1.531479\n\n\nwhich produces the same results. This is because R uses positions in the function to determine argument values; therefore, if the positions are correct, you do not need to specify the argument name.\nGoing back to the First Lecture example, log(34), we did not specify the base. This is because functions have default values for arguments. The help documentation tells us what arguments have defaults and do not need to be specified.\n\n\nInstall packages\nYou can extend the functionality of R. The tidyverse package includes a popular set of R packages for data wrangling and analysis. To install tidyverse, use the install.packages() function2:\n\ninstall.packages('tidyverse')\n\nOnce you installed the R package, you will need to load with every R session using the library() function:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/week_1.html#footnotes",
    "href": "posts/week_1.html#footnotes",
    "title": "Week 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(\\log_{10}(x)\\)↩︎\nThe package name must be inputted with quotes in the function.↩︎"
  },
  {
    "objectID": "lectures/lecture_7.html#learning-outcomes",
    "href": "lectures/lecture_7.html#learning-outcomes",
    "title": "Moment Generating Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMoments\nMoment Generating Functions\nProperties"
  },
  {
    "objectID": "lectures/lecture_7.html#moments",
    "href": "lectures/lecture_7.html#moments",
    "title": "Moment Generating Functions",
    "section": "Moments",
    "text": "Moments\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of \\(Y\\) is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]\nwhere \\(P(y)\\) is the PMF of \\(Y\\)."
  },
  {
    "objectID": "lectures/lecture_7.html#moment-generating-functions",
    "href": "lectures/lecture_7.html#moment-generating-functions",
    "title": "Moment Generating Functions",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(c) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(c\\), and setting \\(c\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(c)}{dc}\\Bigg|_{c=0}\n\\]"
  },
  {
    "objectID": "lectures/lecture_7.html#mgf",
    "href": "lectures/lecture_7.html#mgf",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/lecture_7.html#ey2",
    "href": "lectures/lecture_7.html#ey2",
    "title": "Moment Generating Functions",
    "section": "\\(E(Y^2)\\)",
    "text": "\\(E(Y^2)\\)"
  },
  {
    "objectID": "lectures/lecture_7.html#mgf-1",
    "href": "lectures/lecture_7.html#mgf-1",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/lecture_7.html#ey3",
    "href": "lectures/lecture_7.html#ey3",
    "title": "Moment Generating Functions",
    "section": "\\(E(Y^3)\\)",
    "text": "\\(E(Y^3)\\)"
  },
  {
    "objectID": "lectures/lecture_7.html#mgf-2",
    "href": "lectures/lecture_7.html#mgf-2",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/lecture_7.html#ey",
    "href": "lectures/lecture_7.html#ey",
    "title": "Moment Generating Functions",
    "section": "\\(E(Y)\\)",
    "text": "\\(E(Y)\\)"
  },
  {
    "objectID": "lectures/lecture_7.html#properties",
    "href": "lectures/lecture_7.html#properties",
    "title": "Moment Generating Functions",
    "section": "Properties",
    "text": "Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(Y)\\}=cE\\{g(Y)\\}\\)\n\\(E\\{g_1(Y)+\\cdots+g_n(Y)\\}=E\\{g_1(Y)\\}+\\cdots+E\\{g_n(Y)\\}\\)"
  },
  {
    "objectID": "lectures/lecture_5.html#learning-outcomes",
    "href": "lectures/lecture_5.html#learning-outcomes",
    "title": "Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDiscrete Random Variables\n\nObtain Probabilities"
  },
  {
    "objectID": "lectures/lecture_5.html#discrete-random-variables-1",
    "href": "lectures/lecture_5.html#discrete-random-variables-1",
    "title": "Distribution Functions",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values.\n\n\n\n\n\n\n\n\n\nDistribution\nParameter(s)\nPMF \\(P(Y=y)\\)\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p^y(1-p)^{1-y}\\)\n\n\nBinomial\n\\(n\\) and \\(p\\)\n\\((^n_y)p^y(1-p)^{n-y}\\)\n\n\nGeometric\n\\(p\\)\n\\((1-p)^{y-1}p\\)\n\n\nNegative Binomial\n\\(r\\) and \\(p\\)\n\\((^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{\\lambda^y}{y!} e^{-\\lambda}\\)"
  },
  {
    "objectID": "lectures/lecture_5.html#probability-mass-function",
    "href": "lectures/lecture_5.html#probability-mass-function",
    "title": "Distribution Functions",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\)."
  },
  {
    "objectID": "lectures/lecture_5.html#cumulative-density-function",
    "href": "lectures/lecture_5.html#cumulative-density-function",
    "title": "Distribution Functions",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\)."
  },
  {
    "objectID": "lectures/lecture_5.html#bernoulli-distribution-1",
    "href": "lectures/lecture_5.html#bernoulli-distribution-1",
    "title": "Distribution Functions",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nA Bernoulli distribution is probability of having a success out of two outcomes. In essence, a coin flip with probability of success \\(p\\)."
  },
  {
    "objectID": "lectures/lecture_5.html#binomial-distribution-1",
    "href": "lectures/lecture_5.html#binomial-distribution-1",
    "title": "Distribution Functions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nFor a given sample, the number of success is represent with a binomial distribution. Binary outcomes are represented with either a Bernoulli of binomial distribution. For a binomial experiment, the following must be satisfied:\n\n\nThere is a fixed \\(n\\) trials\nThe there are two outcomes for each trial\nThe probability of success (\\(p\\)) is constant for each trial\nThe trials are independent of each other"
  },
  {
    "objectID": "lectures/lecture_5.html#pmf",
    "href": "lectures/lecture_5.html#pmf",
    "title": "Distribution Functions",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/lecture_5.html#example",
    "href": "lectures/lecture_5.html#example",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.3. An arborist decides to plant 8 seeds in the community. What is the probability that 3 to 5 seeds will germinate?"
  },
  {
    "objectID": "lectures/lecture_5.html#example-1",
    "href": "lectures/lecture_5.html#example-1",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.6. An arborist decides to plant 8 seeds in the community. What is the probability that at least 1 seed germinates?"
  },
  {
    "objectID": "lectures/lecture_5.html#example-2",
    "href": "lectures/lecture_5.html#example-2",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.8. An arborist decides to plant 6 seeds in the community. What is the probability that an even number of seed will germinate?"
  },
  {
    "objectID": "lectures/lecture_5.html#poisson-distribution-1",
    "href": "lectures/lecture_5.html#poisson-distribution-1",
    "title": "Distribution Functions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe poisson distribution describes an experiment that measures that occurrence of an event at specific point and/or time period."
  },
  {
    "objectID": "lectures/lecture_5.html#pmf-1",
    "href": "lectures/lecture_5.html#pmf-1",
    "title": "Distribution Functions",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/lecture_5.html#example-3",
    "href": "lectures/lecture_5.html#example-3",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe number of industrial accidents at a particular manufacturing plant is found to be an average of 1 accident per month. In the previous month, there were 3 accidents. What is the probability of observing 3 or more accidents in a given month?"
  },
  {
    "objectID": "lectures/lecture_5.html#example-4",
    "href": "lectures/lecture_5.html#example-4",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe number of industrial accidents at a particular manufacturing plant is found to be an average of 8 accidents per year. What is the probability of observing 15 or more accidents in a given year?"
  },
  {
    "objectID": "lectures/lecture_5.html#geometric-distribution-1",
    "href": "lectures/lecture_5.html#geometric-distribution-1",
    "title": "Distribution Functions",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\nA geometric distribution models the number of Bernoulli trials until the first success (or failure)."
  },
  {
    "objectID": "lectures/lecture_5.html#pmf-2",
    "href": "lectures/lecture_5.html#pmf-2",
    "title": "Distribution Functions",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/lecture_5.html#example-5",
    "href": "lectures/lecture_5.html#example-5",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nSuppose 30% of the applicants for a certain industrial job possess the necessary skills. What is the probability that the first applicant with the necessary skills is found on the 5th interview."
  },
  {
    "objectID": "lectures/lecture_5.html#negative-binomial-distribution-1",
    "href": "lectures/lecture_5.html#negative-binomial-distribution-1",
    "title": "Distribution Functions",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\nA negative binomial distribution models the number of successful Bernoulli trials until the \\(r\\)th failure."
  },
  {
    "objectID": "lectures/lecture_5.html#pmf-3",
    "href": "lectures/lecture_5.html#pmf-3",
    "title": "Distribution Functions",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/lecture_5.html#example-6",
    "href": "lectures/lecture_5.html#example-6",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nTen percent of engines manufactured on an assembly line are defective. If engines are randomly selected one at a time and tested, what is the probability that third nondefective engine is found on the 4th trial."
  },
  {
    "objectID": "lectures/lecture_3.html#learning-objectives",
    "href": "lectures/lecture_3.html#learning-objectives",
    "title": "Introduction to Probability",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine sample space and experiment\nDefine probabilities\nDefine random variable and distribution function"
  },
  {
    "objectID": "lectures/lecture_3.html#sample-space",
    "href": "lectures/lecture_3.html#sample-space",
    "title": "Introduction to Probability",
    "section": "Sample Space",
    "text": "Sample Space"
  },
  {
    "objectID": "lectures/lecture_3.html#event",
    "href": "lectures/lecture_3.html#event",
    "title": "Introduction to Probability",
    "section": "Event",
    "text": "Event\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/lecture_3.html#set-rules",
    "href": "lectures/lecture_3.html#set-rules",
    "title": "Introduction to Probability",
    "section": "Set Rules",
    "text": "Set Rules\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/lecture_3.html#set-rules-1",
    "href": "lectures/lecture_3.html#set-rules-1",
    "title": "Introduction to Probability",
    "section": "Set Rules",
    "text": "Set Rules\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/lecture_3.html#enumerating-outcomes",
    "href": "lectures/lecture_3.html#enumerating-outcomes",
    "title": "Introduction to Probability",
    "section": "Enumerating outcomes",
    "text": "Enumerating outcomes\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/lecture_3.html#probability-rules",
    "href": "lectures/lecture_3.html#probability-rules",
    "title": "Introduction to Probability",
    "section": "Probability Rules",
    "text": "Probability Rules"
  },
  {
    "objectID": "lectures/lecture_3.html#probability-rules-1",
    "href": "lectures/lecture_3.html#probability-rules-1",
    "title": "Introduction to Probability",
    "section": "Probability Rules",
    "text": "Probability Rules"
  },
  {
    "objectID": "lectures/lecture_3.html#random-variable-1",
    "href": "lectures/lecture_3.html#random-variable-1",
    "title": "Introduction to Probability",
    "section": "Random Variable",
    "text": "Random Variable"
  },
  {
    "objectID": "lectures/lecture_3.html#probability-mass-function",
    "href": "lectures/lecture_3.html#probability-mass-function",
    "title": "Introduction to Probability",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function"
  },
  {
    "objectID": "lectures/lecture_3.html#cumulative-density-function",
    "href": "lectures/lecture_3.html#cumulative-density-function",
    "title": "Introduction to Probability",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function"
  },
  {
    "objectID": "lectures/lecture_3.html#example-1",
    "href": "lectures/lecture_3.html#example-1",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nSuppose we want to understand the efficacy of a test for a certain disease. Consider the following table:\n\n\n\n\n\nDisease\nPresence\nTotal\n\n\n\n\n\n\nYes\nNo\n\n\n\nTest Result\nYes\n42\n6\n\n\n\n\nNo\n17\n35\n\n\n\n\n\n\n\n100"
  },
  {
    "objectID": "lectures/lecture_3.html#example-2",
    "href": "lectures/lecture_3.html#example-2",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\nFind the probability that an individual has a disease\nFind the probability that an individual tests negative for a disease\nFind the probability that and tests positive for a disease or they don’t have the disease\nFind the probability that the test gives an accurate result\nFind the probability that the test gives and inaccurate result"
  },
  {
    "objectID": "lectures/lecture_23.html#learning-objectives",
    "href": "lectures/lecture_23.html#learning-objectives",
    "title": "Standard Errors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nStandard Errors\n\nLinear Regression\nGLM\n\nSampling Distributions"
  },
  {
    "objectID": "lectures/lecture_23.html#standard-errors",
    "href": "lectures/lecture_23.html#standard-errors",
    "title": "Standard Errors",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nFind the variance of the estimate\nFind the information matrix\nUse for Inference"
  },
  {
    "objectID": "lectures/lecture_23.html#finding-the-variance",
    "href": "lectures/lecture_23.html#finding-the-variance",
    "title": "Standard Errors",
    "section": "Finding the Variance",
    "text": "Finding the Variance"
  },
  {
    "objectID": "lectures/lecture_23.html#estimate-for-sigma2",
    "href": "lectures/lecture_23.html#estimate-for-sigma2",
    "title": "Standard Errors",
    "section": "Estimate for \\(\\sigma^2\\)",
    "text": "Estimate for \\(\\sigma^2\\)\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum^n_{i=1} (Y_i-\\boldsymbol X_i^\\mathrm T\\hat{\\boldsymbol \\beta})^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#standard-errors-of-betas",
    "href": "lectures/lecture_23.html#standard-errors-of-betas",
    "title": "Standard Errors",
    "section": "Standard Errors of \\(\\beta\\)’s",
    "text": "Standard Errors of \\(\\beta\\)’s\n\\[\nSE(\\hat\\beta_0)=\\sqrt{\\frac{\\sum^n_{i=1}x_i^2\\hat\\sigma^2}{n\\sum^n_{i=1}(x_i-\\bar x)^2}}\n\\]\n\\[\nSE(\\hat\\beta_1)=\\sqrt\\frac{\\hat\\sigma^2}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#standard-errors-matrix-form",
    "href": "lectures/lecture_23.html#standard-errors-matrix-form",
    "title": "Standard Errors",
    "section": "Standard Errors Matrix Form",
    "text": "Standard Errors Matrix Form\n\\[\nVar(\\hat {\\boldsymbol \\beta}) = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1} \\hat \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#large-sample-theory",
    "href": "lectures/lecture_23.html#large-sample-theory",
    "title": "Standard Errors",
    "section": "Large Sample Theory",
    "text": "Large Sample Theory\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be the MLE estimator for a parameter \\(\\theta\\). As \\(n\\rightarrow\\infty\\), then \\(\\hat \\theta\\) has a normal distribution with mean \\(\\theta\\) and variance \\(1/nI(\\theta)\\), where\n\\[\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right]\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#sampling-distributions-1",
    "href": "lectures/lecture_23.html#sampling-distributions-1",
    "title": "Standard Errors",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\n\n\\(\\phi\\) known\n\\[\n\\frac{\\hat\\beta_j - \\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]\n\n\\(\\phi\\) unknown\n\\[\n\\frac{\\hat\\beta_j-\\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#learning-outcomes",
    "href": "lectures/lecture_21.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExponential Family of Distributions\nGeneralized Linear Models"
  },
  {
    "objectID": "lectures/lecture_21.html#exponential-family-of-distributions-1",
    "href": "lectures/lecture_21.html#exponential-family-of-distributions-1",
    "title": "Generalized Linear Models",
    "section": "Exponential Family of Distributions",
    "text": "Exponential Family of Distributions\nAn exponential family of distributions are random variables that allow their probability density function to have the following form:\n\\[\nf(y; \\theta,\\phi) = a(y,\\phi)\\exp\\left\\{\\frac{y\\theta-\\kappa(\\theta)}{\\phi}\\right\\}\n\\]\n\n\\(\\theta\\): is the canonical parameter (also a function of other parameters)\n\\(\\kappa(\\theta)\\): is a known cumulant function\n\\(\\phi&gt;0\\): dispersion parameter function\n\\(a(y,\\phi)\\): normalizing constant"
  },
  {
    "objectID": "lectures/lecture_21.html#canonical-parameter",
    "href": "lectures/lecture_21.html#canonical-parameter",
    "title": "Generalized Linear Models",
    "section": "Canonical Parameter",
    "text": "Canonical Parameter\nThe canonical parameter represents the relationship between the random variable and the \\(E(Y)=\\mu\\)"
  },
  {
    "objectID": "lectures/lecture_21.html#normal-distribution",
    "href": "lectures/lecture_21.html#normal-distribution",
    "title": "Generalized Linear Models",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\\[\nf(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#binomial-distribution",
    "href": "lectures/lecture_21.html#binomial-distribution",
    "title": "Generalized Linear Models",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\\[\nf(x;n,p)=\\big(^n_x\\big) p^x(1-p)^{n-x}\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#poisson-distribution",
    "href": "lectures/lecture_21.html#poisson-distribution",
    "title": "Generalized Linear Models",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\\[\nf(x;\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#common-distributions-and-canonical-parameters",
    "href": "lectures/lecture_21.html#common-distributions-and-canonical-parameters",
    "title": "Generalized Linear Models",
    "section": "Common Distributions and Canonical Parameters",
    "text": "Common Distributions and Canonical Parameters\n\n\n\nRandom Variable\nCanonical Parameter\n\n\n\n\nNormal\n\\(\\mu\\)\n\n\nBinomial\n\\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\n\nNegative Binomial\n\\(\\log\\left(\\frac{\\mu}{\\mu+k}\\right)\\)\n\n\nPoisson\n\\(\\log(\\mu)\\)\n\n\nGamma\n\\(-\\frac{1}{\\mu}\\)\n\n\nInverse Gaussian\n\\(-\\frac{1}{2\\mu^2}\\)"
  },
  {
    "objectID": "lectures/lecture_21.html#generalized-linear-models-1",
    "href": "lectures/lecture_21.html#generalized-linear-models-1",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA generalized linear model (GLM) is used to model the association between an outcome variable (of any data type) and a set of predictor values. We estimate a set of regression coefficients \\(\\boldsymbol \\beta\\) to explain how each predictor is related to the expected value of the outcome."
  },
  {
    "objectID": "lectures/lecture_21.html#generalized-linear-models-2",
    "href": "lectures/lecture_21.html#generalized-linear-models-2",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA GLM is composed of a systematic and random component."
  },
  {
    "objectID": "lectures/lecture_21.html#random-component",
    "href": "lectures/lecture_21.html#random-component",
    "title": "Generalized Linear Models",
    "section": "Random Component",
    "text": "Random Component\nThe random component is the random variable that defines the randomness and variation of the outcome variable."
  },
  {
    "objectID": "lectures/lecture_21.html#systematic-component",
    "href": "lectures/lecture_21.html#systematic-component",
    "title": "Generalized Linear Models",
    "section": "Systematic Component",
    "text": "Systematic Component\nThe systematic component is the linear model that models the association between a set of predictors and the expected value of Y:\n\\[\ng(\\mu)=\\eta=\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta\n\\]\n\n\\(\\boldsymbol\\beta\\): regression coefficients\n\\(\\boldsymbol X_i=(1, X_{i1}, \\ldots, X_{ip})^\\mathrm T\\): design vector\n\\(\\eta\\): linear model\n\\(\\mu=E(Y)\\)\n\\(g(\\cdot)\\): link function"
  },
  {
    "objectID": "lectures/lecture_2.html#r-programming",
    "href": "lectures/lecture_2.html#r-programming",
    "title": "Introduction to R/RStudio",
    "section": "R Programming",
    "text": "R Programming\nR is a statistical programming package that allows you to conduct different types of analysis.\nR"
  },
  {
    "objectID": "lectures/lecture_2.html#rstudio",
    "href": "lectures/lecture_2.html#rstudio",
    "title": "Introduction to R/RStudio",
    "section": "RStudio",
    "text": "RStudio\nA piece of software that organizes how you conduct statistical analysis in R.\nRStudio"
  },
  {
    "objectID": "lectures/lecture_2.html#posit-cloud",
    "href": "lectures/lecture_2.html#posit-cloud",
    "title": "Introduction to R/RStudio",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nA web version of RStudio.\nPosit Cloud"
  },
  {
    "objectID": "lectures/lecture_2.html#r-packages",
    "href": "lectures/lecture_2.html#r-packages",
    "title": "Introduction to R/RStudio",
    "section": "R Packages",
    "text": "R Packages\n\nTidyverse\nINQS Tools"
  },
  {
    "objectID": "lectures/lecture_2.html#r-as-a-calculator",
    "href": "lectures/lecture_2.html#r-as-a-calculator",
    "title": "Introduction to R/RStudio",
    "section": "R as a calculator",
    "text": "R as a calculator\nR can evaluate different expressions in the console tab.\nTry the following:\n\n\\(4(4+2)/34\\)\n\\(6^3\\)\n\\(3-1\\)\n\\(4+4/3+45(32*34-54)\\)"
  },
  {
    "objectID": "lectures/lecture_2.html#r-functions",
    "href": "lectures/lecture_2.html#r-functions",
    "title": "Introduction to R/RStudio",
    "section": "R Functions",
    "text": "R Functions\nR functions performs tasks to specific data values.\nEvaluate the following values in R:\n\n\\(\\sqrt{3}\\)\n\\(e^3\\)\n\\(\\ln(53)\\)\n\\(\\log(324)\\)\n\\(\\sin(3)\\)\n\\(\\sin(3\\pi)\\)"
  },
  {
    "objectID": "lectures/lecture_2.html#types-of-data",
    "href": "lectures/lecture_2.html#types-of-data",
    "title": "Introduction to R/RStudio",
    "section": "Types of Data",
    "text": "Types of Data\n\nNumeric\nCharacter\nLogical\nMissing\n\nEvaluate the following code:"
  },
  {
    "objectID": "lectures/lecture_2.html#types-of-objects",
    "href": "lectures/lecture_2.html#types-of-objects",
    "title": "Introduction to R/RStudio",
    "section": "Types of Objects",
    "text": "Types of Objects\nIn R, an object contains a set of data. The most common types are vectors and matrix.\nRun this code and print out the objects in the console:"
  },
  {
    "objectID": "lectures/lecture_2.html#data-frames",
    "href": "lectures/lecture_2.html#data-frames",
    "title": "Introduction to R/RStudio",
    "section": "Data Frames",
    "text": "Data Frames\nData frames can be thought of as R’s version of a data set.\nPlay around with mtcars:\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "lectures/lecture_2.html#lists",
    "href": "lectures/lecture_2.html#lists",
    "title": "Introduction to R/RStudio",
    "section": "Lists",
    "text": "Lists\nList can be thought as an extended vector, but each element is a different R object.\nTry playing with this R object:"
  },
  {
    "objectID": "lectures/lecture_18.html#learning-outcomes",
    "href": "lectures/lecture_18.html#learning-outcomes",
    "title": "Maximum Likelihood Estimators",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMaximum Likelihood Estimators\nProperties"
  },
  {
    "objectID": "lectures/lecture_18.html#estimators",
    "href": "lectures/lecture_18.html#estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample."
  },
  {
    "objectID": "lectures/lecture_18.html#data",
    "href": "lectures/lecture_18.html#data",
    "title": "Maximum Likelihood Estimators",
    "section": "Data",
    "text": "Data\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)\\) where \\(F(\\cdot)\\) is a known distribution function and \\(\\boldsymbol\\theta\\) is a vector of parameters. Let \\(\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}\\), be the sample collected."
  },
  {
    "objectID": "lectures/lecture_18.html#unbiased-estimators",
    "href": "lectures/lecture_18.html#unbiased-estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Unbiased Estimators",
    "text": "Unbiased Estimators\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be an estimator for a parameter \\(\\theta\\). Then \\(\\hat \\theta\\) is an unbiased estimator if \\(E(\\hat \\theta) = \\theta\\). Otherwise, \\(\\hat\\theta\\) is considered biased."
  },
  {
    "objectID": "lectures/lecture_18.html#consistent-estimators",
    "href": "lectures/lecture_18.html#consistent-estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Consistent Estimators",
    "text": "Consistent Estimators\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). The estimator \\(\\hat \\theta\\) is a consistent estimator of the \\(\\theta\\) if\n\n\\(E\\{(\\hat\\theta-\\theta)^2\\}\\rightarrow0\\) as \\(n\\rightarrow \\infty\\)\n\\(P(|\\hat\\theta-\\theta|\\ge \\epsilon)\\rightarrow0\\) as \\(n\\rightarrow \\infty\\) for every \\(\\epsilon&gt;0\\)"
  },
  {
    "objectID": "lectures/lecture_18.html#invariance-property",
    "href": "lectures/lecture_18.html#invariance-property",
    "title": "Maximum Likelihood Estimators",
    "section": "Invariance Property",
    "text": "Invariance Property\nIf \\(\\hat \\theta\\) is an ML estimator of \\(\\theta\\), then for any one-to-one function \\(g\\), the ML estimator for \\(g(\\theta)\\) is \\(g(\\hat\\theta)\\)."
  },
  {
    "objectID": "lectures/lecture_18.html#large-sample-theory",
    "href": "lectures/lecture_18.html#large-sample-theory",
    "title": "Maximum Likelihood Estimators",
    "section": "Large Sample Theory",
    "text": "Large Sample Theory\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be the MLE estimator for a parameter \\(\\theta\\). As \\(n\\rightarrow\\infty\\), then \\(\\hat \\theta\\) has a normal distribution with mean \\(\\theta\\) and variance \\(1/nI(\\theta)\\), where\n\\[\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right]\n\\]"
  },
  {
    "objectID": "lectures/lecture_18.html#exponential-distribution",
    "href": "lectures/lecture_18.html#exponential-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}Exp(\\lambda)\\). Find the sampling distribution of the MLE of \\(\\lambda\\)"
  },
  {
    "objectID": "lectures/lecture_18.html#poisson-distribution",
    "href": "lectures/lecture_18.html#poisson-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Pois}(\\lambda)\\), Find the sampling distribution of the MLE."
  },
  {
    "objectID": "lectures/lecture_18.html#normal-distribution",
    "href": "lectures/lecture_18.html#normal-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). Are the MLE’s of \\(\\mu\\) and \\(\\sigma^2\\) unbiased?"
  },
  {
    "objectID": "lectures/lecture_16.html#learning-outcomes",
    "href": "lectures/lecture_16.html#learning-outcomes",
    "title": "Sampling Distributions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCovariance\nStatistics and Inference\nSampling Distributions\nCentral Limit Theorem"
  },
  {
    "objectID": "lectures/lecture_16.html#covariance-1",
    "href": "lectures/lecture_16.html#covariance-1",
    "title": "Sampling Distributions",
    "section": "Covariance",
    "text": "Covariance\nThe covariance measures the average dependence between multiple random variables. Let \\(W=(^X_Y)\\) be a random vector. The variance of \\(W\\) is defined as\n\\[\nVar(W) = \\left(\\begin{array}{cc}\n\\sigma^2_X & \\sigma_{XY} \\\\\n\\sigma_{XY} & \\sigma^2_{Y}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#covariance-2",
    "href": "lectures/lecture_16.html#covariance-2",
    "title": "Sampling Distributions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(\\mu_1\\) and \\(\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#correlation",
    "href": "lectures/lecture_16.html#correlation",
    "title": "Sampling Distributions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#mgf-property-independence",
    "href": "lectures/lecture_16.html#mgf-property-independence",
    "title": "Sampling Distributions",
    "section": "MGF Property: Independence",
    "text": "MGF Property: Independence\nLet \\(X\\) and \\(Y\\) be independent random variables. Let \\(Z = X+Y\\), the MGF of Z is\n\\[\nM_Z(t) = M_X(t)M_Y(t)\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#sample",
    "href": "lectures/lecture_16.html#sample",
    "title": "Sampling Distributions",
    "section": "Sample",
    "text": "Sample"
  },
  {
    "objectID": "lectures/lecture_16.html#statistics",
    "href": "lectures/lecture_16.html#statistics",
    "title": "Sampling Distributions",
    "section": "Statistics",
    "text": "Statistics"
  },
  {
    "objectID": "lectures/lecture_16.html#inference",
    "href": "lectures/lecture_16.html#inference",
    "title": "Sampling Distributions",
    "section": "Inference",
    "text": "Inference"
  },
  {
    "objectID": "lectures/lecture_16.html#iid-random-variables",
    "href": "lectures/lecture_16.html#iid-random-variables",
    "title": "Sampling Distributions",
    "section": "iid Random Variables",
    "text": "iid Random Variables"
  },
  {
    "objectID": "lectures/lecture_16.html#sampling-distributions-1",
    "href": "lectures/lecture_16.html#sampling-distributions-1",
    "title": "Sampling Distributions",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\nA sampling distribution is the distribution of a statistic. Many known statistics have a known distribution."
  },
  {
    "objectID": "lectures/lecture_16.html#bar-x",
    "href": "lectures/lecture_16.html#bar-x",
    "title": "Sampling Distributions",
    "section": "\\(\\bar X\\)",
    "text": "\\(\\bar X\\)"
  },
  {
    "objectID": "lectures/lecture_16.html#s2",
    "href": "lectures/lecture_16.html#s2",
    "title": "Sampling Distributions",
    "section": "\\(s^2\\)",
    "text": "\\(s^2\\)"
  },
  {
    "objectID": "lectures/lecture_16.html#t-distribution",
    "href": "lectures/lecture_16.html#t-distribution",
    "title": "Sampling Distributions",
    "section": "t-distribution",
    "text": "t-distribution\nLet \\(Z\\sim N(0,1)\\), \\(W\\sim \\chi^2_\\nu\\), \\(Z\\perp W\\); therefore:\n\\[\nT=\\frac{Z}{\\sqrt{W/\\nu}} \\sim t_\\nu\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#f-distribution",
    "href": "lectures/lecture_16.html#f-distribution",
    "title": "Sampling Distributions",
    "section": "F-distribution",
    "text": "F-distribution\nLet \\(W_1\\sim\\chi^2_{\\nu_1}\\) \\(W_2\\sim\\chi^2_{\\nu_2}\\), and \\(W_1\\perp W_2\\); therefore:\n\\[\nF = \\frac{W_1/\\nu_1}{W_2/\\nu_2}\\sim F_{\\nu_1,\\nu_2}\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#example",
    "href": "lectures/lecture_16.html#example",
    "title": "Sampling Distributions",
    "section": "Example",
    "text": "Example\nLet \\(X_1, X_2, \\ldots, X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) , show that \\(\\bar X \\sim N(\\mu,\\sigma^2/n)\\). Note: the MGF of \\(X_i\\) is \\(e^{\\mu t + \\frac{t^2\\sigma^2}{2}}\\)."
  },
  {
    "objectID": "lectures/lecture_16.html#central-limit-theorem-1",
    "href": "lectures/lecture_16.html#central-limit-theorem-1",
    "title": "Sampling Distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nLet \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define\n\\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\)."
  },
  {
    "objectID": "lectures/lecture_16.html#central-limit-theorem-2",
    "href": "lectures/lecture_16.html#central-limit-theorem-2",
    "title": "Sampling Distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\\[\n\\bar X \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#example-1",
    "href": "lectures/lecture_16.html#example-1",
    "title": "Sampling Distributions",
    "section": "Example",
    "text": "Example\nLet \\(X_1, \\ldots, X_n \\overset{iid}{\\sim} \\chi^2_p\\), the MGF is \\(M(t)=(1-2t)^{-p/2}\\). Find the distribution of \\(\\bar X\\)."
  },
  {
    "objectID": "lectures/lecture_14.html#learning-outcomes",
    "href": "lectures/lecture_14.html#learning-outcomes",
    "title": "Joint Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConditional Distributions\nIndependence\nExpectations\nCovariance"
  },
  {
    "objectID": "lectures/lecture_14.html#conditional-distributions-1",
    "href": "lectures/lecture_14.html#conditional-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable."
  },
  {
    "objectID": "lectures/lecture_14.html#discrete-conditional-distributions",
    "href": "lectures/lecture_14.html#discrete-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Discrete Conditional Distributions",
    "text": "Discrete Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#continuous-conditional-distributions",
    "href": "lectures/lecture_14.html#continuous-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Continuous Conditional Distributions",
    "text": "Continuous Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#example",
    "href": "lectures/lecture_14.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\nLet the joint density function of \\(X_1\\) and \\(X_2\\) be defined as\n\\[\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\nFind the conditional density function of \\(X_2|X_1=x_1\\)."
  },
  {
    "objectID": "lectures/lecture_14.html#independent-random-variables",
    "href": "lectures/lecture_14.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/lecture_14.html#discrete-independent-random-variables",
    "href": "lectures/lecture_14.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#continuous-independent-random-variables",
    "href": "lectures/lecture_14.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#matrix-algebra",
    "href": "lectures/lecture_14.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#bivariate-normal-distribution",
    "href": "lectures/lecture_14.html#bivariate-normal-distribution",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Normal Distribution",
    "text": "Bivariate Normal Distribution\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/lecture_14.html#expectations-1",
    "href": "lectures/lecture_14.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/lecture_14.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/lecture_14.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/lecture_14.html#conditional-expectations",
    "href": "lectures/lecture_14.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#conditional-expectations-1",
    "href": "lectures/lecture_14.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#covariance-1",
    "href": "lectures/lecture_14.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(E(X_1)=\\mu_1\\) and \\(E(X_2)=\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#correlation",
    "href": "lectures/lecture_14.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#learning-outcomes",
    "href": "lectures/lecture_12.html#learning-outcomes",
    "title": "Joint Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMarginal Distributions\nConditional Distributions\nIndependence\nExpectations\nCovariance"
  },
  {
    "objectID": "lectures/lecture_12.html#marginal-density-functions",
    "href": "lectures/lecture_12.html#marginal-density-functions",
    "title": "Joint Distribution Functions",
    "section": "Marginal Density Functions",
    "text": "Marginal Density Functions\nA Marginal Density Function is density function of one random variable from a random vector."
  },
  {
    "objectID": "lectures/lecture_12.html#marginal-discrete-probability-mass-function",
    "href": "lectures/lecture_12.html#marginal-discrete-probability-mass-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Discrete Probability Mass Function",
    "text": "Marginal Discrete Probability Mass Function\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe marginal distribution of \\(X_1\\) is defined as\n\\[\np_{X_1}(x_1) = \\sum_{x_2}p_{X_1,X_2}(x_1,x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#marginal-continuous-density-function",
    "href": "lectures/lecture_12.html#marginal-continuous-density-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Continuous Density Function",
    "text": "Marginal Continuous Density Function\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The marginal distribution of \\(X_1\\) is defined as\n\\[\nf_{X_1}(x_1) = \\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#example",
    "href": "lectures/lecture_12.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf_{X,Y}(x,y) \\left\\{\\begin{array}{cc}\n2x & 0\\le y \\le 1;\\ 0 \\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(f_X(x)\\)"
  },
  {
    "objectID": "lectures/lecture_12.html#conditional-distributions-1",
    "href": "lectures/lecture_12.html#conditional-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable."
  },
  {
    "objectID": "lectures/lecture_12.html#discrete-conditional-distributions",
    "href": "lectures/lecture_12.html#discrete-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Discrete Conditional Distributions",
    "text": "Discrete Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#continuous-conditional-distributions",
    "href": "lectures/lecture_12.html#continuous-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Continuous Conditional Distributions",
    "text": "Continuous Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2=_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#example-1",
    "href": "lectures/lecture_12.html#example-1",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\nLet the joint density function of \\(X_1\\) and \\(X_2\\) be defined as\n\\[\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\nFind the conditional density function of \\(X_2|X_1=x_1\\)."
  },
  {
    "objectID": "lectures/lecture_12.html#independent-random-variables",
    "href": "lectures/lecture_12.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/lecture_12.html#discrete-independent-random-variables",
    "href": "lectures/lecture_12.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#continuous-independent-random-variables",
    "href": "lectures/lecture_12.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#matrix-algebra",
    "href": "lectures/lecture_12.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#example-2",
    "href": "lectures/lecture_12.html#example-2",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\nShow that \\(X\\perp Y\\).\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/lecture_12.html#expectations-1",
    "href": "lectures/lecture_12.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/lecture_12.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/lecture_12.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/lecture_12.html#conditional-expectations",
    "href": "lectures/lecture_12.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#conditional-expectations-1",
    "href": "lectures/lecture_12.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#covariance-1",
    "href": "lectures/lecture_12.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(\\mu_1\\) and \\(\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#correlation",
    "href": "lectures/lecture_12.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_10.html#learning-outcomes",
    "href": "lectures/lecture_10.html#learning-outcomes",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExpectations\nVariance\nProperties"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value",
    "href": "lectures/lecture_10.html#expected-value",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value for a continuous distribution is defined as\n\\[\nE(X)=\\int x f(x)dx\n\\]\nThe expectation of a function \\(g(X)\\) is defined as\n\\[\nE\\{g(X)\\}=\\int g(x)f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-properties",
    "href": "lectures/lecture_10.html#expected-value-properties",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value Properties",
    "text": "Expected Value Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(X)\\}=cE\\{g(X)\\}\\)\n\\(E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance",
    "href": "lectures/lecture_10.html#variance",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\nThe variance of continuous variable is defined as\n\\[\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-properties",
    "href": "lectures/lecture_10.html#variance-properties",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance Properties",
    "text": "Variance Properties\n\\(Y=aX+b\\)\n\n\\(Var(Y) = Var(aX+b) = Var(aX) + Var(b) = a^2Var(X)\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-1",
    "href": "lectures/lecture_10.html#expected-value-1",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-1",
    "href": "lectures/lecture_10.html#variance-1",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-2",
    "href": "lectures/lecture_10.html#expected-value-2",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-2",
    "href": "lectures/lecture_10.html#variance-2",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-3",
    "href": "lectures/lecture_10.html#expected-value-3",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{Beta}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-3",
    "href": "lectures/lecture_10.html#variance-3",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{Beta}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-4",
    "href": "lectures/lecture_10.html#expected-value-4",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-4",
    "href": "lectures/lecture_10.html#variance-4",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/9a.html#learning-outcomes",
    "href": "lectures/9a.html#learning-outcomes",
    "title": "Sampling Distributions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDistribution of Mean\nDistribution of Sample Variance\nLikelihood Function"
  },
  {
    "objectID": "lectures/9a.html#mgf-independence-property",
    "href": "lectures/9a.html#mgf-independence-property",
    "title": "Sampling Distributions",
    "section": "MGF Independence Property",
    "text": "MGF Independence Property\nIf \\(X\\) and \\(Y\\) are independent, then the MGF of \\(Z = X + Y\\) equals the product of their MGFs:\n\\[\nM_{Z}(t) = M_X(t)M_Y(t).\n\\]"
  },
  {
    "objectID": "lectures/9a.html#distribution-of-means",
    "href": "lectures/9a.html#distribution-of-means",
    "title": "Sampling Distributions",
    "section": "Distribution of Means",
    "text": "Distribution of Means\nSuppose \\(X_1, \\dots, X_n \\sim \\text{N}(\\mu, \\sigma^2)\\)"
  },
  {
    "objectID": "lectures/9a.html#distribution-of-sample-variance",
    "href": "lectures/9a.html#distribution-of-sample-variance",
    "title": "Sampling Distributions",
    "section": "Distribution of Sample Variance",
    "text": "Distribution of Sample Variance\nSuppose \\(X_1, \\dots, X_n \\sim \\text{N}(\\mu, \\sigma^2)\\)"
  },
  {
    "objectID": "lectures/9a.html#likelihood-function-1",
    "href": "lectures/9a.html#likelihood-function-1",
    "title": "Sampling Distributions",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe likelihood function tells us how plausible a set of parameter values is, given the observed data."
  },
  {
    "objectID": "lectures/9a.html#setup",
    "href": "lectures/9a.html#setup",
    "title": "Sampling Distributions",
    "section": "Setup",
    "text": "Setup\nGiven a random sample \\(X_1, X_2, \\dots, X_n \\overset{iid}{\\sim} F(x;\\theta)\\), where \\(\\theta\\) are the parameters that shape the distribution function.\nThen, the joint density function of the sample is:\n\\[\nf(x_1, x_2, \\dots, x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta)\n\\]\nThis expression, viewed as a function of \\(\\theta\\) (for fixed data \\(x_i\\)), is the likelihood function:\n\\[\nL(\\theta \\mid x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n f(x_i; \\theta)\n\\]"
  },
  {
    "objectID": "lectures/9a.html#likelihood-vs.-probability",
    "href": "lectures/9a.html#likelihood-vs.-probability",
    "title": "Sampling Distributions",
    "section": "Likelihood vs. Probability",
    "text": "Likelihood vs. Probability\n\n\n\n\n\n\n\nConcept\nInterpretation\n\n\n\n\nProbability\n\\(P(X = x \\mid \\theta)\\): \\(x\\) is random, \\(\\theta\\) fixed\n\n\nLikelihood\n\\(L(\\theta \\mid x)\\): \\(x\\) fixed, \\(\\theta\\) variable\n\n\n\nAlthough they share the same mathematical form, their interpretations differ."
  },
  {
    "objectID": "lectures/9a.html#log-likelihood-function",
    "href": "lectures/9a.html#log-likelihood-function",
    "title": "Sampling Distributions",
    "section": "Log-Likelihood Function",
    "text": "Log-Likelihood Function\nFor convenience, we often work with the log-likelihood:\n\\[\n\\ell(\\theta) = \\log L(\\theta)\n\\]"
  },
  {
    "objectID": "lectures/9a.html#example-1-bernoulli-random-variables",
    "href": "lectures/9a.html#example-1-bernoulli-random-variables",
    "title": "Sampling Distributions",
    "section": "Example 1: Bernoulli Random Variables",
    "text": "Example 1: Bernoulli Random Variables\nSuppose \\(X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)\\).\n\\[\nf(x_i; p) = p^{x_i}(1 - p)^{1 - x_i}\n\\]"
  },
  {
    "objectID": "lectures/9a.html#example-2-poisson-random-variables",
    "href": "lectures/9a.html#example-2-poisson-random-variables",
    "title": "Sampling Distributions",
    "section": "Example 2: Poisson Random Variables",
    "text": "Example 2: Poisson Random Variables\nSuppose \\(X_1, \\dots, X_n \\sim \\text{Pois}(\\lambda)\\)\n\\[\nf(x_i; \\lambda) = \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!}\n\\]"
  },
  {
    "objectID": "lectures/9a.html#example-3-normal-random-variables",
    "href": "lectures/9a.html#example-3-normal-random-variables",
    "title": "Sampling Distributions",
    "section": "Example 3: Normal Random Variables",
    "text": "Example 3: Normal Random Variables\nSuppose \\(X_1, \\dots, X_n \\sim \\text{N}(\\mu, \\sigma^2)\\)\n\\[\nf(x_i; \\lambda) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "lectures/9a.html#optimization-1",
    "href": "lectures/9a.html#optimization-1",
    "title": "Sampling Distributions",
    "section": "Optimization",
    "text": "Optimization\nFor a given function \\(f(x)\\), optimization is the process of finding the value of \\(x\\) where \\(f(x)\\) is either the maximum or minimum."
  },
  {
    "objectID": "lectures/9a.html#finding-x",
    "href": "lectures/9a.html#finding-x",
    "title": "Sampling Distributions",
    "section": "Finding X",
    "text": "Finding X\nTo find \\(x\\),\n\\[\nf^\\prime (x) = 0\n\\]"
  },
  {
    "objectID": "lectures/9a.html#maximum-or-minimum",
    "href": "lectures/9a.html#maximum-or-minimum",
    "title": "Sampling Distributions",
    "section": "Maximum or Minimum",
    "text": "Maximum or Minimum\n\nMaximum\n\\[\nf^{\\prime\\prime} (X) &lt;0  \n\\]\n\nMinimum\n\\[\nf^{\\prime\\prime} (X) &lt;0  \n\\]"
  },
  {
    "objectID": "lectures/9a.html#example",
    "href": "lectures/9a.html#example",
    "title": "Sampling Distributions",
    "section": "Example",
    "text": "Example\n\\[\nf(x) = x^2 + 5x + 3\n\\]"
  },
  {
    "objectID": "lectures/9a.html#example-1",
    "href": "lectures/9a.html#example-1",
    "title": "Sampling Distributions",
    "section": "Example",
    "text": "Example\n\\[\nf(x) = -4(3x - 9)^2 + 12\n\\]"
  },
  {
    "objectID": "lectures/8a.html#introduction",
    "href": "lectures/8a.html#introduction",
    "title": "Sampling Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nGoal: Understand how sample statistics (like means or proportions) vary from sample to sample and how we can model this variability.\n\nWhy it matters: The concept of sampling distributions underlies statistical inference — including confidence intervals and hypothesis testing."
  },
  {
    "objectID": "lectures/8a.html#population-vs.-sample",
    "href": "lectures/8a.html#population-vs.-sample",
    "title": "Sampling Distributions",
    "section": "Population vs. Sample",
    "text": "Population vs. Sample"
  },
  {
    "objectID": "lectures/8a.html#population-vs.-sample-1",
    "href": "lectures/8a.html#population-vs.-sample-1",
    "title": "Sampling Distributions",
    "section": "Population vs. Sample",
    "text": "Population vs. Sample\n\n\n\nConcept\nPopulation\nSample\n\n\n\n\nDescription\nEntire group of interest\nSubset of the population\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nVariance\n\\(\\sigma^2\\)\n\\(s^2\\)\n\n\nProportion\n\\(p\\)\n\\(\\hat{p}\\)"
  },
  {
    "objectID": "lectures/8a.html#population-vs.-sample-2",
    "href": "lectures/8a.html#population-vs.-sample-2",
    "title": "Sampling Distributions",
    "section": "Population vs. Sample",
    "text": "Population vs. Sample\n\nParameter (Greek letters): Describes the population (usually unknown).\n\nStatistic (Roman letters): Describes the sample (used for inference)."
  },
  {
    "objectID": "lectures/8a.html#random-sample",
    "href": "lectures/8a.html#random-sample",
    "title": "Sampling Distributions",
    "section": "Random Sample",
    "text": "Random Sample\nLet \\(X_1, X_2, \\ldots, X_n\\) come from distribution function \\(f_X(x; \\theta)\\), if the following conditions are met:\n\nAll random variables are independent of each other\nAll random variables come from identical distributions\n\nThen the random variables \\(X_1, X_2, \\ldots, X_n\\) are said to independent and identically distributed (iid) and a random sample."
  },
  {
    "objectID": "lectures/8a.html#joint-distribution",
    "href": "lectures/8a.html#joint-distribution",
    "title": "Sampling Distributions",
    "section": "Joint Distribution",
    "text": "Joint Distribution\nIf \\(X_1, X_2, \\ldots, X_n\\) are iid of \\(f_{X}(x; \\theta)\\), then\n\\[\nf_{X_1, X_2, \\ldots, X_n}(x_1, x_2, \\ldots, x_n) = \\prod^n_{i=1}f_{X_i}(x_i)\n\\]"
  },
  {
    "objectID": "lectures/8a.html#what-are-statistics",
    "href": "lectures/8a.html#what-are-statistics",
    "title": "Sampling Distributions",
    "section": "What Are Statistics?",
    "text": "What Are Statistics?\nA statistic is a numerical value that describes a characteristic of a sample, (\\(\\theta\\)).\nFor MATH 352:\n&gt; A statistic is any function of the sample data that does not depend on unknown population parameters.\nStatistic Examples:\n\nSample mean: \\(\\bar{x} = \\frac{1}{n}\\sum x_i\\)\nSample variance: \\(s^2 = \\frac{1}{n-1}\\sum (x_i - \\bar{x})^2\\)\nSample proportion: \\(\\hat{p} = \\frac{x}{n}\\)"
  },
  {
    "objectID": "lectures/8a.html#types-of-statistics",
    "href": "lectures/8a.html#types-of-statistics",
    "title": "Sampling Distributions",
    "section": "Types of Statistics",
    "text": "Types of Statistics\nDescriptive Statistics\n\nSummarize or describe data.\nExamples: mean, median, mode, standard deviation, histograms.\n\nInferential Statistics\n\nUse samples to make generalizations about populations.\nExamples: confidence intervals, hypothesis tests, regression."
  },
  {
    "objectID": "lectures/8a.html#properties-of-a-good-statistic",
    "href": "lectures/8a.html#properties-of-a-good-statistic",
    "title": "Sampling Distributions",
    "section": "Properties of a Good Statistic",
    "text": "Properties of a Good Statistic\n\n\n\nProperty\nMeaning\n\n\n\n\nUnbiasedness\n\\(E[\\hat{\\theta}] = \\theta\\)\n\n\nConsistency\n\\(\\hat{\\theta_n} \\to \\theta\\) as \\(n \\to \\infty\\)\n\n\nEfficiency\nSmallest variance among unbiased estimators\n\n\nSufficiency\nUses all information in the data about \\(\\theta\\)\n\n\n\nExample: \\(\\bar{X}\\) is an unbiased and consistent estimator of \\(\\mu\\)."
  },
  {
    "objectID": "lectures/8a.html#sampling-distributions",
    "href": "lectures/8a.html#sampling-distributions",
    "title": "Sampling Distributions",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\nA sampling distribution is the probability distribution of a sample statistic (like \\(\\bar{X}\\)) based on all possible random samples of a given size \\(n\\).\n\nEach sample yields a different statistic.\nThe distribution of these statistics forms the sampling distribution."
  },
  {
    "objectID": "lectures/8a.html#sampling-distribution-of-the-mean",
    "href": "lectures/8a.html#sampling-distribution-of-the-mean",
    "title": "Sampling Distributions",
    "section": "Sampling Distribution of the Mean",
    "text": "Sampling Distribution of the Mean\nIf: - Population mean = \\(\\mu\\) - Population standard deviation = \\(\\sigma\\) - Sample size = \\(n\\)\nThen: - Mean of \\(\\bar{X}\\): \\(\\mu_{\\bar{X}} = \\mu\\) - Standard deviation (standard error): \\(\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "lectures/8a.html#convergence-concepts",
    "href": "lectures/8a.html#convergence-concepts",
    "title": "Sampling Distributions",
    "section": "Convergence Concepts",
    "text": "Convergence Concepts\nThe idea of convergence concepts is to undertand how the statistics behave as \\(n \\rightarrow \\infty\\). As the sample size gets large, statistics generally behave as known distribution functions.\nThere are 3 types of distributions:\n\nConvergence almost surely\nConvergence in probability\nConvergence in distribution"
  },
  {
    "objectID": "lectures/8a.html#convergence-almost-surely",
    "href": "lectures/8a.html#convergence-almost-surely",
    "title": "Sampling Distributions",
    "section": "Convergence almost surely",
    "text": "Convergence almost surely\n\\(X_1, X_2, \\ldots, X_n\\) converges almost surely to a random variable \\(X\\) if, for every \\(\\epsilon &gt; 0\\),\n\\[\nP(\\lim_{n\\rightarrow \\infty} |X_n - X| &lt; \\epsilon ) = 1\n\\]"
  },
  {
    "objectID": "lectures/8a.html#convergence-in-probability",
    "href": "lectures/8a.html#convergence-in-probability",
    "title": "Sampling Distributions",
    "section": "Convergence in probability",
    "text": "Convergence in probability\n\\(X_1, X_2, \\ldots, X_n\\) converges in a probability to a random variable \\(X\\) if, for every \\(\\epsilon &gt; 0\\),\n\\[\n\\lim_{n\\rightarrow \\infty}  P(|X_n - X| &lt; \\epsilon ) = 1\n\\]"
  },
  {
    "objectID": "lectures/8a.html#convergence-in-distribution",
    "href": "lectures/8a.html#convergence-in-distribution",
    "title": "Sampling Distributions",
    "section": "Convergence in distribution",
    "text": "Convergence in distribution\n\\(X_1, X_2, \\ldots, X_n\\) converges in distribution to a random variable \\(X\\) if,\n\\[\n\\lim_{n\\rightarrow \\infty}  F_{X_n}(x_n) = F_{X}(x)\n\\]"
  },
  {
    "objectID": "lectures/8a.html#the-law-of-large-numbers-lln",
    "href": "lectures/8a.html#the-law-of-large-numbers-lln",
    "title": "Sampling Distributions",
    "section": "The Law of Large Numbers (LLN)",
    "text": "The Law of Large Numbers (LLN)\nAs the sample size \\(n\\) increases, the sample mean \\(\\bar{X}\\) tends to get closer to the population mean \\(\\mu\\)."
  },
  {
    "objectID": "lectures/8a.html#the-law-of-large-numbers",
    "href": "lectures/8a.html#the-law-of-large-numbers",
    "title": "Sampling Distributions",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\n\\[\n\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\to \\mu \\text{ as } n \\to \\infty\n\\]\n\nRandomness averages out over many trials.\nThe sample mean stabilizes around the true mean."
  },
  {
    "objectID": "lectures/8a.html#two-versions-of-lln",
    "href": "lectures/8a.html#two-versions-of-lln",
    "title": "Sampling Distributions",
    "section": "Two Versions of LLN",
    "text": "Two Versions of LLN\n\n\n\n\n\n\n\n\nVersion\nType of Convergence\nDescription\n\n\n\n\nWeak Law\nIn probability\n\\(\\bar{X}_n\\) becomes probably close to \\(\\mu\\)\n\n\nStrong Law\nAlmost surely\n\\(\\bar{X}_n\\) converges to \\(\\mu\\) with probability 1"
  },
  {
    "objectID": "lectures/8a.html#lln-intuition-coin-flip-example",
    "href": "lectures/8a.html#lln-intuition-coin-flip-example",
    "title": "Sampling Distributions",
    "section": "LLN Intuition: Coin Flip Example",
    "text": "LLN Intuition: Coin Flip Example\n\nFlip a fair coin (\\(p = 0.5\\)).\nIf you flip 10 times → maybe 7 heads.\nIf you flip 1,000 times → about 500 heads.\nAs \\(n\\) grows, \\(\\hat{p} \\to p\\).\n\n\nThe Law of Large Numbers says that with enough data, sample proportions and means approximate population values."
  },
  {
    "objectID": "lectures/8a.html#central-limit-theorem-clt",
    "href": "lectures/8a.html#central-limit-theorem-clt",
    "title": "Sampling Distributions",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\nLet \\(X_1, X_2, \\ldots, X_n\\) be a sequence of iid random variables whose \\(E(X_i) = \\mu &lt; \\infty\\) and \\(Var(X_i) = \\sigma^2 &lt; \\infty\\). For \\(\\bar X_n = \\frac{1}{n} \\sum^n_{i=1} X_i\\):\n\\[\n\\frac{\\sqrt{n}(\\bar X_n - \\mu)}{\\sigma} \\rightarrow N(0,1)\n\\]\nas \\(n\\rightarrow \\infty\\)."
  },
  {
    "objectID": "lectures/8a.html#clt",
    "href": "lectures/8a.html#clt",
    "title": "Sampling Distributions",
    "section": "CLT",
    "text": "CLT\n\\[\n\\bar{X} \\overset{\\cdot}{\\sim} \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\n\nMean: \\(\\mu_{\\bar{X}} = \\mu\\)\nStandard deviation: \\(\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\\)\nShape: Approximately Normal for large \\(n\\)"
  },
  {
    "objectID": "lectures/8a.html#sampling-distribution-of-a-proportion",
    "href": "lectures/8a.html#sampling-distribution-of-a-proportion",
    "title": "Sampling Distributions",
    "section": "Sampling Distribution of a Proportion",
    "text": "Sampling Distribution of a Proportion\nFor a sample proportion \\(\\hat p\\) and sample size \\(n\\):\n\nMean: \\(\\mu_{\\hat{p}} = p\\)\nStandard deviation (standard error):\n\\[\n\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\n\\]\n\nNormal approximation holds if:\n\n\\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\)\n\n\\[\n\\hat p \\overset{\\cdot}{\\sim}IQ N(p,   \\frac{p(1-p)}{n})\n\\]"
  },
  {
    "objectID": "lectures/7a.html#learning-outcomes",
    "href": "lectures/7a.html#learning-outcomes",
    "title": "Joint Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMarginal Distributions\nConditional Distributions\nIndependence\nExpectations\nCovariance"
  },
  {
    "objectID": "lectures/7a.html#joint-distribution-functions",
    "href": "lectures/7a.html#joint-distribution-functions",
    "title": "Joint Distribution Functions",
    "section": "Joint Distribution Functions",
    "text": "Joint Distribution Functions\n\nA joint distribution of two random variables \\(X\\) and \\(Y\\) describes how they behave together.\n\nIt tells us the probability (discrete case) or probability density (continuous case) assigned to different pairs \\((x,y)\\).\n\nThink of it as a “probability landscape” in 2D:\n\nOn a grid for discrete outcomes.\n\nAs a surface over the plane for continuous outcomes."
  },
  {
    "objectID": "lectures/7a.html#discrete-joint-mass-function",
    "href": "lectures/7a.html#discrete-joint-mass-function",
    "title": "Joint Distribution Functions",
    "section": "Discrete Joint Mass Function",
    "text": "Discrete Joint Mass Function\nDefinition: If \\(X\\) and \\(Y\\) take values in countable sets \\(\\mathcal{X}, \\mathcal{Y}\\), then\n\\[\np_{X,Y}(x,y) = \\Pr(X=x, Y=y), \\quad (x,y)\\in \\mathcal{X}\\times \\mathcal{Y}.\n\\]\nProperties:\n\nNon-negativity: \\(p_{X,Y}(x,y) \\ge 0\\) for all \\((x,y)\\).\n\nNormalization:\n\\[\n\\sum_{x\\in \\mathcal{X}} \\sum_{y\\in \\mathcal{Y}} p_{X,Y}(x,y) = 1.\n\\]"
  },
  {
    "objectID": "lectures/7a.html#continuous-joint-density-function",
    "href": "lectures/7a.html#continuous-joint-density-function",
    "title": "Joint Distribution Functions",
    "section": "Continuous Joint Density Function",
    "text": "Continuous Joint Density Function\nDefinition: If \\((X,Y)\\) is continuous, it is described by a joint density \\(f_{X,Y}(x,y)\\) such that for any region \\(A \\subseteq \\mathbb{R}^2\\):\n\\[\n\\Pr((X,Y)\\in A) = \\iint_A f_{X,Y}(x,y)\\, dx\\,dy.\n\\]\nProperties:\n\nNon-negativity: \\(f_{X,Y}(x,y) \\ge 0\\) everywhere.\n\nNormalization:\n\\[\n\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty f_{X,Y}(x,y)\\,dx\\,dy = 1.\n\\]"
  },
  {
    "objectID": "lectures/7a.html#more-than-2-rvs",
    "href": "lectures/7a.html#more-than-2-rvs",
    "title": "Joint Distribution Functions",
    "section": "More than 2 RV’s",
    "text": "More than 2 RV’s\nLet \\(X_1, X_2, \\dots, X_n\\) be random variables.\n\nDiscrete case:\nThe joint pmf is\n\\[\np_{X_1,\\dots,X_n}(x_1,\\dots,x_n) = \\Pr(X_1=x_1, \\dots, X_n=x_n)\n\\]\nContinuous case:\nThe joint pdf is\n\\[\nf_{X_1,\\dots,X_n}(x_1,\\dots,x_n)\n\\]"
  },
  {
    "objectID": "lectures/7a.html#simulate-bivariate-normal",
    "href": "lectures/7a.html#simulate-bivariate-normal",
    "title": "Joint Distribution Functions",
    "section": "Simulate Bivariate Normal",
    "text": "Simulate Bivariate Normal"
  },
  {
    "objectID": "lectures/7a.html#bivariate-normal-distribution",
    "href": "lectures/7a.html#bivariate-normal-distribution",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Normal Distribution",
    "text": "Bivariate Normal Distribution\n\n\nCode\nlibrary(MASS)\nlibrary(ggplot2)\n\n# Parameters\nmu &lt;- c(0, 0)        # mean vector\nsigma_x &lt;- 2\nsigma_y &lt;- 1\nrho &lt;- 0.7           # correlation\nSigma &lt;- matrix(c(sigma_x^2, rho*sigma_x*sigma_y,\n                  rho*sigma_x*sigma_y, sigma_y^2), ncol = 2)\n\n# Simulate data\nset.seed(123)\nn &lt;- 2000\nXY &lt;- as.data.frame(mvrnorm(n = n, mu = mu, Sigma = Sigma))\nnames(XY) &lt;- c(\"X\", \"Y\")\n\n# Scatter plot with density contours\nggplot(XY, aes(x = X, y = Y)) +\n  geom_point(alpha = 0.3, color = \"steelblue\") +\n  stat_density_2d(aes(z = ..level..), bins = 8,\n                  geom = \"polygon\", alpha = 0.2, fill = \"tomato\") +\n  theme_bw(base_size = 14)"
  },
  {
    "objectID": "lectures/7a.html#marginal-density-functions",
    "href": "lectures/7a.html#marginal-density-functions",
    "title": "Joint Distribution Functions",
    "section": "Marginal Density Functions",
    "text": "Marginal Density Functions\nA Marginal Density Function is density function of one random variable from a random vector."
  },
  {
    "objectID": "lectures/7a.html#marginal-discrete-probability-mass-function",
    "href": "lectures/7a.html#marginal-discrete-probability-mass-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Discrete Probability Mass Function",
    "text": "Marginal Discrete Probability Mass Function\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe marginal distribution of \\(X_1\\) is defined as\n\\[\np_{X_1}(x_1) = \\sum_{x_2}p_{X_1,X_2}(x_1,x_2)\n\\]"
  },
  {
    "objectID": "lectures/7a.html#marginal-continuous-density-function",
    "href": "lectures/7a.html#marginal-continuous-density-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Continuous Density Function",
    "text": "Marginal Continuous Density Function\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The marginal distribution of \\(X_1\\) is defined as\n\\[\nf_{X_1}(x_1) = \\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2\n\\]"
  },
  {
    "objectID": "lectures/7a.html#example",
    "href": "lectures/7a.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf_{X,Y}(x,y) \\left\\{\\begin{array}{cc}\n2x & 0\\le y \\le 1;\\ 0 \\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(f_X(x)\\)"
  },
  {
    "objectID": "lectures/7a.html#conditional-distributions-1",
    "href": "lectures/7a.html#conditional-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable."
  },
  {
    "objectID": "lectures/7a.html#discrete-conditional-distributions",
    "href": "lectures/7a.html#discrete-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Discrete Conditional Distributions",
    "text": "Discrete Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/7a.html#continuous-conditional-distributions",
    "href": "lectures/7a.html#continuous-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Continuous Conditional Distributions",
    "text": "Continuous Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2=_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/7a.html#example-1",
    "href": "lectures/7a.html#example-1",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\nLet the joint density function of \\(X_1\\) and \\(X_2\\) be defined as\n\\[\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\nFind the conditional density function of \\(X_2|X_1=x_1\\)."
  },
  {
    "objectID": "lectures/7a.html#independent-random-variables",
    "href": "lectures/7a.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/7a.html#discrete-independent-random-variables",
    "href": "lectures/7a.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/7a.html#continuous-independent-random-variables",
    "href": "lectures/7a.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/7a.html#matrix-algebra",
    "href": "lectures/7a.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/7a.html#example-2",
    "href": "lectures/7a.html#example-2",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\nShow that \\(X\\perp Y\\).\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/7a.html#expectations-1",
    "href": "lectures/7a.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/7a.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/7a.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/7a.html#conditional-expectations",
    "href": "lectures/7a.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/7a.html#conditional-expectations-1",
    "href": "lectures/7a.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/7a.html#covariance-1",
    "href": "lectures/7a.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(\\mu_1\\) and \\(\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/7a.html#correlation",
    "href": "lectures/7a.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/5b.html#learning-outcomes",
    "href": "lectures/5b.html#learning-outcomes",
    "title": "Expected Values",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExpectations\nVariance\nProperties"
  },
  {
    "objectID": "lectures/5b.html#expected-value",
    "href": "lectures/5b.html#expected-value",
    "title": "Expected Values",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value for a continuous distribution is defined as\n\\[\nE(X)=\\int x f(x)dx\n\\]\nThe expectation of a function \\(g(X)\\) is defined as\n\\[\nE\\{g(X)\\}=\\int g(x)f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/5b.html#expected-value-properties",
    "href": "lectures/5b.html#expected-value-properties",
    "title": "Expected Values",
    "section": "Expected Value Properties",
    "text": "Expected Value Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(X)\\}=cE\\{g(X)\\}\\)\n\\(E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}\\)"
  },
  {
    "objectID": "lectures/5b.html#variance",
    "href": "lectures/5b.html#variance",
    "title": "Expected Values",
    "section": "Variance",
    "text": "Variance\nThe variance of continuous variable is defined as\n\\[\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/5b.html#variance-properties",
    "href": "lectures/5b.html#variance-properties",
    "title": "Expected Values",
    "section": "Variance Properties",
    "text": "Variance Properties\n\\(Y=aX+b\\)\n\n\\[\n\\begin{array}{lll}\n\\mathrm{Var}(Y) & = & Var(aX+b) \\\\\n& = & Var(aX) + Var(b) \\\\\n& = & a^2Var(X)\n\\end{array}\n\\]"
  },
  {
    "objectID": "lectures/5b.html#expected-value-1",
    "href": "lectures/5b.html#expected-value-1",
    "title": "Expected Values",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/5b.html#variance-1",
    "href": "lectures/5b.html#variance-1",
    "title": "Expected Values",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/5b.html#expected-value-2",
    "href": "lectures/5b.html#expected-value-2",
    "title": "Expected Values",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/5b.html#variance-2",
    "href": "lectures/5b.html#variance-2",
    "title": "Expected Values",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/5b.html#expected-value-3",
    "href": "lectures/5b.html#expected-value-3",
    "title": "Expected Values",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{Beta}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)"
  },
  {
    "objectID": "lectures/5b.html#variance-3",
    "href": "lectures/5b.html#variance-3",
    "title": "Expected Values",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{Beta}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)"
  },
  {
    "objectID": "lectures/5b.html#expected-value-4",
    "href": "lectures/5b.html#expected-value-4",
    "title": "Expected Values",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/5b.html#variance-4",
    "href": "lectures/5b.html#variance-4",
    "title": "Expected Values",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/4a.html#learning-outcomes",
    "href": "lectures/4a.html#learning-outcomes",
    "title": "Moment Generating Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMoments\nMoment Generating Functions\nProperties"
  },
  {
    "objectID": "lectures/4a.html#expected-values",
    "href": "lectures/4a.html#expected-values",
    "title": "Moment Generating Functions",
    "section": "Expected Values",
    "text": "Expected Values\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of \\(Y\\) is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]\nwhere \\(P(y)\\) is the PMF of \\(Y\\)."
  },
  {
    "objectID": "lectures/4a.html#moments",
    "href": "lectures/4a.html#moments",
    "title": "Moment Generating Functions",
    "section": "Moments",
    "text": "Moments\nMoments are numerical measures that describe the shape and characteristics of a random variable’s probability distribution.\n\nThe \\(k\\)th moment is described as\n\\[\nE(X^k)\n\\]"
  },
  {
    "objectID": "lectures/4a.html#moment-generating-functions",
    "href": "lectures/4a.html#moment-generating-functions",
    "title": "Moment Generating Functions",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(t) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(t\\), and setting \\(t\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(t)}{dt^k}\\Bigg|_{c=0}\n\\]"
  },
  {
    "objectID": "lectures/4a.html#mgf",
    "href": "lectures/4a.html#mgf",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/4a.html#ey2",
    "href": "lectures/4a.html#ey2",
    "title": "Moment Generating Functions",
    "section": "\\(E(Y^2)\\)",
    "text": "\\(E(Y^2)\\)"
  },
  {
    "objectID": "lectures/4a.html#mgf-1",
    "href": "lectures/4a.html#mgf-1",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/4a.html#ey2-1",
    "href": "lectures/4a.html#ey2-1",
    "title": "Moment Generating Functions",
    "section": "\\(E(Y^2)\\)",
    "text": "\\(E(Y^2)\\)"
  },
  {
    "objectID": "lectures/4a.html#properties",
    "href": "lectures/4a.html#properties",
    "title": "Moment Generating Functions",
    "section": "Properties",
    "text": "Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(Y)\\}=cE\\{g(Y)\\}\\)\n\\(E\\{g_1(Y)+\\cdots+g_n(Y)\\}=E\\{g_1(Y)\\}+\\cdots+E\\{g_n(Y)\\}\\)"
  },
  {
    "objectID": "lectures/3a.html#learning-outcomes",
    "href": "lectures/3a.html#learning-outcomes",
    "title": "Discrete Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDiscrete Random Variables\n\nObtain Probabilities"
  },
  {
    "objectID": "lectures/3a.html#discrete-random-variables-1",
    "href": "lectures/3a.html#discrete-random-variables-1",
    "title": "Discrete Random Variables",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values.\n\n\n\n\n\n\n\n\n\nDistribution\nParameter(s)\nPMF \\(P(Y=y)\\)\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p^y(1-p)^{1-y}\\)\n\n\nBinomial\n\\(n\\) and \\(p\\)\n\\((^n_y)p^y(1-p)^{n-y}\\)\n\n\nGeometric\n\\(p\\)\n\\((1-p)^{y-1}p\\)\n\n\nNegative Binomial\n\\(r\\) and \\(p\\)\n\\((^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{\\lambda^y}{y!} e^{-\\lambda}\\)"
  },
  {
    "objectID": "lectures/3a.html#probability-mass-function",
    "href": "lectures/3a.html#probability-mass-function",
    "title": "Discrete Random Variables",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\)."
  },
  {
    "objectID": "lectures/3a.html#cumulative-density-function",
    "href": "lectures/3a.html#cumulative-density-function",
    "title": "Discrete Random Variables",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\)."
  },
  {
    "objectID": "lectures/3a.html#bernoulli-distribution-1",
    "href": "lectures/3a.html#bernoulli-distribution-1",
    "title": "Discrete Random Variables",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nA Bernoulli distribution is probability of having a success out of two outcomes. In essence, a coin flip with probability of success \\(p\\)."
  },
  {
    "objectID": "lectures/3a.html#binomial-distribution-1",
    "href": "lectures/3a.html#binomial-distribution-1",
    "title": "Discrete Random Variables",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nFor a given sample, the number of successes represents a binomial distribution. Binary outcomes are represented with with a Bernoulli experiment. The sum of Bernoulli experiments represents the binomial distribution."
  },
  {
    "objectID": "lectures/3a.html#binomial-distribution-2",
    "href": "lectures/3a.html#binomial-distribution-2",
    "title": "Discrete Random Variables",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nFor a binomial experiment, the following must be satisfied:\n\n\nThere is a fixed \\(n\\) trials\nThe there are two outcomes for each trial\nThe probability of success (\\(p\\)) is constant for each trial\nThe trials are independent of each other"
  },
  {
    "objectID": "lectures/3a.html#pmf",
    "href": "lectures/3a.html#pmf",
    "title": "Discrete Random Variables",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/3a.html#example",
    "href": "lectures/3a.html#example",
    "title": "Discrete Random Variables",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.3. An arborist decides to plant 8 seeds in the community. What is the probability that 3 to 5 seeds will germinate?"
  },
  {
    "objectID": "lectures/3a.html#example-1",
    "href": "lectures/3a.html#example-1",
    "title": "Discrete Random Variables",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.6. An arborist decides to plant 8 seeds in the community. What is the probability that at least 1 seed germinates?"
  },
  {
    "objectID": "lectures/3a.html#example-2",
    "href": "lectures/3a.html#example-2",
    "title": "Discrete Random Variables",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.8. An arborist decides to plant 6 seeds in the community. What is the probability that an even number of seed will germinate?"
  },
  {
    "objectID": "lectures/3a.html#poisson-distribution-1",
    "href": "lectures/3a.html#poisson-distribution-1",
    "title": "Discrete Random Variables",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe poisson distribution describes an experiment that measures that occurrence of an event at specific point and/or time period."
  },
  {
    "objectID": "lectures/3a.html#pmf-1",
    "href": "lectures/3a.html#pmf-1",
    "title": "Discrete Random Variables",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/3a.html#example-3",
    "href": "lectures/3a.html#example-3",
    "title": "Discrete Random Variables",
    "section": "Example",
    "text": "Example\nThe number of industrial accidents at a particular manufacturing plant is found to be an average of 1 accident per month. In the previous month, there were 3 accidents. What is the probability of observing 3 or more accidents in a given month?"
  },
  {
    "objectID": "lectures/3a.html#example-4",
    "href": "lectures/3a.html#example-4",
    "title": "Discrete Random Variables",
    "section": "Example",
    "text": "Example\nThe number of industrial accidents at a particular manufacturing plant is found to be an average of 8 accidents per year. What is the probability of observing 15 or more accidents in a given year?"
  },
  {
    "objectID": "lectures/3a.html#geometric-distribution-1",
    "href": "lectures/3a.html#geometric-distribution-1",
    "title": "Discrete Random Variables",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\nA geometric distribution models the number of Bernoulli trials until the first success (or failure)."
  },
  {
    "objectID": "lectures/3a.html#pmf-2",
    "href": "lectures/3a.html#pmf-2",
    "title": "Discrete Random Variables",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/3a.html#example-5",
    "href": "lectures/3a.html#example-5",
    "title": "Discrete Random Variables",
    "section": "Example",
    "text": "Example\nSuppose 30% of the applicants for a certain industrial job possess the necessary skills. What is the probability that the first applicant with the necessary skills is found on the 5th interview."
  },
  {
    "objectID": "lectures/3a.html#negative-binomial-distribution-1",
    "href": "lectures/3a.html#negative-binomial-distribution-1",
    "title": "Discrete Random Variables",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\nA negative binomial distribution models the number of successful Bernoulli trials until the \\(r\\)th failure."
  },
  {
    "objectID": "lectures/3a.html#pmf-3",
    "href": "lectures/3a.html#pmf-3",
    "title": "Discrete Random Variables",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/3a.html#example-6",
    "href": "lectures/3a.html#example-6",
    "title": "Discrete Random Variables",
    "section": "Example",
    "text": "Example\nTen percent of engines manufactured on an assembly line are defective. If engines are randomly selected one at a time and tested, what is the probability that third nondefective engine is found on the 4th trial."
  },
  {
    "objectID": "lectures/2a.html#learning-outcomes",
    "href": "lectures/2a.html#learning-outcomes",
    "title": "Introduction to Probability",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDescribe disjoint events\nDescribe a conditional probability\nDefine independent events\nLaw of Total Probability\nBaye’s Theorem"
  },
  {
    "objectID": "lectures/2a.html#disjoint-events-1",
    "href": "lectures/2a.html#disjoint-events-1",
    "title": "Introduction to Probability",
    "section": "Disjoint Events",
    "text": "Disjoint Events\nTwo events A and B are considered disjoint if \\(\\Pr(A\\cap B)=0\\). In general terms, only one event can occur, not both."
  },
  {
    "objectID": "lectures/2a.html#conditional-probability-1",
    "href": "lectures/2a.html#conditional-probability-1",
    "title": "Introduction to Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nLet there be 2 events A and B. Given that B has occurred, what is the probability that A occurs? The conditional probability requires there to be at least 2 events and one event must have occurred. Additionally, the events cannot be disjoint. Conditional probabilities are denoted as \\(\\Pr(A|B)\\), the probability of A given B has occurred.\n\\[\n\\Pr(A|B)=\\frac{\\Pr(A\\cap B)}{\\Pr(B)}\n\\]"
  },
  {
    "objectID": "lectures/2a.html#example-1-drawing-cards",
    "href": "lectures/2a.html#example-1-drawing-cards",
    "title": "Introduction to Probability",
    "section": "Example 1: Drawing Cards",
    "text": "Example 1: Drawing Cards\nWe have a standard deck of 52 cards.\nLet’s find the probability of drawing an ace, given that the card is a spade."
  },
  {
    "objectID": "lectures/2a.html#example-2-rolling-dice",
    "href": "lectures/2a.html#example-2-rolling-dice",
    "title": "Introduction to Probability",
    "section": "Example 2: Rolling Dice",
    "text": "Example 2: Rolling Dice\nTwo fair dice are rolled.\nFind the probability that the sum is 10, given that the first die shows a 6."
  },
  {
    "objectID": "lectures/2a.html#example-3-medical-test",
    "href": "lectures/2a.html#example-3-medical-test",
    "title": "Introduction to Probability",
    "section": "Example 3: Medical Test",
    "text": "Example 3: Medical Test\nA certain disease affects 1% of the population.\nA test correctly identifies the disease 95% of the time, but has a 5% false-positive rate.\nFind the probability that someone has the disease given a positive test result."
  },
  {
    "objectID": "lectures/2a.html#example-4-eye-glasses",
    "href": "lectures/2a.html#example-4-eye-glasses",
    "title": "Introduction to Probability",
    "section": "Example 4: Eye Glasses",
    "text": "Example 4: Eye Glasses\n\n\n\n\nUses Eye Glasses\n\n\n\nNeeds Glasses\nYes\nNo\n\n\nYes\n44\n14\n\n\nNo\n2\n40\n\n\n\n\nFind the probability of needing glasses\nFind the probability of not using glasses\nFind the probability of not using glasses and needing glasses\nFind the probability of not using glasses, given they need glasses"
  },
  {
    "objectID": "lectures/2a.html#law-of-total-probability-1",
    "href": "lectures/2a.html#law-of-total-probability-1",
    "title": "Introduction to Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\nThe law of total probability allows you to compute the probability of an event A given that the sets {\\(B_1\\), … , \\(B_n\\)} partitions event A. The law of total probability is given as\n\\[\n\\Pr(A)= \\sum^n_{i=1}\\Pr(A\\cap B_i)\n\\]\n\n\\[\n\\Pr(A)=\\sum^n_{i=1}\\Pr(A|B_i)\\Pr(B_i)\n\\]"
  },
  {
    "objectID": "lectures/2a.html#diagrams",
    "href": "lectures/2a.html#diagrams",
    "title": "Introduction to Probability",
    "section": "Diagrams",
    "text": "Diagrams"
  },
  {
    "objectID": "lectures/2a.html#example-1-weather-and-rain-forecast",
    "href": "lectures/2a.html#example-1-weather-and-rain-forecast",
    "title": "Introduction to Probability",
    "section": "Example 1: Weather and Rain Forecast",
    "text": "Example 1: Weather and Rain Forecast\nSuppose the weather forecast is either Sunny (S) or Cloudy (C).\n\n\\(P(S) = 0.7\\), \\(P(C) = 0.3\\)\n\nProbability of rain given sunny: \\(P(R \\mid S) = 0.1\\)\n\nProbability of rain given cloudy: \\(P(R \\mid C) = 0.5\\)\n\nWhat is the probability of rain?"
  },
  {
    "objectID": "lectures/2a.html#example-2-disease",
    "href": "lectures/2a.html#example-2-disease",
    "title": "Introduction to Probability",
    "section": "Example 2: Disease",
    "text": "Example 2: Disease\nThe probability of an individual having a disease, given that they test positive for the disease, is 0.82. The probability of an individual having a disease, given they tested negative, is 0.14. The probability of testing positive is 0.6. What is the prevalence of a disease (probability of having a disease)?"
  },
  {
    "objectID": "lectures/2a.html#independent-events-1",
    "href": "lectures/2a.html#independent-events-1",
    "title": "Introduction to Probability",
    "section": "Independent Events",
    "text": "Independent Events\nEvents A and B are considered independent if \\(\\Pr(A\\cap B)=\\Pr(A)\\Pr(B)\\). In other words, The occurrence of one event will not have an effect on the occurrence of the other event."
  },
  {
    "objectID": "lectures/2a.html#example-1-eye-glasses",
    "href": "lectures/2a.html#example-1-eye-glasses",
    "title": "Introduction to Probability",
    "section": "Example 1: Eye Glasses",
    "text": "Example 1: Eye Glasses\n\n\n\n\nUses Eye Glasses\n\n\n\n\n\nNeeds Glasses\nYes\nNo\n\n\nYes\n44\n14\n\n\nNo\n2\n40"
  },
  {
    "objectID": "lectures/2a.html#example-2-drawing-with-replacement",
    "href": "lectures/2a.html#example-2-drawing-with-replacement",
    "title": "Introduction to Probability",
    "section": "Example 2: Drawing with Replacement",
    "text": "Example 2: Drawing with Replacement\nA jar contains 5 red balls and 3 blue balls (8 total).\nWe draw two balls with replacement."
  },
  {
    "objectID": "lectures/2a.html#example-3-drawing-without-replacement",
    "href": "lectures/2a.html#example-3-drawing-without-replacement",
    "title": "Introduction to Probability",
    "section": "Example 3: Drawing without Replacement",
    "text": "Example 3: Drawing without Replacement\nA jar contains 5 red balls and 3 blue balls (8 total).\nWe draw two balls with replacement."
  },
  {
    "objectID": "lectures/2a.html#bayes-theorem-1",
    "href": "lectures/2a.html#bayes-theorem-1",
    "title": "Introduction to Probability",
    "section": "Baye’s Theorem",
    "text": "Baye’s Theorem\nBaye’s theorem computes the probability of an event \\(B_i\\) given event A\n\\[\n\\Pr(B_i|A) = \\frac{\\Pr(A\\cap B_i)}{\\Pr(A)}=\\frac{\\Pr(A|B_i)\\Pr(B_i)}{\\sum^n_{i=1}\\Pr(A|B_i)\\Pr(B_i)}\n\\]"
  },
  {
    "objectID": "lectures/2a.html#example-1-factory-machines",
    "href": "lectures/2a.html#example-1-factory-machines",
    "title": "Introduction to Probability",
    "section": "Example 1: Factory Machines",
    "text": "Example 1: Factory Machines\nA factory has three machines producing items:\n\nMachine A produces 40% of items, with a defect rate of 2%.\n\nMachine B produces 35% of items, with a defect rate of 3%.\n\nMachine C produces 25% of items, with a defect rate of 5%.\n\nIf an item is defective, what is the probability it came from Machine C?"
  },
  {
    "objectID": "lectures/2a.html#example-2-email-spam-filtering",
    "href": "lectures/2a.html#example-2-email-spam-filtering",
    "title": "Introduction to Probability",
    "section": "Example 2: Email Spam Filtering",
    "text": "Example 2: Email Spam Filtering\nSuppose 80% of emails are legitimate, 20% are spam. A keyword appears in 60% of spam emails but in only 5% of legitimate emails. If an email contains the keyword, what is the probability it is spam?"
  },
  {
    "objectID": "lectures/2a.html#example-3-disease",
    "href": "lectures/2a.html#example-3-disease",
    "title": "Introduction to Probability",
    "section": "Example 3: Disease",
    "text": "Example 3: Disease\nThe probability of an having a disease, given that they test positive for the disease, is 0.82. The probability of and individual having a disease, given they tested negative, is 0.14. The probability of testing positive is 0.6. What is the probability of resulting in a false negative?"
  },
  {
    "objectID": "lectures/1b.html#learning-objectives",
    "href": "lectures/1b.html#learning-objectives",
    "title": "Introduction to Probability",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine sample space and experiment\nDefine probabilities\nDefine random variable and distribution function"
  },
  {
    "objectID": "lectures/1b.html#experiments-and-outcomes",
    "href": "lectures/1b.html#experiments-and-outcomes",
    "title": "Introduction to Probability",
    "section": "Experiments and Outcomes",
    "text": "Experiments and Outcomes\n\nA random experiment is a process with uncertain results but well-defined possible outcomes."
  },
  {
    "objectID": "lectures/1b.html#examples",
    "href": "lectures/1b.html#examples",
    "title": "Introduction to Probability",
    "section": "Examples",
    "text": "Examples\n\nTossing a coin → outcomes: {Heads, Tails}\n\nRolling a die → outcomes: {1, 2, 3, 4, 5, 6}\n\nDrawing a card → outcomes: {all 52 cards in a deck}"
  },
  {
    "objectID": "lectures/1b.html#sample-space",
    "href": "lectures/1b.html#sample-space",
    "title": "Introduction to Probability",
    "section": "Sample Space",
    "text": "Sample Space\n\nThe sample space (denoted \\(S\\)) is the set of all possible outcomes of an experiment."
  },
  {
    "objectID": "lectures/1b.html#examples-1",
    "href": "lectures/1b.html#examples-1",
    "title": "Introduction to Probability",
    "section": "Examples",
    "text": "Examples\n\nCoin toss: \\(S = \\{H, T\\}\\)\nDie roll: \\(S = \\{1, 2, 3, 4, 5, 6\\}\\)\n\nTwo coin tosses: \\(S = \\{HH, HT, TH, TT\\}\\)"
  },
  {
    "objectID": "lectures/1b.html#events",
    "href": "lectures/1b.html#events",
    "title": "Introduction to Probability",
    "section": "Events",
    "text": "Events\n\nAn event is any subset of the sample space.\n\nExamples:\n- Event A: “Die shows an even number”\n\\(A = \\{2,4,6\\}\\)\n\nEvent B: “At least one Head in two tosses”\n\\(B = \\{HH, HT, TH\\}\\)"
  },
  {
    "objectID": "lectures/1b.html#certain-and-impossible-events",
    "href": "lectures/1b.html#certain-and-impossible-events",
    "title": "Introduction to Probability",
    "section": "Certain and Impossible Events",
    "text": "Certain and Impossible Events\n\nCertain event: The sample space itself \\(S\\) (probability = 1)\nImpossible event: The empty set \\(\\varnothing\\) (probability = 0)"
  },
  {
    "objectID": "lectures/1b.html#event-operations",
    "href": "lectures/1b.html#event-operations",
    "title": "Introduction to Probability",
    "section": "Event Operations",
    "text": "Event Operations\nSince events are subsets of the sample space, we can use set theory:\n\nUnion: \\(A \\cup B\\) → either A or B occurs\n\nIntersection: \\(A \\cap B\\) → both A and B occur\n\nComplement: \\(A^c\\) → event A does not occur"
  },
  {
    "objectID": "lectures/1b.html#probability-axioms",
    "href": "lectures/1b.html#probability-axioms",
    "title": "Introduction to Probability",
    "section": "Probability Axioms",
    "text": "Probability Axioms\nFor any event \\(A\\):\n\n\\(0\\leq \\Pr(A) \\leq 1\\)\n\\(\\Pr(S) = 1\\) where \\(S\\) is the sample space."
  },
  {
    "objectID": "lectures/1b.html#complement-rule",
    "href": "lectures/1b.html#complement-rule",
    "title": "Introduction to Probability",
    "section": "Complement Rule",
    "text": "Complement Rule\nThe complement of \\(A\\) (denoted \\(A^c\\)) is “not \\(A\\)”.\n\\[\\Pr(A^c) = 1 - \\Pr(A)\\]"
  },
  {
    "objectID": "lectures/1b.html#example",
    "href": "lectures/1b.html#example",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nIf the probability of rain is 0.3,\nthen the probability of no rain is:"
  },
  {
    "objectID": "lectures/1b.html#addition-rule",
    "href": "lectures/1b.html#addition-rule",
    "title": "Introduction to Probability",
    "section": "Addition Rule",
    "text": "Addition Rule\nFor any two events \\(A\\) and \\(B\\):\n\\[\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\]"
  },
  {
    "objectID": "lectures/1b.html#disjoint-events-mutually-exclusive",
    "href": "lectures/1b.html#disjoint-events-mutually-exclusive",
    "title": "Introduction to Probability",
    "section": "Disjoint Events (mutually exclusive)",
    "text": "Disjoint Events (mutually exclusive)\nIf \\(A\\) and \\(B\\) cannot happen together,\n\\[ \\Pr(A \\cap B) = 0 \\]\nThen:\n\\[ \\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) \\]"
  },
  {
    "objectID": "lectures/1b.html#multiplication-rule",
    "href": "lectures/1b.html#multiplication-rule",
    "title": "Introduction to Probability",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\nFor any two events \\(A\\) and \\(B\\):\n\\[ \\Pr(A \\cap B) = \\Pr(A \\mid B)\\Pr(B) \\]\nor equivalently,\n\\[ \\Pr(A \\cap B) = \\Pr(B \\mid A)\\Pr(A) \\]"
  },
  {
    "objectID": "lectures/1b.html#conditional-probability",
    "href": "lectures/1b.html#conditional-probability",
    "title": "Introduction to Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nThe probability of (A) given that (B) occurred:\n\\[ \\Pr(A \\mid B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}, \\quad \\Pr(B) &gt; 0 \\]"
  },
  {
    "objectID": "lectures/1b.html#example-1",
    "href": "lectures/1b.html#example-1",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nIf 30% of students play soccer, 20% play basketball,\nand 10% play both:"
  },
  {
    "objectID": "lectures/1b.html#summary",
    "href": "lectures/1b.html#summary",
    "title": "Introduction to Probability",
    "section": "Summary",
    "text": "Summary\n\nComplement Rule:\n\\[ \\Pr(A^c) = 1 - \\Pr(A) \\]\nAddition Rule:\n\\[ \\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B) \\]\nMultiplication Rule:\n\\[ \\Pr(A \\cap B) = \\Pr(A \\mid B)\\Pr(B) \\]\nConditional Probability:\n\\[ \\Pr(A \\mid B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)} \\]"
  },
  {
    "objectID": "lectures/1b.html#example-3",
    "href": "lectures/1b.html#example-3",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nSuppose we want to understand the efficacy of a test for a certain disease. Consider the following table:\n\n\n\n\n\nDisease\nPresence\nTotal\n\n\n\n\n\n\nYes\nNo\n\n\n\nTest Result\nYes\n42\n6\n\n\n\n\nNo\n17\n35\n\n\n\n\n\n\n\n100"
  },
  {
    "objectID": "lectures/1b.html#example-4",
    "href": "lectures/1b.html#example-4",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\nFind the probability that an individual has a disease\nFind the probability that an individual tests negative for a disease\nFind the probability for someone who has the disease, given that they test positive.\nFind the probability that the test gives an accurate result\nFind the probability that the test gives and inaccurate result"
  },
  {
    "objectID": "lectures/13b.html#learning-objectives",
    "href": "lectures/13b.html#learning-objectives",
    "title": "Standard Errors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nStandard Errors\n\nLinear Regression\nGLM\n\nSampling Distributions"
  },
  {
    "objectID": "lectures/13b.html#standard-errors",
    "href": "lectures/13b.html#standard-errors",
    "title": "Standard Errors",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nFind the variance of the estimate\nFind the information matrix\nUse for Inference"
  },
  {
    "objectID": "lectures/13b.html#finding-the-variance",
    "href": "lectures/13b.html#finding-the-variance",
    "title": "Standard Errors",
    "section": "Finding the Variance",
    "text": "Finding the Variance"
  },
  {
    "objectID": "lectures/13b.html#estimate-for-sigma2",
    "href": "lectures/13b.html#estimate-for-sigma2",
    "title": "Standard Errors",
    "section": "Estimate for \\(\\sigma^2\\)",
    "text": "Estimate for \\(\\sigma^2\\)\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum^n_{i=1} (Y_i-\\boldsymbol X_i^\\mathrm T\\hat{\\boldsymbol \\beta})^2\n\\]"
  },
  {
    "objectID": "lectures/13b.html#standard-errors-of-betas",
    "href": "lectures/13b.html#standard-errors-of-betas",
    "title": "Standard Errors",
    "section": "Standard Errors of \\(\\beta\\)’s",
    "text": "Standard Errors of \\(\\beta\\)’s\n\\[\nSE(\\hat\\beta_0)=\\sqrt{\\frac{\\sum^n_{i=1}x_i^2\\hat\\sigma^2}{n\\sum^n_{i=1}(x_i-\\bar x)^2}}\n\\]\n\\[\nSE(\\hat\\beta_1)=\\sqrt\\frac{\\hat\\sigma^2}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\]"
  },
  {
    "objectID": "lectures/13b.html#standard-errors-matrix-form",
    "href": "lectures/13b.html#standard-errors-matrix-form",
    "title": "Standard Errors",
    "section": "Standard Errors Matrix Form",
    "text": "Standard Errors Matrix Form\n\\[\nVar(\\hat {\\boldsymbol \\beta}) = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1} \\hat \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/13b.html#large-sample-theory",
    "href": "lectures/13b.html#large-sample-theory",
    "title": "Standard Errors",
    "section": "Large Sample Theory",
    "text": "Large Sample Theory\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be the MLE estimator for a parameter \\(\\theta\\). As \\(n\\rightarrow\\infty\\), then \\(\\hat \\theta\\) has a normal distribution with mean \\(\\theta\\) and variance \\(1/nI(\\theta)\\), where\n\\[\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right]\n\\]"
  },
  {
    "objectID": "lectures/13b.html#sampling-distributions-1",
    "href": "lectures/13b.html#sampling-distributions-1",
    "title": "Standard Errors",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\n\n\\(\\phi\\) known\n\\[\n\\frac{\\hat\\beta_j - \\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]\n\n\\(\\phi\\) unknown\n\\[\n\\frac{\\hat\\beta_j-\\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]"
  },
  {
    "objectID": "lectures/12a.html#learning-outcomes",
    "href": "lectures/12a.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExponential Family of Distributions\nGeneralized Linear Models"
  },
  {
    "objectID": "lectures/12a.html#exponential-family-of-distributions-1",
    "href": "lectures/12a.html#exponential-family-of-distributions-1",
    "title": "Generalized Linear Models",
    "section": "Exponential Family of Distributions",
    "text": "Exponential Family of Distributions\nAn exponential family of distributions are random variables that allow their probability density function to have the following form:\n\\[\nf(y; \\theta,\\phi) = a(y,\\phi)\\exp\\left\\{\\frac{y\\theta-\\kappa(\\theta)}{\\phi}\\right\\}\n\\]\n\n\\(\\theta\\): is the canonical parameter (also a function of other parameters)\n\\(\\kappa(\\theta)\\): is a known cumulant function\n\\(\\phi&gt;0\\): dispersion parameter function\n\\(a(y,\\phi)\\): normalizing constant"
  },
  {
    "objectID": "lectures/12a.html#canonical-parameter",
    "href": "lectures/12a.html#canonical-parameter",
    "title": "Generalized Linear Models",
    "section": "Canonical Parameter",
    "text": "Canonical Parameter\nThe canonical parameter represents the relationship between the random variable and the \\(E(Y)=\\mu\\)"
  },
  {
    "objectID": "lectures/12a.html#normal-distribution",
    "href": "lectures/12a.html#normal-distribution",
    "title": "Generalized Linear Models",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\\[\nf(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "lectures/12a.html#binomial-distribution",
    "href": "lectures/12a.html#binomial-distribution",
    "title": "Generalized Linear Models",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\\[\nf(x;n,p)=\\big(^n_x\\big) p^x(1-p)^{n-x}\n\\]"
  },
  {
    "objectID": "lectures/12a.html#poisson-distribution",
    "href": "lectures/12a.html#poisson-distribution",
    "title": "Generalized Linear Models",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\\[\nf(x;\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\n\\]"
  },
  {
    "objectID": "lectures/12a.html#common-distributions-and-canonical-parameters",
    "href": "lectures/12a.html#common-distributions-and-canonical-parameters",
    "title": "Generalized Linear Models",
    "section": "Common Distributions and Canonical Parameters",
    "text": "Common Distributions and Canonical Parameters\n\n\n\nRandom Variable\nCanonical Parameter\n\n\n\n\nNormal\n\\(\\mu\\)\n\n\nBinomial\n\\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\n\nNegative Binomial\n\\(\\log\\left(\\frac{\\mu}{\\mu+k}\\right)\\)\n\n\nPoisson\n\\(\\log(\\mu)\\)\n\n\nGamma\n\\(-\\frac{1}{\\mu}\\)\n\n\nInverse Gaussian\n\\(-\\frac{1}{2\\mu^2}\\)"
  },
  {
    "objectID": "lectures/12a.html#generalized-linear-models-1",
    "href": "lectures/12a.html#generalized-linear-models-1",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA generalized linear model (GLM) is used to model the association between an outcome variable (of any data type) and a set of predictor values. We estimate a set of regression coefficients \\(\\boldsymbol \\beta\\) to explain how each predictor is related to the expected value of the outcome."
  },
  {
    "objectID": "lectures/12a.html#generalized-linear-models-2",
    "href": "lectures/12a.html#generalized-linear-models-2",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA GLM is composed of a systematic and random component."
  },
  {
    "objectID": "lectures/12a.html#random-component",
    "href": "lectures/12a.html#random-component",
    "title": "Generalized Linear Models",
    "section": "Random Component",
    "text": "Random Component\nThe random component is the random variable that defines the randomness and variation of the outcome variable."
  },
  {
    "objectID": "lectures/12a.html#systematic-component",
    "href": "lectures/12a.html#systematic-component",
    "title": "Generalized Linear Models",
    "section": "Systematic Component",
    "text": "Systematic Component\nThe systematic component is the linear model that models the association between a set of predictors and the expected value of Y:\n\\[\ng(\\mu)=\\eta=\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta\n\\]\n\n\\(\\boldsymbol\\beta\\): regression coefficients\n\\(\\boldsymbol X_i=(1, X_{i1}, \\ldots, X_{ip})^\\mathrm T\\): design vector\n\\(\\eta\\): linear model\n\\(\\mu=E(Y)\\)\n\\(g(\\cdot)\\): link function"
  },
  {
    "objectID": "lectures/11a.html#learning-outcomes",
    "href": "lectures/11a.html#learning-outcomes",
    "title": "Linear Regression",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nScatter Plot\nLinear Regression\nOrdinary Least Squares\nUnbiasedness"
  },
  {
    "objectID": "lectures/11a.html#scatter-plot-1",
    "href": "lectures/11a.html#scatter-plot-1",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "lectures/11a.html#scatter-plot-2",
    "href": "lectures/11a.html#scatter-plot-2",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "lectures/11a.html#linear-regression-1",
    "href": "lectures/11a.html#linear-regression-1",
    "title": "Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is used to model the association between a set of predictor variables (x’s) and an outcome variable (y). Linear regression will fit a line that best describes the data points."
  },
  {
    "objectID": "lectures/11a.html#simple-linear-regression",
    "href": "lectures/11a.html#simple-linear-regression",
    "title": "Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nSimple linear regression will model the association between one predictor variable and an outcome:\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\\(\\beta_0\\): Intercept term\n\\(\\beta_1\\): Slope term\n\\(\\epsilon\\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "lectures/11a.html#fitting-a-line",
    "href": "lectures/11a.html#fitting-a-line",
    "title": "Linear Regression",
    "section": "Fitting a Line",
    "text": "Fitting a Line"
  },
  {
    "objectID": "lectures/11a.html#interpretation",
    "href": "lectures/11a.html#interpretation",
    "title": "Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\n\\hat y = 136.73 + 0.015 x\n\\]"
  },
  {
    "objectID": "lectures/11a.html#ordinary-least-squares-1",
    "href": "lectures/11a.html#ordinary-least-squares-1",
    "title": "Linear Regression",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nFor a data pair \\((X_i,Y_i)_{i=1}^n\\), the ordinary least squares estimator will find the estimates of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize the following function:\n\\[\n\\sum^n_{i=1}\\{y_i-(\\beta_0+\\beta_1x_i)\\}^2\n\\]"
  },
  {
    "objectID": "lectures/11a.html#estimating-betas",
    "href": "lectures/11a.html#estimating-betas",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta\\)’s",
    "text": "Estimating \\(\\beta\\)’s"
  },
  {
    "objectID": "lectures/11a.html#estimating-beta_1",
    "href": "lectures/11a.html#estimating-beta_1",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta_1\\)",
    "text": "Estimating \\(\\beta_1\\)"
  },
  {
    "objectID": "lectures/11a.html#estimating-beta_0",
    "href": "lectures/11a.html#estimating-beta_0",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta_0\\)",
    "text": "Estimating \\(\\beta_0\\)"
  },
  {
    "objectID": "lectures/11a.html#estimates",
    "href": "lectures/11a.html#estimates",
    "title": "Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\n\\] \\[\n\\hat\\beta_1 = \\frac{\\sum^n_{i=1}(y_i-\\bar y)(x_i-\\bar x)}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\] \\[\n\\hat\\sigma^2 = \\frac{1}{n-2}\\sum^n_{i=1}(y_i-\\hat y_i)^2\n\\]"
  },
  {
    "objectID": "lectures/11a.html#unbiasedness-of-betas-1",
    "href": "lectures/11a.html#unbiasedness-of-betas-1",
    "title": "Linear Regression",
    "section": "Unbiasedness of \\(\\beta\\)’s",
    "text": "Unbiasedness of \\(\\beta\\)’s\nBoth \\(\\beta_0\\) and \\(\\beta_1\\) are unbiased estimators."
  },
  {
    "objectID": "lectures/11a.html#ebeta_0",
    "href": "lectures/11a.html#ebeta_0",
    "title": "Linear Regression",
    "section": "\\(E(\\beta_0)\\)",
    "text": "\\(E(\\beta_0)\\)"
  },
  {
    "objectID": "lectures/11a.html#ebeta_1",
    "href": "lectures/11a.html#ebeta_1",
    "title": "Linear Regression",
    "section": "\\(E(\\beta_1)\\)",
    "text": "\\(E(\\beta_1)\\)"
  },
  {
    "objectID": "lectures/10a.html#learning-outcomes",
    "href": "lectures/10a.html#learning-outcomes",
    "title": "Maximum Likelihood Estimators",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMaximum Likelihood Estimators\nLog-Likelihood Functions"
  },
  {
    "objectID": "lectures/10a.html#estimators",
    "href": "lectures/10a.html#estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets a parameter, using measurements from a sample."
  },
  {
    "objectID": "lectures/10a.html#data",
    "href": "lectures/10a.html#data",
    "title": "Maximum Likelihood Estimators",
    "section": "Data",
    "text": "Data\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)\\) where \\(F(\\cdot)\\) is a known distribution function and \\(\\boldsymbol\\theta\\) is a vector of parameters. Let \\(\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}\\), be the sample collected."
  },
  {
    "objectID": "lectures/10a.html#likelihood-function",
    "href": "lectures/10a.html#likelihood-function",
    "title": "Maximum Likelihood Estimators",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nUsing the joint pdf or pmf of the sample \\(\\boldsymbol X\\), the likelihood function is a function of \\(\\boldsymbol \\theta\\), given the observed data \\(\\boldsymbol X =\\boldsymbol x\\), defined as\n\\[\nL(\\boldsymbol \\theta|\\boldsymbol x)=f(\\boldsymbol x|\\boldsymbol \\theta)\n\\]\nIf the data is iid, then\n\\[\nf(\\boldsymbol x|\\boldsymbol \\theta) = \\prod^n_{i=1}f(x_i|\\boldsymbol\\theta)\n\\]"
  },
  {
    "objectID": "lectures/10a.html#log-likelihood-function",
    "href": "lectures/10a.html#log-likelihood-function",
    "title": "Maximum Likelihood Estimators",
    "section": "Log-Likelihood Function",
    "text": "Log-Likelihood Function\nIf \\(\\ln\\{L(\\boldsymbol \\theta)\\}\\) is monotone of \\(\\boldsymbol \\theta\\), then maximizing \\(\\ell(\\boldsymbol\\theta) = \\ln\\{L(\\boldsymbol \\theta)\\}\\) will yield the maximum likelihood estimators."
  },
  {
    "objectID": "lectures/10a.html#maximum-log-likelihood-estimator",
    "href": "lectures/10a.html#maximum-log-likelihood-estimator",
    "title": "Maximum Likelihood Estimators",
    "section": "Maximum log-Likelihood Estimator",
    "text": "Maximum log-Likelihood Estimator\nThe maximum likelihood estimator are the estimates of \\(\\boldsymbol \\theta\\) that maximize \\(\\ell(\\boldsymbol\\theta)\\)."
  },
  {
    "objectID": "lectures/10a.html#poisson-distribution",
    "href": "lectures/10a.html#poisson-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Pois}(\\lambda)\\), show that the MLE of \\(\\lambda\\) is \\(\\bar x\\)."
  },
  {
    "objectID": "lectures/10a.html#normal-distribution",
    "href": "lectures/10a.html#normal-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). Show that the MLE’s of \\(\\mu\\) and \\(\\sigma^2\\) are \\(\\bar x\\) and \\(\\frac{n-1}{n}s^2\\), respectively."
  },
  {
    "objectID": "lectures/10a.html#exponential-distribution",
    "href": "lectures/10a.html#exponential-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}Exp(\\lambda)\\). Find the MLE of \\(\\lambda\\)"
  },
  {
    "objectID": "hws/hw5.html",
    "href": "hws/hw5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Submit your homework on Canvas as one PDF document.\n\nLet X_1, X_2, \\ldots, X_n be iid with the following density function\n\nf(x) = \\left\\{\\begin{array}{cc}\n(\\theta + 1)x^\\theta& 0\\le x\\le 1;\\theta&gt;-1 \\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\n\\right.\n\nFind the MLE for \\theta.\nLet X_1, X_2, \\ldots, X_n be iid with the following density function\n\nf(x) = \\left\\{\\begin{array}{cc}\n\\frac{1}{\\Gamma(\\alpha)\\theta^\\alpha}x^{\\alpha-1}e^{-x/\\theta} & 0&lt;x;0&lt; \\theta \\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\n\\right.\n\nwhere \\alpha&gt;0 is known. Find the MLE for \\theta.\nLet X_1, \\ldots, X_n\\overset{iid}{\\sim}Geo(p), what is the MLE of p."
  },
  {
    "objectID": "hws/hw3.html",
    "href": "hws/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Submit the assignment as one PDF file. Due October 5th, 2025 at 11:59 PM."
  },
  {
    "objectID": "hws/hw3.html#problem-1",
    "href": "hws/hw3.html#problem-1",
    "title": "Homework 3",
    "section": "Problem 1",
    "text": "Problem 1\nProblem from Probability 4 Data Science Chapter 4: Exercises 2"
  },
  {
    "objectID": "hws/hw3.html#problem-2",
    "href": "hws/hw3.html#problem-2",
    "title": "Homework 3",
    "section": "Problem 2",
    "text": "Problem 2\nProblem from Probability 4 Data Science Chapter 4: Exercises 3"
  },
  {
    "objectID": "hws/hw3.html#problem-3",
    "href": "hws/hw3.html#problem-3",
    "title": "Homework 3",
    "section": "Problem 3",
    "text": "Problem 3\nProblem from Probability 4 Data Science Chapter 4: Exercises 4"
  },
  {
    "objectID": "hws/hw3.html#problem-4",
    "href": "hws/hw3.html#problem-4",
    "title": "Homework 3",
    "section": "Problem 4",
    "text": "Problem 4\nProblem from Probability 4 Data Science Chapter 4: Exercises 5"
  },
  {
    "objectID": "hws/hw3.html#problem-5",
    "href": "hws/hw3.html#problem-5",
    "title": "Homework 3",
    "section": "Problem 5",
    "text": "Problem 5\nShow that the Expected Value of a \\mathrm{Beta}(\\alpha, \\beta) random variable is \\frac{\\alpha}{\\alpha + \\beta}"
  },
  {
    "objectID": "hws/hw0.html",
    "href": "hws/hw0.html",
    "title": "Homework 0",
    "section": "",
    "text": "If you plan to submit this assignment, please try to submit the assignment as 1 PDF if possible."
  },
  {
    "objectID": "hws/hw0.html#footnotes",
    "href": "hws/hw0.html#footnotes",
    "title": "Homework 0",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEither convert to summation notation or evaluate the summation.↩︎"
  },
  {
    "objectID": "exc/exc4.html",
    "href": "exc/exc4.html",
    "title": "Extra Credit 3",
    "section": "",
    "text": "Create a video essay on Monte Carlo Methods. Provide a brief history and their purpose. Lastly, provide a toy example demonstrating the use of Monte Carlo Methods in R or Python. The video essay should be 10 minutes being narrated by your own voice. Use visual aids, such as a powerpoint presentation, to highlight your main findings.\nWorth 3 percent points.\n\nResources\n\nhttps://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694\nhttps://towardsdatascience.com/monte-carlo-simulation-a-practical-guide-85da45597f0e\n\nVideo Essay Guidelines:\n\n10 Minutes Long\nUse of visual aids\nNarrated by your own voice\nDue 12/5/2025\n\nWorth 3 final grade percentage points."
  },
  {
    "objectID": "ec.html",
    "href": "ec.html",
    "title": "Extra Credit",
    "section": "",
    "text": "Extra Credit is designed to expand on different topics that related to Probability and Statistics, but are not necessarily required for the course. Additionally, these opportunities provide students relief when unexpected situations occur during the semester. While it is not required, I encourage everyone to attempt each opportunity.\nBelow is more information on each assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 1\n\n\nInstructions for extra credit one.\n\n\n\n\n\nAug 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 2\n\n\nInstructions for extra credit three.\n\n\n\n\n\nAug 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 3\n\n\nInstructions for extra credit four.\n\n\n\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 4\n\n\nInstructions for extra credit five.\n\n\n\n\n\nAug 24, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "exc/exc1.html",
    "href": "exc/exc1.html",
    "title": "Extra Credit 1",
    "section": "",
    "text": "Write a 2-page report practices related to self care. As you move on through college and your future careers, it is important to practice self care. For this extra credit assignments, write about what are some practices you will do to for your well being.\nWorth 1 percent points.\nReport guidelines\n\n2-3 Pages\nDouble Spaced\n12 point font\nDue 9/21/2025"
  },
  {
    "objectID": "exc/exc3.html",
    "href": "exc/exc3.html",
    "title": "Extra Credit 2",
    "section": "",
    "text": "Create a video essay on Survival Analysis. Provide a brief history and their purpose. Lastly, provide a toy example demonstrating the use of Survival Analysis in R or Python. The video essay should be 10 minutes being narrated by your own voice. Use visual aids, such as a powerpoint presentation, to highlight your main findings.\nWorth 3 percent points.\n\nResources\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/\nhttps://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/BS704_Survival_print.html\n\nVideo Essay Guidelines:\n\n10 Minutes Long\nUse of visual aids\nNarrated by your own voice\nDue 12/5/2025\n\nWorth 3 final grade percentage points."
  },
  {
    "objectID": "exc/exc5.html",
    "href": "exc/exc5.html",
    "title": "Extra Credit 4",
    "section": "",
    "text": "After reading one book below, create a video essay that summarizes the book, provide a brief analysis in supporting or opposing the book, and connect key elements of the book to your every day life. The video essay should be 10 minutes being narrated by your own voice. Use is visual aids, such as a powerpoint presentation, to highlight your main findings. All books should be available through the Broome Library.\nBooks:\n\nEmpire of AI\n\nKaren Hao\n\nThe Last Human Job\n\nAllison Pugh\n\nThe Book of Why\n\nJudea Pearl\n\nAlgorithms of Oppression\n\nSafiya Umoja Noble\n\nData Feminism\n\nCatherine D’Ignazio and Lauren Klein\n\nWeapons of Math Destruction\n\nCathy O’Niel\n\nInvisible Women : data bias in a world designed for men\n\nCaroline Criado Perez\n\nFactfulness: Ten Reasons We’re Wrong About the World… and Why Things are Better Than You Think\n\nHans Rosling\n\nArtificial Unintelligence: How Computers Misunderstand the World\n\nMerideth Broussard\n\nTechnically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech\n\nSara Wachter-Boettcher\n\nOR ANY Book Approved By Me (Deadline for Approval by March 28, 2025)\n\nVideo Essay Guidelines:\n\n10 Minutes Long\nUse of visual aids\nNarrated by your own voice\nDue 12/5/2025\n\nWorth 3 final grade percentage points."
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "Homework",
    "section": "",
    "text": "Below are the different homework assignments for the course. Make sure to upload your assignment as as single PDF on Canvas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 5\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4\n\n\n\n\n\n\n\n\nOct 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3\n\n\n\n\n\n\n\n\nSep 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\n\n\nSep 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1\n\n\n\n\n\n\n\n\nSep 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0\n\n\n\n\n\n\n\n\nAug 24, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hws/hw1.html",
    "href": "hws/hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Problems from Probability 4 Data Science Chapter 2:\nExercises 1, 6 (11 is 22; ignore F), 8, 9\nHomework 1 is due 9/14/2025 at 11:59 PM. Submit your homework on Canvas as one PDF document."
  },
  {
    "objectID": "hws/hw2.html",
    "href": "hws/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Problem from Probability 4 Data Science Chapter 3: Exercises 4 (ignore c)"
  },
  {
    "objectID": "hws/hw2.html#problem-1",
    "href": "hws/hw2.html#problem-1",
    "title": "Homework 2",
    "section": "",
    "text": "Problem from Probability 4 Data Science Chapter 3: Exercises 4 (ignore c)"
  },
  {
    "objectID": "hws/hw2.html#problem-2",
    "href": "hws/hw2.html#problem-2",
    "title": "Homework 2",
    "section": "Problem 2",
    "text": "Problem 2\nProblem from Probability 4 Data Science Chapter 3: Exercises 5"
  },
  {
    "objectID": "hws/hw2.html#problem-3",
    "href": "hws/hw2.html#problem-3",
    "title": "Homework 2",
    "section": "Problem 3",
    "text": "Problem 3\nProblem from Probability 4 Data Science Chapter 3: Exercises 8"
  },
  {
    "objectID": "hws/hw2.html#problem-4",
    "href": "hws/hw2.html#problem-4",
    "title": "Homework 2",
    "section": "Problem 4",
    "text": "Problem 4\nProblem from Probability 4 Data Science Chapter 3: Exercises 13"
  },
  {
    "objectID": "hws/hw2.html#problem-5",
    "href": "hws/hw2.html#problem-5",
    "title": "Homework 2",
    "section": "Problem 5",
    "text": "Problem 5\nShow that the variance of a geometric distribution is \\frac{1-p}{p^2}.\nHomework 2 is due 9/21/2025 at 11:59 PM. Submit your homework on Canvas as one PDF document."
  },
  {
    "objectID": "hws/hw4.html",
    "href": "hws/hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Find the likelihood function and log-likelihood function for a random sample from the following distributions:\n\nNormal Distribution\nBinomial Distrubition\nUniform Distribution\nPoisson Distribution\nGamma Distribution"
  },
  {
    "objectID": "hws/hw4.html#problem-3",
    "href": "hws/hw4.html#problem-3",
    "title": "Homework 4",
    "section": "Problem 3",
    "text": "Problem 3\nFind the value of x that maximizes or minimizes the function:\n\nf(x) = -4(3x - 9)^2 + 12"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Probability and Statistics!",
    "section": "",
    "text": "Brief Introduction\n\n\n\n\n\nWELCOME TO THE COURSE! This is the home page of the course where I will provide a recap on what was covered in the week. Here I will post any documents or videos for your reference. If you have any questions, please email me at isaac.qs@csuci.edu.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\nThis week, we will discuss maximum likelihood estimators.\n\n\n\n\n\nOct 26, 2025\n\n\n\n\n\n\n\nWeek 9\n\n\nThis week, we will discuss sampling distributions further.\n\n\n\n\n\nOct 19, 2025\n\n\n\n\n\n\n\nWeek 8\n\n\nMidterm this week\n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\n\n\nWeek 7\n\n\nThis week, we will discuss joint distributions and their properties. As well as the Central Limit Theorem.\n\n\n\n\n\nOct 3, 2025\n\n\n\n\n\n\n\nWeek 6\n\n\nThis week, we will be discussing MGFs.\n\n\n\n\n\nSep 29, 2025\n\n\n\n\n\n\n\nWeek 5\n\n\nThis week, we will discuss continuous random variables and their properties.\n\n\n\n\n\nSep 21, 2025\n\n\n\n\n\n\n\nWeek 4\n\n\nThis week, we will discuss Expected Values and Moment Generating Function.\n\n\n\n\n\nSep 12, 2025\n\n\n\n\n\n\n\nWeek 3\n\n\nThis week, we will discuss the properties of discrete random variables.\n\n\n\n\n\nSep 6, 2025\n\n\n\n\n\n\n\nWeek 2\n\n\nThis week, we will briefly discuss topics related to conditional probabilities. Additionally, we will introduce commonly used distribution functions.\n\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\n\nWeek 1\n\n\nThis week is designed to be an introduction week. We will briefly discuss topics related to statistics and inference. Then we will look at installing R and RStudio as well as the basics of using R.\n\n\n\n\n\nAug 24, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/10b.html#learning-outcomes",
    "href": "lectures/10b.html#learning-outcomes",
    "title": "Maximum Likelihood Estimators",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMaximum Likelihood Estimators\nProperties"
  },
  {
    "objectID": "lectures/10b.html#estimators",
    "href": "lectures/10b.html#estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample."
  },
  {
    "objectID": "lectures/10b.html#data",
    "href": "lectures/10b.html#data",
    "title": "Maximum Likelihood Estimators",
    "section": "Data",
    "text": "Data\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)\\) where \\(F(\\cdot)\\) is a known distribution function and \\(\\boldsymbol\\theta\\) is a vector of parameters. Let \\(\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}\\), be the sample collected."
  },
  {
    "objectID": "lectures/10b.html#unbiased-estimators",
    "href": "lectures/10b.html#unbiased-estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Unbiased Estimators",
    "text": "Unbiased Estimators\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be an estimator for a parameter \\(\\theta\\). Then \\(\\hat \\theta\\) is an unbiased estimator if \\(E(\\hat \\theta) = \\theta\\). Otherwise, \\(\\hat\\theta\\) is considered biased."
  },
  {
    "objectID": "lectures/10b.html#consistent-estimators",
    "href": "lectures/10b.html#consistent-estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Consistent Estimators",
    "text": "Consistent Estimators\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). The estimator \\(\\hat \\theta\\) is a consistent estimator of the \\(\\theta\\) if\n\n\\(E\\{(\\hat\\theta-\\theta)^2\\}\\rightarrow0\\) as \\(n\\rightarrow \\infty\\)\n\\(P(|\\hat\\theta-\\theta|\\ge \\epsilon)\\rightarrow0\\) as \\(n\\rightarrow \\infty\\) for every \\(\\epsilon&gt;0\\)"
  },
  {
    "objectID": "lectures/10b.html#invariance-property",
    "href": "lectures/10b.html#invariance-property",
    "title": "Maximum Likelihood Estimators",
    "section": "Invariance Property",
    "text": "Invariance Property\nIf \\(\\hat \\theta\\) is an ML estimator of \\(\\theta\\), then for any one-to-one function \\(g\\), the ML estimator for \\(g(\\theta)\\) is \\(g(\\hat\\theta)\\)."
  },
  {
    "objectID": "lectures/10b.html#large-sample-theory",
    "href": "lectures/10b.html#large-sample-theory",
    "title": "Maximum Likelihood Estimators",
    "section": "Large Sample Theory",
    "text": "Large Sample Theory\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be the MLE estimator for a parameter \\(\\theta\\). As \\(n\\rightarrow\\infty\\), then \\(\\hat \\theta\\) has a normal distribution with mean \\(\\theta\\) and variance \\(1/nI(\\theta)\\), where\n\\[\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right]\n\\]"
  },
  {
    "objectID": "lectures/10b.html#binomial-distribution",
    "href": "lectures/10b.html#binomial-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Bin}(m, p)\\), find the mle of \\(p\\). Check if it is an unbiased estimator."
  },
  {
    "objectID": "lectures/10b.html#poisson-distribution",
    "href": "lectures/10b.html#poisson-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nCheck if \\(\\bar X\\) is an unbiased estimator for \\(\\lambda\\) in the Poisson distribution."
  },
  {
    "objectID": "lectures/10b.html#normal-distribution",
    "href": "lectures/10b.html#normal-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nUsing the MLE’s for a normal distribution, \\(\\hat\\mu = \\bar X = \\frac{1}{n}\\sum X_i\\) and \\(\\hat\\sigma^2 = \\frac{1}{n}\\sum (X_i-\\bar X)^2\\) check if there are unbiased estimators for \\(\\mu\\) and \\(\\sigma^2\\)."
  },
  {
    "objectID": "lectures/10b.html#bernoulli-distribution",
    "href": "lectures/10b.html#bernoulli-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Bernoulli}(p)\\), show that the MLE of \\(p\\) is \\(\\bar X\\). Find the distribution of the MLE as \\(n\\rightarrow\\infty\\)"
  },
  {
    "objectID": "lectures/11b.html#learning-objectives",
    "href": "lectures/11b.html#learning-objectives",
    "title": "Linear Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nMatrix Formulation\nMultiple Linear Regression\nModel Assumptions"
  },
  {
    "objectID": "lectures/11b.html#matrix-version-of-model",
    "href": "lectures/11b.html#matrix-version-of-model",
    "title": "Linear Regression",
    "section": "Matrix Version of Model",
    "text": "Matrix Version of Model\n\\[\nY_i = \\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta + \\epsilon_i\n\\]\n\n\\(Y_i\\): Outcome Variable\n\\(\\boldsymbol X_i=(1, X_i)^\\mathrm T\\): Predictors\n\\(\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T\\): Coefficients\n\\(\\epsilon_i\\): error term"
  },
  {
    "objectID": "lectures/11b.html#data-matrix-formulation",
    "href": "lectures/11b.html#data-matrix-formulation",
    "title": "Linear Regression",
    "section": "Data Matrix Formulation",
    "text": "Data Matrix Formulation\nFor \\(n\\) data points\n\\[\n\\boldsymbol Y = \\boldsymbol X^\\mathrm T\\boldsymbol \\beta + \\boldsymbol \\epsilon\n\\]\n\n\\(\\boldsymbol Y = (Y_1, \\cdots, Y_n)^\\mathrm T\\): Outcome Variable\n\\(\\boldsymbol X=(\\boldsymbol X_1, \\cdots, \\boldsymbol X_n)^\\mathrm T\\): Predictors\n\\(\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T\\): Coefficients\n\\(\\boldsymbol \\epsilon = (\\epsilon_1, \\cdots, \\epsilon_n)^\\mathrm T\\): Error terms"
  },
  {
    "objectID": "lectures/11b.html#least-squares-formula",
    "href": "lectures/11b.html#least-squares-formula",
    "title": "Linear Regression",
    "section": "Least Squares Formula",
    "text": "Least Squares Formula\n\\[\n(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)^\\mathrm T(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)\n\\]"
  },
  {
    "objectID": "lectures/11b.html#estimates",
    "href": "lectures/11b.html#estimates",
    "title": "Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat{\\boldsymbol \\beta} = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1}\\boldsymbol X ^\\mathrm T\\boldsymbol Y\n\\]"
  },
  {
    "objectID": "lectures/11b.html#mlr",
    "href": "lectures/11b.html#mlr",
    "title": "Linear Regression",
    "section": "MLR",
    "text": "MLR\nMultivariable linear regression models are used when more than one explanatory variable is used to explain the outcome of interest."
  },
  {
    "objectID": "lectures/11b.html#continuous-variable",
    "href": "lectures/11b.html#continuous-variable",
    "title": "Linear Regression",
    "section": "Continuous Variable",
    "text": "Continuous Variable\nTo fit an additional continuous random variable to the model, we will only need to add it to the model:\n\\[\nY = \\beta_0 +\\beta_1 X_1 + \\beta_2 X_2\n\\]"
  },
  {
    "objectID": "lectures/11b.html#categorical-variable",
    "href": "lectures/11b.html#categorical-variable",
    "title": "Linear Regression",
    "section": "Categorical Variable",
    "text": "Categorical Variable\nA categorical variable can be included in a model, but a reference category must be specified."
  },
  {
    "objectID": "lectures/11b.html#fitting-a-model-with-categorical-variables",
    "href": "lectures/11b.html#fitting-a-model-with-categorical-variables",
    "title": "Linear Regression",
    "section": "Fitting a model with categorical variables",
    "text": "Fitting a model with categorical variables\nTo fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use \\(C-1\\) dummy variables where \\(C\\) indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables."
  },
  {
    "objectID": "lectures/11b.html#example",
    "href": "lectures/11b.html#example",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nIf we have 4 categories, we will need 3 dummy variables:\n\n\n\n\nCat 1\nCat 2\nCat 3\nCat 4\n\n\n\n\nDummy 1\n1\n0\n0\n0\n\n\nDummy 2\n0\n1\n0\n0\n\n\nDummy 3\n0\n0\n1\n0\n\n\n\nWhich one is the reference category?"
  },
  {
    "objectID": "lectures/11b.html#matrix-notation",
    "href": "lectures/11b.html#matrix-notation",
    "title": "Linear Regression",
    "section": "Matrix Notation",
    "text": "Matrix Notation\n\\[\nY = \\boldsymbol \\beta^T\\boldsymbol X\n\\]\n\n\\(\\boldsymbol \\beta\\): a column vector of regression coefficients\n\\(\\boldsymbol X\\): a column vector of predictor variables"
  },
  {
    "objectID": "lectures/11b.html#model",
    "href": "lectures/11b.html#model",
    "title": "Linear Regression",
    "section": "Model",
    "text": "Model\n\\[\nY = \\boldsymbol \\beta^T\\boldsymbol X\n\\]\n\n\\(\\epsilon \\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "lectures/11b.html#model-scatter-plot",
    "href": "lectures/11b.html#model-scatter-plot",
    "title": "Linear Regression",
    "section": "Model Scatter Plot",
    "text": "Model Scatter Plot"
  },
  {
    "objectID": "lectures/11b.html#model-assumptions-1",
    "href": "lectures/11b.html#model-assumptions-1",
    "title": "Linear Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nErrors are normally distributed\nConstant Variance\nLinearity\nIndependence\nNo outliers"
  },
  {
    "objectID": "lectures/11b.html#errors-normally-distributed",
    "href": "lectures/11b.html#errors-normally-distributed",
    "title": "Linear Regression",
    "section": "Errors Normally Distributed",
    "text": "Errors Normally Distributed"
  },
  {
    "objectID": "lectures/11b.html#constant-variance",
    "href": "lectures/11b.html#constant-variance",
    "title": "Linear Regression",
    "section": "Constant Variance",
    "text": "Constant Variance"
  },
  {
    "objectID": "lectures/11b.html#linearity",
    "href": "lectures/11b.html#linearity",
    "title": "Linear Regression",
    "section": "Linearity",
    "text": "Linearity"
  },
  {
    "objectID": "lectures/11b.html#linearity-1",
    "href": "lectures/11b.html#linearity-1",
    "title": "Linear Regression",
    "section": "Linearity",
    "text": "Linearity"
  },
  {
    "objectID": "lectures/11b.html#no-outliers",
    "href": "lectures/11b.html#no-outliers",
    "title": "Linear Regression",
    "section": "No Outliers",
    "text": "No Outliers"
  },
  {
    "objectID": "lectures/11b.html#residual-analysis",
    "href": "lectures/11b.html#residual-analysis",
    "title": "Linear Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nA residual analysis is used to assess the validity of the assumptions."
  },
  {
    "objectID": "lectures/13a.html#learning-outcomes",
    "href": "lectures/13a.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nEstimation Procedures\n\nRegression Coefficients\nDispersion Parameter\n\nNewton-Raphson Algorithm"
  },
  {
    "objectID": "lectures/13a.html#estimating-boldsymbolbeta",
    "href": "lectures/13a.html#estimating-boldsymbolbeta",
    "title": "Generalized Linear Models",
    "section": "Estimating \\(\\boldsymbol\\beta\\)",
    "text": "Estimating \\(\\boldsymbol\\beta\\)\nTo obtain the estimates of \\(\\boldsymbol \\beta\\) we can use the maximum log-likelihood approach to obtain \\(\\hat{\\boldsymbol\\beta}\\).\n\\[\nL(\\boldsymbol \\beta) =  \\prod^n_{i=1}f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\n\\]"
  },
  {
    "objectID": "lectures/13a.html#maximum-likelihood-approach",
    "href": "lectures/13a.html#maximum-likelihood-approach",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood Approach",
    "text": "Maximum Likelihood Approach\n\\[\n\\ell(\\boldsymbol \\beta) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/13a.html#numerical-approaches",
    "href": "lectures/13a.html#numerical-approaches",
    "title": "Generalized Linear Models",
    "section": "Numerical Approaches",
    "text": "Numerical Approaches\n\nNewton-Rhapson Algorithm\nFisher-Scoring Algorithm\nNelder-Mead\nBFGS"
  },
  {
    "objectID": "lectures/13a.html#estimating-phi-1",
    "href": "lectures/13a.html#estimating-phi-1",
    "title": "Generalized Linear Models",
    "section": "Estimating \\(\\phi\\)",
    "text": "Estimating \\(\\phi\\)\nDepending on the random variable, the dispersion parameter will need to be estimated to conduct inference procedures. There are 4 methods to estimate the dispersion parameter:\n\nMaximum Likelihood\nMaximum (Modified) Profile Likelihood Approach\nMean Deviance Estimator\nPearson Estimator"
  },
  {
    "objectID": "lectures/13a.html#maximum-likelihood-approach-1",
    "href": "lectures/13a.html#maximum-likelihood-approach-1",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood Approach",
    "text": "Maximum Likelihood Approach\n\\[\n\\ell(\\phi) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/13a.html#maximum-modified-profile-likelihood-approach",
    "href": "lectures/13a.html#maximum-modified-profile-likelihood-approach",
    "title": "Generalized Linear Models",
    "section": "Maximum (Modified) Profile Likelihood Approach",
    "text": "Maximum (Modified) Profile Likelihood Approach\n\\[\n\\ell_p(\\phi) = \\frac{p}{2}\\log \\phi + \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\hat{\\boldsymbol \\beta},\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/13a.html#mean-deviance-estimator",
    "href": "lectures/13a.html#mean-deviance-estimator",
    "title": "Generalized Linear Models",
    "section": "Mean Deviance Estimator",
    "text": "Mean Deviance Estimator\n\\[\n\\tilde \\phi = \\frac{D(y,\\hat\\mu)}{n-p}\n\\]\n\n\\(D(y,\\hat\\mu)=2\\sum^n_{i=1}\\left\\{t(y,y) - t(y,\\mu) \\right\\}\\)\n\\(t(y,\\mu)=y\\theta-\\kappa(\\theta)\\)\n\\(p\\): number of regression coefficients"
  },
  {
    "objectID": "lectures/13a.html#pearson-estimator",
    "href": "lectures/13a.html#pearson-estimator",
    "title": "Generalized Linear Models",
    "section": "Pearson Estimator",
    "text": "Pearson Estimator\n\\[\n\\bar \\phi = \\frac{\\Lambda^2}{n-p}\n\\]\n\n\\(\\Lambda^2=\\sum^n_{i=1}\\frac{y_i-\\hat\\mu_i}{V(\\hat\\mu_i)}\\)\n\\(\\hat \\mu_i = g^{-1}(\\hat\\beta_0 + \\sum^n_{j=1}{X_{ij}\\hat\\beta_j})\\)\n\\(V(\\hat\\mu_i)=\\frac{d^2\\kappa(\\hat\\theta_i)}{d\\theta_i^2}\\)"
  },
  {
    "objectID": "lectures/13a.html#numerical-algorithm",
    "href": "lectures/13a.html#numerical-algorithm",
    "title": "Generalized Linear Models",
    "section": "Numerical Algorithm",
    "text": "Numerical Algorithm\nIn Mathematics and Statistics, numerical algorithms are used to approximate the value of different functions:\n\nRoot Finding:\n\nNewton’s Method\n\nDerivatives\n\nSecant Step-size\n\nIntegrals\n\nReimman Sums\n\nMaximization\n\nNewton-Raphson"
  },
  {
    "objectID": "lectures/13a.html#optimization",
    "href": "lectures/13a.html#optimization",
    "title": "Generalized Linear Models",
    "section": "Optimization",
    "text": "Optimization\nOptimization is the techniques used to find the values that maximizes the a function:\n\\[\nx_0 = \\mathrm{argmax}_{x}f(x)\n\\]"
  },
  {
    "objectID": "lectures/13a.html#newton-raphson",
    "href": "lectures/13a.html#newton-raphson",
    "title": "Generalized Linear Models",
    "section": "Newton-Raphson",
    "text": "Newton-Raphson\nThe Newton-Raphson algorithm is used to estimate the parameters using an iterative algorithm. Given initial estimates, it will update the estimates of the parameters using the Newton step. It will continue iterating and updating the steps until the function converges to the maximum value."
  },
  {
    "objectID": "lectures/13a.html#newton-raphson-1",
    "href": "lectures/13a.html#newton-raphson-1",
    "title": "Generalized Linear Models",
    "section": "Newton-Raphson",
    "text": "Newton-Raphson\n\\[\n\\beta_j^{(it+1)} = \\beta_j^{(it)} - \\frac{G_{\\beta_j}^{(it)}}{H_{\\beta_j}^{(it)}}\n\\]\n\n\\(\\beta_j^{(it)}\\): current estimate of \\(\\beta_j\\)\n\\(G_{\\beta_j}^{(it)}=d\\ell(\\boldsymbol \\beta)/d\\beta_j|_{\\beta_j=\\beta_j^{(it)}}\\)\n\\(H_{\\beta_j}^{(it)}=d^2\\ell(\\boldsymbol \\beta)/d\\beta_j^2|_{\\beta_j=\\beta_j^{(it)}}\\)\n\\(\\beta_j^{(it+1)}\\): Updated estimate of \\(\\beta_j\\)"
  },
  {
    "objectID": "lectures/13a.html#logistic-regression",
    "href": "lectures/13a.html#logistic-regression",
    "title": "Generalized Linear Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet \\((Y_i,X_i)_{i=1}^n\\) be a data set where \\(Y_i\\overset{iid}{\\sim}Bernoulli(p)\\). Find the first and second derivative for \\(\\beta_1\\), when a GLM is fitted to the model."
  },
  {
    "objectID": "lectures/13a.html#poisson-regression",
    "href": "lectures/13a.html#poisson-regression",
    "title": "Generalized Linear Models",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nLet \\((Y_i,X_i)_{i=1}^n\\) be a data set where \\(Y_i\\overset{iid}{\\sim}Pois(\\lambda)\\). Find the first and second derivative for \\(\\beta_0\\), when a GLM is fitted to the model."
  },
  {
    "objectID": "lectures/1a.html#introductions",
    "href": "lectures/1a.html#introductions",
    "title": "Weclome to Math 352",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "lectures/1a.html#introductions-1",
    "href": "lectures/1a.html#introductions-1",
    "title": "Weclome to Math 352",
    "section": "Introductions",
    "text": "Introductions\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "lectures/1a.html#goals-for-the-course",
    "href": "lectures/1a.html#goals-for-the-course",
    "title": "Weclome to Math 352",
    "section": "Goals for the Course",
    "text": "Goals for the Course\n\nIntroduction to Probability\nIntroduction to Statistics\nIntroduction to Regression"
  },
  {
    "objectID": "lectures/1a.html#office-hours",
    "href": "lectures/1a.html#office-hours",
    "title": "Weclome to Math 352",
    "section": "Office Hours",
    "text": "Office Hours\nMarin Hall 2326\nT/TH 5-6 PM\nWed 2-4 PM"
  },
  {
    "objectID": "lectures/1a.html#syllabus-1",
    "href": "lectures/1a.html#syllabus-1",
    "title": "Weclome to Math 352",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus"
  },
  {
    "objectID": "lectures/1a.html#use-of-generative-ai-1",
    "href": "lectures/1a.html#use-of-generative-ai-1",
    "title": "Weclome to Math 352",
    "section": "Use of Generative AI",
    "text": "Use of Generative AI\nThe use of generative artificial intelligence (AI) to complete any part or all of an assignment/exam is strictly prohibited in this class. This includes, but not limited to, ChatGPT, Claude, Meta AI, and Google Gemini.\n\nYou may use AI to enhance you understanding of the material.\n\n\nYou may not upload any course material to any AI platforms such as ChatGPT, Claude, Meta AI, and Google Gemini. Exceptions are allowed for DASS-approved services."
  },
  {
    "objectID": "lectures/1a.html#use-of-ai",
    "href": "lectures/1a.html#use-of-ai",
    "title": "Weclome to Math 352",
    "section": "Use of AI",
    "text": "Use of AI\nThere are consequences when you use of AI:\n\nEducational Mislearning\nTrusting AI\nStolen Work\nPrivacy Concerns\nEnvironmental Impacts\nWorking Exploitation"
  },
  {
    "objectID": "lectures/1a.html#educational-mislearning",
    "href": "lectures/1a.html#educational-mislearning",
    "title": "Weclome to Math 352",
    "section": "Educational Mislearning",
    "text": "Educational Mislearning\nThe purpose of this class, and college, is for you to learn about critical thinking skills and perseverance. Using AI will only teach you how to get an answer, which may or may not be correct.\n\nYou will not develop the skills needed to problem solve a challenge. Additionally, developing grit is essential to become successful in college and life. There is no easy way out and AI is an illusion to your success in life.\n\n\nTo learn something, it requires hours of work! If not years!"
  },
  {
    "objectID": "lectures/1a.html#trusting-ai",
    "href": "lectures/1a.html#trusting-ai",
    "title": "Weclome to Math 352",
    "section": "Trusting AI",
    "text": "Trusting AI\nWhen using AI, you must acknowledge its limitations:\n\nResponses provided may be incorrect\nResponses may not be fair\nCompanies may manipulate responses and/or terms of service for their benefit\nCompanies may not have your best interst in mind\n\n\nYou should always proceed with caution utilizing these tools!"
  },
  {
    "objectID": "lectures/1a.html#stolen-work",
    "href": "lectures/1a.html#stolen-work",
    "title": "Weclome to Math 352",
    "section": "Stolen Work",
    "text": "Stolen Work\n\nAdditionally, all these individuals are not receiving any royalties for the work to be used in creating generative AI models.\n\n\nInside Higher Ed and The New Yorker highlight individual’s concern of their work being used to train AI models."
  },
  {
    "objectID": "lectures/1a.html#privacy-concerns",
    "href": "lectures/1a.html#privacy-concerns",
    "title": "Weclome to Math 352",
    "section": "Privacy Concerns",
    "text": "Privacy Concerns\nThe use of generative AI raises concerns of what data is being harvested from us, possibly without informed consent.\n\nWhen you used any large language models, you do not know what information is being harvested from you.\n\n\nDo you want to upload your thoughts and ideas to a company that can monetize, and possibly exploit.\n\n\nDoes your Professors consent with you uploading their assignments to large language models?\n\n\nStanford provided a report highlighting the risks of our personal data use in large language models."
  },
  {
    "objectID": "lectures/1a.html#environmental-impact",
    "href": "lectures/1a.html#environmental-impact",
    "title": "Weclome to Math 352",
    "section": "Environmental Impact",
    "text": "Environmental Impact\nIn order to run these large language models, companies need to use a large amounts of energy. This is because large servers are needed to both train and execute a model.\n\nThe LA Times reports the potential impact that running AI models in California.\n\n\nAdditionally, Time reports that a ChatGPT query uses ten times more energy than a Google search query, and global AI demands can consume of 1 trillion gallons of water by 2027.\n\n\nThere are also environmental justice questions about where these data centers are constructed."
  },
  {
    "objectID": "lectures/1a.html#worker-exploitation",
    "href": "lectures/1a.html#worker-exploitation",
    "title": "Weclome to Math 352",
    "section": "Worker Exploitation",
    "text": "Worker Exploitation\nThe Washington Post and Time (Article 1 and Article 2) reported that AI companies utilize “digital sweatshops” to classify data points for model training.\n\nThere is a human cost from the Global South, both financially and mentally, to develop the AI models for users in the United States and Europe.\n\n\nWe must be conscious consumers and demand more from these companies to provide safe working conditions and livable wages."
  },
  {
    "objectID": "lectures/1a.html#is-using-ai-bad",
    "href": "lectures/1a.html#is-using-ai-bad",
    "title": "Weclome to Math 352",
    "section": "Is Using AI Bad?",
    "text": "Is Using AI Bad?\n\nNO\n\n\nBut, we need to acknowledge the consequences of using AI!\n\n\nAnd, we need to demand companies to create and implement AI in an ethical manner!"
  },
  {
    "objectID": "lectures/1a.html#homework-0",
    "href": "lectures/1a.html#homework-0",
    "title": "Weclome to Math 352",
    "section": "Homework 0",
    "text": "Homework 0\nHomework 0 is posted!"
  },
  {
    "objectID": "lectures/1a.html#plot-a-thon",
    "href": "lectures/1a.html#plot-a-thon",
    "title": "Weclome to Math 352",
    "section": "Plot-a-thon",
    "text": "Plot-a-thon"
  },
  {
    "objectID": "lectures/1a.html#plot-a-thon-1",
    "href": "lectures/1a.html#plot-a-thon-1",
    "title": "Weclome to Math 352",
    "section": "Plot-a-thon",
    "text": "Plot-a-thon"
  },
  {
    "objectID": "lectures/1a.html#learning-objectives",
    "href": "lectures/1a.html#learning-objectives",
    "title": "Weclome to Math 352",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPopulation\nSample\nInference\nAssociations"
  },
  {
    "objectID": "lectures/1a.html#population",
    "href": "lectures/1a.html#population",
    "title": "Weclome to Math 352",
    "section": "Population",
    "text": "Population\n\nA set of all measurements of interest to the sample collector."
  },
  {
    "objectID": "lectures/1a.html#sample",
    "href": "lectures/1a.html#sample",
    "title": "Weclome to Math 352",
    "section": "Sample",
    "text": "Sample\n\nA sample is any subset of measurements selected from the population."
  },
  {
    "objectID": "lectures/1a.html#inference",
    "href": "lectures/1a.html#inference",
    "title": "Weclome to Math 352",
    "section": "Inference",
    "text": "Inference\n\nParameter: a measurement describing the population\nStatistic: a measurement describing the sample"
  },
  {
    "objectID": "lectures/1a.html#associations",
    "href": "lectures/1a.html#associations",
    "title": "Weclome to Math 352",
    "section": "Associations",
    "text": "Associations\nAn association describes the relationship between two characteristics of a population."
  },
  {
    "objectID": "lectures/1c.html#r-programming",
    "href": "lectures/1c.html#r-programming",
    "title": "Introduction to R/RStudio",
    "section": "R Programming",
    "text": "R Programming\nR is a statistical programming package that allows you to conduct different types of analysis.\nR"
  },
  {
    "objectID": "lectures/1c.html#rstudio",
    "href": "lectures/1c.html#rstudio",
    "title": "Introduction to R/RStudio",
    "section": "RStudio",
    "text": "RStudio\nA piece of software that organizes how you conduct statistical analysis in R.\nRStudio"
  },
  {
    "objectID": "lectures/1c.html#posit-cloud",
    "href": "lectures/1c.html#posit-cloud",
    "title": "Introduction to R/RStudio",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nA web version of RStudio.\nPosit Cloud"
  },
  {
    "objectID": "lectures/1c.html#r-packages",
    "href": "lectures/1c.html#r-packages",
    "title": "Introduction to R/RStudio",
    "section": "R Packages",
    "text": "R Packages\n\nTidyverse"
  },
  {
    "objectID": "lectures/1c.html#r-as-a-calculator",
    "href": "lectures/1c.html#r-as-a-calculator",
    "title": "Introduction to R/RStudio",
    "section": "R as a calculator",
    "text": "R as a calculator\nR can evaluate different expressions in the console tab.\nTry the following:\n\n\\(4(4+2)/34\\)\n\\(6^3\\)\n\\(3-1\\)\n\\(4+4/3+45(32*34-54)\\)"
  },
  {
    "objectID": "lectures/1c.html#r-functions",
    "href": "lectures/1c.html#r-functions",
    "title": "Introduction to R/RStudio",
    "section": "R Functions",
    "text": "R Functions\nR functions performs tasks to specific data values.\nEvaluate the following values in R:\n\n\\(\\sqrt{3}\\)\n\\(e^3\\)\n\\(\\ln(53)\\)\n\\(\\log(324)\\)\n\\(\\sin(3)\\)\n\\(\\sin(3\\pi)\\)"
  },
  {
    "objectID": "lectures/1c.html#types-of-data",
    "href": "lectures/1c.html#types-of-data",
    "title": "Introduction to R/RStudio",
    "section": "Types of Data",
    "text": "Types of Data\n\nNumeric\nCharacter\nLogical\nMissing\n\nEvaluate the following code:"
  },
  {
    "objectID": "lectures/1c.html#types-of-objects",
    "href": "lectures/1c.html#types-of-objects",
    "title": "Introduction to R/RStudio",
    "section": "Types of Objects",
    "text": "Types of Objects\nIn R, an object contains a set of data. The most common types are vectors and matrix.\nRun this code and print out the objects in the console:"
  },
  {
    "objectID": "lectures/1c.html#data-frames",
    "href": "lectures/1c.html#data-frames",
    "title": "Introduction to R/RStudio",
    "section": "Data Frames",
    "text": "Data Frames\nData frames can be thought of as R’s version of a data set.\nPlay around with mtcars:\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "lectures/1c.html#lists",
    "href": "lectures/1c.html#lists",
    "title": "Introduction to R/RStudio",
    "section": "Lists",
    "text": "Lists\nList can be thought as an extended vector, but each element is a different R object.\nTry playing with this R object:"
  },
  {
    "objectID": "lectures/2b.html#motivation-intuition",
    "href": "lectures/2b.html#motivation-intuition",
    "title": "Random Variables",
    "section": "Motivation & Intuition",
    "text": "Motivation & Intuition\n\nProbability starts with experiments:\n\nRolling a die\nTossing a coin\nMeasuring temperature\n\nOutcomes may be descriptive (“heads”) or numeric (3).\nTo analyze mathematically, we assign numbers to outcomes.\n\nThis leads to random variables."
  },
  {
    "objectID": "lectures/2b.html#probability-space-the-foundation",
    "href": "lectures/2b.html#probability-space-the-foundation",
    "title": "Random Variables",
    "section": "Probability Space (The Foundation)",
    "text": "Probability Space (The Foundation)\nA probability space is a triple:\n\\[\n(\\Omega, \\mathcal{F}, P)\n\\]\n\nSample Space (\\(\\Omega\\))\nAll possible outcomes.\nSigma-Algebra (\\(\\mathcal{F}\\))\nCollection of events (subsets of \\(\\Omega\\)) closed under complements and unions.\nProbability Measure (\\(P\\))\nAssigns probabilities to events."
  },
  {
    "objectID": "lectures/2b.html#sample-space-omega",
    "href": "lectures/2b.html#sample-space-omega",
    "title": "Random Variables",
    "section": "Sample Space (\\(\\Omega\\))",
    "text": "Sample Space (\\(\\Omega\\))\n\nThe set of all possible outcomes of a random experiment.\nEach element of \\(\\Omega\\) is an elementary outcome."
  },
  {
    "objectID": "lectures/2b.html#examples",
    "href": "lectures/2b.html#examples",
    "title": "Random Variables",
    "section": "Examples",
    "text": "Examples\n\nCoin toss: \\(\\Omega = \\{H, T\\}\\)\nDie roll: \\(\\Omega = \\{1,2,3,4,5,6\\}\\)\nTwo coins: \\(\\Omega = \\{HH, HT, TH, TT\\}\\)\nHeight of a student: \\(\\Omega = [100,220]\\) cm"
  },
  {
    "objectID": "lectures/2b.html#sigma-algebra-mathcalf",
    "href": "lectures/2b.html#sigma-algebra-mathcalf",
    "title": "Random Variables",
    "section": "Sigma-Algebra (\\(\\mathcal{F}\\))",
    "text": "Sigma-Algebra (\\(\\mathcal{F}\\))\n\nA collection of subsets of \\(\\Omega\\) (called events) with special properties.\n\nRequirements:\n\n\\(\\Omega \\in \\mathcal{F}\\)\nIf \\(A \\in \\mathcal{F}\\), then \\(A^\\mathrm{C} \\in \\mathcal{F}\\)\nIf \\(A_1, A_2, \\dots \\in \\mathcal{F}\\), then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathcal{F}\\) and \\(\\bigcap_{i=1}^\\infty A_i \\in \\mathcal{F}\\)"
  },
  {
    "objectID": "lectures/2b.html#borel-sigma-field",
    "href": "lectures/2b.html#borel-sigma-field",
    "title": "Random Variables",
    "section": "Borel Sigma-Field",
    "text": "Borel Sigma-Field\n\nWhen \\(\\Omega = \\mathbb{R}\\) (or an interval), we need a sigma-algebra of subsets to define probabilities.\nNot every subset of \\(\\mathbb{R}\\) is “nice” (some are too pathological).\nThe Borel sigma-field provides a rigorous and practical choice."
  },
  {
    "objectID": "lectures/2b.html#definition",
    "href": "lectures/2b.html#definition",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\n\nThe Borel sigma-field on \\(\\mathbb{R}\\), denoted \\(\\mathcal{B}\\), is the smallest \\(\\sigma\\)-algebra based on semi-closed intervals:\n\n\\[\n(\\infty, b] = \\{x: -\\infty &lt; x \\leq \\infty \\}\n\\]"
  },
  {
    "objectID": "lectures/2b.html#mathcalb-contain",
    "href": "lectures/2b.html#mathcalb-contain",
    "title": "Random Variables",
    "section": "\\(\\mathcal{B}\\) Contain",
    "text": "\\(\\mathcal{B}\\) Contain\n\n\\((a,b)\\)\n\\([a,b]\\)\n\\([a,b)\\) and \\((a,b]\\)\nclosed under complement\nclosed under unions\nclosed under intersection"
  },
  {
    "objectID": "lectures/2b.html#probability-measure-p",
    "href": "lectures/2b.html#probability-measure-p",
    "title": "Random Variables",
    "section": "Probability Measure (\\(P\\))",
    "text": "Probability Measure (\\(P\\))\n\nA function \\(P: \\mathcal{F} \\to [0,1]\\) that assigns probabilities."
  },
  {
    "objectID": "lectures/2b.html#axioms",
    "href": "lectures/2b.html#axioms",
    "title": "Random Variables",
    "section": "Axioms",
    "text": "Axioms\n\nNon-negativity: \\(P(A) \\geq 0\\)\nNormalization: \\(P(\\Omega)=1\\)\nCountable additivity:\nIf \\(A_1, A_2, \\dots\\) are disjoint,\n\\[\nP\\Big(\\bigcup_{i=1}^\\infty A_i\\Big) = \\sum_{i=1}^\\infty P(A_i)\n\\]"
  },
  {
    "objectID": "lectures/2b.html#summary",
    "href": "lectures/2b.html#summary",
    "title": "Random Variables",
    "section": "Summary",
    "text": "Summary\n\nSample space (\\(\\Omega\\)): all possible outcomes\n\nSigma-algebra (\\(\\mathcal{F}\\)): collection of “allowable” events\n\nProbability measure (\\(P\\)): assigns probabilities consistently\n\nTogether: Probability Space = \\((\\Omega, \\mathcal{F}, P)\\)"
  },
  {
    "objectID": "lectures/2b.html#random-variables-1",
    "href": "lectures/2b.html#random-variables-1",
    "title": "Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\n\nA random variable is a measurable function:\n\n\\[\nX: \\Omega \\to \\mathbb{R}\n\\]\n\nFor any \\(a \\in \\mathbb{R}\\),\n\\(\\{ \\omega \\in \\Omega : X(\\omega) \\leq a \\} \\in \\mathcal{F}\\)."
  },
  {
    "objectID": "lectures/2b.html#types-of-random-variables",
    "href": "lectures/2b.html#types-of-random-variables",
    "title": "Random Variables",
    "section": "Types of Random Variables",
    "text": "Types of Random Variables\n\nDiscrete\n\nCountable values\n\nExample: Die roll, number of heads in 3 tosses\n\nContinuous\n\nAny value in an interval\n\nExample: Height, time, weight"
  },
  {
    "objectID": "lectures/2b.html#distribution-function",
    "href": "lectures/2b.html#distribution-function",
    "title": "Random Variables",
    "section": "Distribution Function",
    "text": "Distribution Function\nFor a random variable \\(X\\), the cumulative distribution function (CDF) is:\n\\[\nF_X(x) = P(X \\leq x), \\quad x \\in \\mathbb{R}\n\\]\nInterpretation:\n\\(F_X(x)\\) gives the probability that \\(X\\) takes a value less than or equal to \\(x\\)."
  },
  {
    "objectID": "lectures/2b.html#properties-of-a-distribution-function",
    "href": "lectures/2b.html#properties-of-a-distribution-function",
    "title": "Random Variables",
    "section": "Properties of a Distribution Function",
    "text": "Properties of a Distribution Function\n\n\\(F_X(x)\\) is non-decreasing.\n\\(\\lim_{x \\to -\\infty} F_X(x) = 0\\)\n\\(\\lim_{x \\to +\\infty} F_X(x) = 1\\)\n\\(F_X\\) is right-continuous: \\[\n\\lim_{t \\downarrow x} F_X(t) = F_X(x)\n\\]"
  },
  {
    "objectID": "lectures/2b.html#distributions",
    "href": "lectures/2b.html#distributions",
    "title": "Random Variables",
    "section": "Distributions",
    "text": "Distributions\n\nDiscrete: Probability Mass Function (PMF)\n\\[\nP(X=x) = p(x)\n\\]"
  },
  {
    "objectID": "lectures/2b.html#distributions-cont.",
    "href": "lectures/2b.html#distributions-cont.",
    "title": "Random Variables",
    "section": "Distributions (cont.)",
    "text": "Distributions (cont.)\n\nContinuous: Probability Density Function (PDF)\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x)\\,dx\n\\]"
  },
  {
    "objectID": "lectures/2b.html#expectation-variance",
    "href": "lectures/2b.html#expectation-variance",
    "title": "Random Variables",
    "section": "Expectation & Variance",
    "text": "Expectation & Variance\n\nExpectation (Mean):\n\nDiscrete:\n\\[\nE[X] = \\sum_x x \\, p(x)\n\\]\nContinuous:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x f(x)\\,dx\n\\]\n\nVariance:\n\\[\n\\text{Var}(X) = E[(X - E[X])^2]\n\\]"
  },
  {
    "objectID": "lectures/2b.html#expected-value-mean",
    "href": "lectures/2b.html#expected-value-mean",
    "title": "Random Variables",
    "section": "Expected Value (Mean)",
    "text": "Expected Value (Mean)\n\nThe long-run average value of a random variable.\n\nThink: if we repeated the experiment many times, what would the average outcome be?"
  },
  {
    "objectID": "lectures/2b.html#variance-spread-of-values",
    "href": "lectures/2b.html#variance-spread-of-values",
    "title": "Random Variables",
    "section": "Variance (Spread of Values)",
    "text": "Variance (Spread of Values)\n\nMeasures how spread out the values of \\(X\\) are around the mean.\n\nHigh variance = outcomes vary a lot.\n\nLow variance = outcomes are close to the mean."
  },
  {
    "objectID": "lectures/2b.html#common-random-variables",
    "href": "lectures/2b.html#common-random-variables",
    "title": "Random Variables",
    "section": "Common Random Variables",
    "text": "Common Random Variables\n\nBernoulli: Success (1) or failure (0), prob. \\(p\\)\n\nBinomial: #successes in \\(n\\) Bernoulli trials\n\nPoisson: #events in fixed time, rate \\(\\lambda\\)\n\nNormal (Gaussian): Bell curve\n\nUniform: Equal chance in an interval"
  },
  {
    "objectID": "lectures/3b.html#learning-outcomes",
    "href": "lectures/3b.html#learning-outcomes",
    "title": "Discrete Random Variable",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExpected Values\nVariances\nProperties of Expected Values"
  },
  {
    "objectID": "lectures/3b.html#expected-value",
    "href": "lectures/3b.html#expected-value",
    "title": "Discrete Random Variable",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of \\(Y\\) is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]\nwhere \\(P(y)\\) is the PMF of \\(Y\\)."
  },
  {
    "objectID": "lectures/3b.html#expected-value-1",
    "href": "lectures/3b.html#expected-value-1",
    "title": "Discrete Random Variable",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value of a function of a random variable \\(Y\\) is provided as\n\\[\nE\\{g(Y)\\} = \\sum_y g(y)P(y)\n\\]"
  },
  {
    "objectID": "lectures/3b.html#variance",
    "href": "lectures/3b.html#variance",
    "title": "Discrete Random Variable",
    "section": "Variance",
    "text": "Variance\nThe variance is the expected squared difference between the random variable and expected value.\n\\[\nVar(Y)= E[\\{Y-E(Y)\\}^2]=\\sum_y\\{y-E(Y)\\}^2P(y)\n\\]\n\\[\nVar(Y) = E(Y^2) - E(Y)^2\n\\]"
  },
  {
    "objectID": "lectures/3b.html#expected-value-2",
    "href": "lectures/3b.html#expected-value-2",
    "title": "Discrete Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/3b.html#variance-1",
    "href": "lectures/3b.html#variance-1",
    "title": "Discrete Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/3b.html#expected-value-3",
    "href": "lectures/3b.html#expected-value-3",
    "title": "Discrete Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/3b.html#variance-2",
    "href": "lectures/3b.html#variance-2",
    "title": "Discrete Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/3b.html#expected-value-4",
    "href": "lectures/3b.html#expected-value-4",
    "title": "Discrete Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/3b.html#variance-3",
    "href": "lectures/3b.html#variance-3",
    "title": "Discrete Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/3b.html#properties",
    "href": "lectures/3b.html#properties",
    "title": "Discrete Random Variable",
    "section": "Properties",
    "text": "Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(Y)\\}=cE\\{g(Y)\\}\\)\n\\(E\\{g_1(Y)+\\cdots+g_n(Y)\\}=E\\{g_1(Y)\\}+\\cdots+E\\{g_n(Y)\\}\\)"
  },
  {
    "objectID": "lectures/5a.html#learning-outcomes",
    "href": "lectures/5a.html#learning-outcomes",
    "title": "Continuous Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nContinuous Random Variables\n\nProbability Density Functions\nCumulative Density/Distribution Function\n\nCommon Distributions"
  },
  {
    "objectID": "lectures/5a.html#continuous-random-variable",
    "href": "lectures/5a.html#continuous-random-variable",
    "title": "Continuous Random Variables",
    "section": "Continuous Random Variable",
    "text": "Continuous Random Variable\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist.\n\n\n\n\n\n\n\n\n\nDistribution\nParameters\nPDF\n\n\n\n\nUniform\n\\(a\\) and \\(b\\)\n\\(\\frac{1}{b-a}\\)\n\n\nNormal\n\\(\\mu\\) and \\(\\sigma^2\\)\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)\n\n\nExponential\n\\(\\lambda\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\nGamma\n\\(\\alpha\\) and \\(\\beta\\)\n\\(\\frac{x^{\\alpha-1}e^{-x/\\beta}}{\\beta^\\alpha\\Gamma(\\alpha)}\\)\n\n\nBeta\n\\(\\alpha\\) and \\(\\beta\\)\n\\(\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\int^1_0x^{\\alpha-1}(1-x)^{\\beta-1}dx}\\)"
  },
  {
    "objectID": "lectures/5a.html#cumulative-density-function",
    "href": "lectures/5a.html#cumulative-density-function",
    "title": "Continuous Random Variables",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function"
  },
  {
    "objectID": "lectures/5a.html#probability-density-function",
    "href": "lectures/5a.html#probability-density-function",
    "title": "Continuous Random Variables",
    "section": "Probability Density Function",
    "text": "Probability Density Function\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)"
  },
  {
    "objectID": "lectures/5a.html#pdf-to-cdf",
    "href": "lectures/5a.html#pdf-to-cdf",
    "title": "Continuous Random Variables",
    "section": "PDF to CDF",
    "text": "PDF to CDF\n\\[\nF(x) = P(X \\leq x) = \\int_{-\\infty}^x f(t)\\,dt\n\\]"
  },
  {
    "objectID": "lectures/5a.html#obtaining-probability",
    "href": "lectures/5a.html#obtaining-probability",
    "title": "Continuous Random Variables",
    "section": "Obtaining Probability",
    "text": "Obtaining Probability\n\\[\n$P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx$\n\\]"
  },
  {
    "objectID": "lectures/5a.html#uniform-distribution-1",
    "href": "lectures/5a.html#uniform-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nA random variable is said to follow uniform distribution if the density function is constant between two parameters."
  },
  {
    "objectID": "lectures/5a.html#pdf",
    "href": "lectures/5a.html#pdf",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/5a.html#cdf",
    "href": "lectures/5a.html#cdf",
    "title": "Continuous Random Variables",
    "section": "CDF",
    "text": "CDF"
  },
  {
    "objectID": "lectures/5a.html#density-function",
    "href": "lectures/5a.html#density-function",
    "title": "Continuous Random Variables",
    "section": "Density Function",
    "text": "Density Function"
  },
  {
    "objectID": "lectures/5a.html#exponential-distribution-1",
    "href": "lectures/5a.html#exponential-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nAn exponential distribution is used to model positive continuous random variables, commonly used for time."
  },
  {
    "objectID": "lectures/5a.html#pdf-1",
    "href": "lectures/5a.html#pdf-1",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/5a.html#cdf-1",
    "href": "lectures/5a.html#cdf-1",
    "title": "Continuous Random Variables",
    "section": "CDF",
    "text": "CDF"
  },
  {
    "objectID": "lectures/5a.html#density-function-1",
    "href": "lectures/5a.html#density-function-1",
    "title": "Continuous Random Variables",
    "section": "Density Function",
    "text": "Density Function"
  },
  {
    "objectID": "lectures/5a.html#normal-distribution-1",
    "href": "lectures/5a.html#normal-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function."
  },
  {
    "objectID": "lectures/5a.html#pdf-2",
    "href": "lectures/5a.html#pdf-2",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/5a.html#density-function-2",
    "href": "lectures/5a.html#density-function-2",
    "title": "Continuous Random Variables",
    "section": "Density Function",
    "text": "Density Function"
  },
  {
    "objectID": "lectures/5a.html#gamma-distribution-1",
    "href": "lectures/5a.html#gamma-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\nA gamma distribution is the general form of distribution for a \\(\\chi^2\\)-distribution and exponential distribution."
  },
  {
    "objectID": "lectures/5a.html#pdf-3",
    "href": "lectures/5a.html#pdf-3",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/5a.html#density-function-3",
    "href": "lectures/5a.html#density-function-3",
    "title": "Continuous Random Variables",
    "section": "Density Function",
    "text": "Density Function"
  },
  {
    "objectID": "lectures/5a.html#beta-distribution-1",
    "href": "lectures/5a.html#beta-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nA beta distribution models a continuous random variable that only has support between the 0 and 1."
  },
  {
    "objectID": "lectures/5a.html#pdf-4",
    "href": "lectures/5a.html#pdf-4",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/5a.html#density-function-4",
    "href": "lectures/5a.html#density-function-4",
    "title": "Continuous Random Variables",
    "section": "Density Function",
    "text": "Density Function"
  },
  {
    "objectID": "lectures/6a.html#learning-outcomes",
    "href": "lectures/6a.html#learning-outcomes",
    "title": "MGF",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMoment Generating Functions\nMGF Properties"
  },
  {
    "objectID": "lectures/6a.html#moments",
    "href": "lectures/6a.html#moments",
    "title": "MGF",
    "section": "Moments",
    "text": "Moments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\)."
  },
  {
    "objectID": "lectures/6a.html#moment-generating-functions",
    "href": "lectures/6a.html#moment-generating-functions",
    "title": "MGF",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(t) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(t\\), and setting \\(t\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(t)}{dt}\\Bigg|_{t=0}\n\\]"
  },
  {
    "objectID": "lectures/6a.html#mgf-properties-linearity",
    "href": "lectures/6a.html#mgf-properties-linearity",
    "title": "MGF",
    "section": "MGF Properties: Linearity",
    "text": "MGF Properties: Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "lectures/6a.html#mgf-properties-uniqueness",
    "href": "lectures/6a.html#mgf-properties-uniqueness",
    "title": "MGF",
    "section": "MGF Properties: Uniqueness",
    "text": "MGF Properties: Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "lectures/6a.html#uniform-distribution-mgf",
    "href": "lectures/6a.html#uniform-distribution-mgf",
    "title": "MGF",
    "section": "Uniform Distribution MGF",
    "text": "Uniform Distribution MGF\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/6a.html#normal-distribution-mgf",
    "href": "lectures/6a.html#normal-distribution-mgf",
    "title": "MGF",
    "section": "Normal Distribution MGF",
    "text": "Normal Distribution MGF\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/6a.html#gamma-distribution-mgf",
    "href": "lectures/6a.html#gamma-distribution-mgf",
    "title": "MGF",
    "section": "Gamma Distribution MGF",
    "text": "Gamma Distribution MGF\n\\(X\\sim\\mathrm{Gamma}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}x^{\\alpha-1}e^{-x/\\beta}\\)"
  },
  {
    "objectID": "lectures/6a.html#chi2-distribution-mgf",
    "href": "lectures/6a.html#chi2-distribution-mgf",
    "title": "MGF",
    "section": "\\(\\chi^2\\)-Distribution MGF",
    "text": "\\(\\chi^2\\)-Distribution MGF\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/7b.html#learning-outcomes",
    "href": "lectures/7b.html#learning-outcomes",
    "title": "Functions of Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCovariance\nFunctions of Random Variables\nFinding PDFs using the distribution function\nFinding the PDF of a function of random variables\nUsing Moment Generating Functions"
  },
  {
    "objectID": "lectures/7b.html#covariance-1",
    "href": "lectures/7b.html#covariance-1",
    "title": "Functions of Random Variables",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(E(X_1)=\\mu_1\\) and \\(E(X_2)=\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/7b.html#correlation",
    "href": "lectures/7b.html#correlation",
    "title": "Functions of Random Variables",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/7b.html#function-of-random-variables-1",
    "href": "lectures/7b.html#function-of-random-variables-1",
    "title": "Functions of Random Variables",
    "section": "Function of Random Variables",
    "text": "Function of Random Variables\nIf \\(X\\) is a random variable with a known distribution and \\(Y = g(X)\\) is a function of \\(X\\),\nthen \\(Y\\) is also a random variable.\n\nWe can obtain the distribution of \\(Y\\) to obtain statistical properties.amp"
  },
  {
    "objectID": "lectures/7b.html#using-the-distribution-function",
    "href": "lectures/7b.html#using-the-distribution-function",
    "title": "Functions of Random Variables",
    "section": "Using the Distribution Function",
    "text": "Using the Distribution Function\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), the density function for the random variable \\(Y=g(X)\\) can be found with the following steps\n\n\nFind the region of \\(Y\\) in the space of \\(X\\), find \\(g^{-1}(y)\\)\nFind the region of \\(Y\\le y\\)\nFind \\(F_Y(y)=P(Y\\le y)\\) using the probability density function of \\(X\\) over region \\(Y\\le y\\)\nFind \\(f_Y(y)\\) by differentiating \\(F_Y(y)\\)"
  },
  {
    "objectID": "lectures/7b.html#example-1",
    "href": "lectures/7b.html#example-1",
    "title": "Functions of Random Variables",
    "section": "Example 1",
    "text": "Example 1\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n2x & 0\\le x \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=3X-1\\)?"
  },
  {
    "objectID": "lectures/7b.html#using-the-pdf",
    "href": "lectures/7b.html#using-the-pdf",
    "title": "Functions of Random Variables",
    "section": "Using the PDF",
    "text": "Using the PDF\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), if the random variable \\(Y=g(X)\\) is either increasing or decreasing, than the probability density function can be found as\n\\[\nf_Y(y) = f_X\\{g^{-1}(y)\\}\\left|\\frac{dg^{-1}(y)}{dy}\\right|\n\\]"
  },
  {
    "objectID": "lectures/7b.html#example-2",
    "href": "lectures/7b.html#example-2",
    "title": "Functions of Random Variables",
    "section": "Example 2",
    "text": "Example 2\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n\\frac{3}{2}x^2 + x & 0\\le x \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=5-(X/2)\\)?"
  },
  {
    "objectID": "lectures/7b.html#mgf-properties-linearity",
    "href": "lectures/7b.html#mgf-properties-linearity",
    "title": "Functions of Random Variables",
    "section": "MGF Properties: Linearity",
    "text": "MGF Properties: Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "lectures/7b.html#mgf-properties-uniqueness",
    "href": "lectures/7b.html#mgf-properties-uniqueness",
    "title": "Functions of Random Variables",
    "section": "MGF Properties: Uniqueness",
    "text": "MGF Properties: Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "lectures/7b.html#using-the-mgf",
    "href": "lectures/7b.html#using-the-mgf",
    "title": "Functions of Random Variables",
    "section": "Using the MGF",
    "text": "Using the MGF\nUsing the uniqueness property of Moment Generating Functions, for a random variable \\(X\\) with a known distribution function \\(F_X(x)\\) and random variable \\(Y=g(X)\\), the distribution of \\(Y\\) can be found by:\n\nFind the moment generating function of \\(Y\\), \\(M_Y(t)\\).\nCompare \\(M_Y(t)\\), with known moment generating functions. If \\(M_Y(t)=M_V(t)\\), for all values \\(t\\), them \\(Y\\) and \\(V\\) have identical distributions."
  },
  {
    "objectID": "lectures/7b.html#example-3",
    "href": "lectures/7b.html#example-3",
    "title": "Functions of Random Variables",
    "section": "Example 3",
    "text": "Example 3\nLet \\(X\\) follow a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Find the distribution of \\(Z=\\frac{X-\\mu}{\\sigma}\\)."
  },
  {
    "objectID": "lectures/7b.html#example-4",
    "href": "lectures/7b.html#example-4",
    "title": "Functions of Random Variables",
    "section": "Example 4",
    "text": "Example 4\nLet \\(Z\\) follow a standard normal distribution with mean \\(0\\) and variance \\(1\\). Find the distribution of \\(Y=Z^2\\)"
  },
  {
    "objectID": "lectures/8b.html#learning-outcomes",
    "href": "lectures/8b.html#learning-outcomes",
    "title": "Functions of Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nFunctions of Random Variables\nFinding PDFs using the distribution function\nFinding the PDF of a function of random variables\nUsing Moment Generating Functions"
  },
  {
    "objectID": "lectures/8b.html#function-of-random-variables-1",
    "href": "lectures/8b.html#function-of-random-variables-1",
    "title": "Functions of Random Variables",
    "section": "Function of Random Variables",
    "text": "Function of Random Variables"
  },
  {
    "objectID": "lectures/8b.html#using-the-distribution-function",
    "href": "lectures/8b.html#using-the-distribution-function",
    "title": "Functions of Random Variables",
    "section": "Using the Distribution Function",
    "text": "Using the Distribution Function\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), the density function for the random variable \\(Y=g(X)\\) can be found with the following steps\n\n\nFind the region of \\(Y\\) in the space of \\(X\\), find \\(g^{-1}(y)\\)\nFind the region of \\(Y\\le y\\)\nFind \\(F_Y(y)=P(Y\\le y)\\) using the probability density function of \\(X\\) over region \\(Y\\le y\\)\nFind \\(f_Y(y)\\) by differentiating \\(F_Y(y)\\)"
  },
  {
    "objectID": "lectures/8b.html#example-1",
    "href": "lectures/8b.html#example-1",
    "title": "Functions of Random Variables",
    "section": "Example 1",
    "text": "Example 1\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n2x & 0\\le x \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=3X-1\\)?"
  },
  {
    "objectID": "lectures/8b.html#using-the-pdf",
    "href": "lectures/8b.html#using-the-pdf",
    "title": "Functions of Random Variables",
    "section": "Using the PDF",
    "text": "Using the PDF\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), if the random variable \\(Y=g(X)\\) is either increasing or decreasing, than the probability density function can be found as\n\\[\nf_Y(y) = f_X\\{g^{-1}(y)\\}\\left|\\frac{dg^{-1}(y)}{dy}\\right|\n\\]"
  },
  {
    "objectID": "lectures/8b.html#example-2",
    "href": "lectures/8b.html#example-2",
    "title": "Functions of Random Variables",
    "section": "Example 2",
    "text": "Example 2\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n\\frac{3}{2}x^2 + x & 0\\le y \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=5-(X/2)\\)?"
  },
  {
    "objectID": "lectures/8b.html#mgf-properties-linearity",
    "href": "lectures/8b.html#mgf-properties-linearity",
    "title": "Functions of Random Variables",
    "section": "MGF Properties: Linearity",
    "text": "MGF Properties: Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "lectures/8b.html#mgf-properties-uniqueness",
    "href": "lectures/8b.html#mgf-properties-uniqueness",
    "title": "Functions of Random Variables",
    "section": "MGF Properties: Uniqueness",
    "text": "MGF Properties: Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "lectures/8b.html#using-the-mgf",
    "href": "lectures/8b.html#using-the-mgf",
    "title": "Functions of Random Variables",
    "section": "Using the MGF",
    "text": "Using the MGF\nUsing the uniqueness property of Moment Generating Functions, for a random variable \\(X\\) with a known distribution function \\(F_X(x)\\) and random variable \\(Y=g(X)\\), the distribution of \\(Y\\) can be found by:\n\nFind the moment generating function of \\(Y\\), \\(M_Y(t)\\).\nCompare \\(M_Y(t)\\), with known moment generating functions. If \\(M_Y(t)=M_V(t)\\), for all values \\(t\\), them \\(Y\\) and \\(V\\) have identical distributions."
  },
  {
    "objectID": "lectures/8b.html#example-3",
    "href": "lectures/8b.html#example-3",
    "title": "Functions of Random Variables",
    "section": "Example 3",
    "text": "Example 3\nLet \\(X\\) follow a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Find the distribution of \\(Z=\\frac{X-\\mu}{\\sigma}\\)."
  },
  {
    "objectID": "lectures/8b.html#example-4",
    "href": "lectures/8b.html#example-4",
    "title": "Functions of Random Variables",
    "section": "Example 4",
    "text": "Example 4\nLet \\(Z\\) follow a standard normal distribution with mean \\(0\\) and variance \\(1\\). Find the distribution of \\(Y=Z^2\\)"
  },
  {
    "objectID": "lectures/lecture_1.html#introductions",
    "href": "lectures/lecture_1.html#introductions",
    "title": "Weclome to Math 352",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "lectures/lecture_1.html#introductions-1",
    "href": "lectures/lecture_1.html#introductions-1",
    "title": "Weclome to Math 352",
    "section": "Introductions",
    "text": "Introductions\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "lectures/lecture_1.html#goals-for-the-course",
    "href": "lectures/lecture_1.html#goals-for-the-course",
    "title": "Weclome to Math 352",
    "section": "Goals for the Course",
    "text": "Goals for the Course\n\nIntroduction to Probability\nIntroduction to Statistics\nIntroduction to Regression"
  },
  {
    "objectID": "lectures/lecture_1.html#oh-traditional",
    "href": "lectures/lecture_1.html#oh-traditional",
    "title": "Weclome to Math 352",
    "section": "OH: Traditional",
    "text": "OH: Traditional\nBTE 2840 MW 5-6 PM"
  },
  {
    "objectID": "lectures/lecture_1.html#oh-casual",
    "href": "lectures/lecture_1.html#oh-casual",
    "title": "Weclome to Math 352",
    "section": "OH: Casual",
    "text": "OH: Casual\nBell Tower Rooftop Terrace Th 1:30 - 3 PM"
  },
  {
    "objectID": "lectures/lecture_1.html#oh-r-programming",
    "href": "lectures/lecture_1.html#oh-r-programming",
    "title": "Weclome to Math 352",
    "section": "OH: R Programming",
    "text": "OH: R Programming\nBTE 2810 F 10AM - 12PM"
  },
  {
    "objectID": "lectures/lecture_1.html#syllabus-1",
    "href": "lectures/lecture_1.html#syllabus-1",
    "title": "Weclome to Math 352",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus"
  },
  {
    "objectID": "lectures/lecture_1.html#homework-0",
    "href": "lectures/lecture_1.html#homework-0",
    "title": "Weclome to Math 352",
    "section": "Homework 0",
    "text": "Homework 0\nHomework 0 is posted!\nIt is due on February 3, 2023!\nSubmit your assignment on Canvas!"
  },
  {
    "objectID": "lectures/lecture_1.html#plot-a-thon",
    "href": "lectures/lecture_1.html#plot-a-thon",
    "title": "Weclome to Math 352",
    "section": "Plot-a-thon",
    "text": "Plot-a-thon"
  },
  {
    "objectID": "lectures/lecture_1.html#learning-objectives",
    "href": "lectures/lecture_1.html#learning-objectives",
    "title": "Weclome to Math 352",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPopulation\nSample\nInference\nAssociations"
  },
  {
    "objectID": "lectures/lecture_1.html#population",
    "href": "lectures/lecture_1.html#population",
    "title": "Weclome to Math 352",
    "section": "Population",
    "text": "Population\n\nA set of all measurements of interest to the sample collector."
  },
  {
    "objectID": "lectures/lecture_1.html#sample",
    "href": "lectures/lecture_1.html#sample",
    "title": "Weclome to Math 352",
    "section": "Sample",
    "text": "Sample\n\nA sample is any subset of measurements selected from the population."
  },
  {
    "objectID": "lectures/lecture_1.html#inference",
    "href": "lectures/lecture_1.html#inference",
    "title": "Weclome to Math 352",
    "section": "Inference",
    "text": "Inference\n\nParameter: a measurement describing the population\nStatistic: a measurement describing the sample"
  },
  {
    "objectID": "lectures/lecture_1.html#associations",
    "href": "lectures/lecture_1.html#associations",
    "title": "Weclome to Math 352",
    "section": "Associations",
    "text": "Associations\nAn association describes the relationship between two characteristics of a population."
  },
  {
    "objectID": "lectures/lecture_11.html#learning-outcomes",
    "href": "lectures/lecture_11.html#learning-outcomes",
    "title": "MGF of Continuous Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMoment Generating Functions\nMGF Properties"
  },
  {
    "objectID": "lectures/lecture_11.html#moments",
    "href": "lectures/lecture_11.html#moments",
    "title": "MGF of Continuous Random Variables",
    "section": "Moments",
    "text": "Moments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\)."
  },
  {
    "objectID": "lectures/lecture_11.html#moment-generating-functions",
    "href": "lectures/lecture_11.html#moment-generating-functions",
    "title": "MGF of Continuous Random Variables",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(t) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(t\\), and setting \\(t\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(t)}{dt}\\Bigg|_{t=0}\n\\]"
  },
  {
    "objectID": "lectures/lecture_11.html#mgf-properties-linearity",
    "href": "lectures/lecture_11.html#mgf-properties-linearity",
    "title": "MGF of Continuous Random Variables",
    "section": "MGF Properties: Linearity",
    "text": "MGF Properties: Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "lectures/lecture_11.html#mgf-properties-uniqueness",
    "href": "lectures/lecture_11.html#mgf-properties-uniqueness",
    "title": "MGF of Continuous Random Variables",
    "section": "MGF Properties: Uniqueness",
    "text": "MGF Properties: Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "lectures/lecture_11.html#uniform-distribution-mgf",
    "href": "lectures/lecture_11.html#uniform-distribution-mgf",
    "title": "MGF of Continuous Random Variables",
    "section": "Uniform Distribution MGF",
    "text": "Uniform Distribution MGF\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/lecture_11.html#normal-distribution-mgf",
    "href": "lectures/lecture_11.html#normal-distribution-mgf",
    "title": "MGF of Continuous Random Variables",
    "section": "Normal Distribution MGF",
    "text": "Normal Distribution MGF\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/lecture_11.html#gamma-distribution-mgf",
    "href": "lectures/lecture_11.html#gamma-distribution-mgf",
    "title": "MGF of Continuous Random Variables",
    "section": "Gamma Distribution MGF",
    "text": "Gamma Distribution MGF\n\\(X\\sim\\mathrm{Gamma}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}x^{\\alpha-1}e^{-x/\\beta}\\)"
  },
  {
    "objectID": "lectures/lecture_11.html#chi2-distribution-mgf",
    "href": "lectures/lecture_11.html#chi2-distribution-mgf",
    "title": "MGF of Continuous Random Variables",
    "section": "\\(\\chi^2\\)-Distribution MGF",
    "text": "\\(\\chi^2\\)-Distribution MGF\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#learning-outcomes",
    "href": "lectures/lecture_13.html#learning-outcomes",
    "title": "Joint Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nJoint Distributions\nMarginal Distributions\nConditional Distributions\nIndependence\nExpectations\nCovariance"
  },
  {
    "objectID": "lectures/lecture_13.html#partial-derivatives",
    "href": "lectures/lecture_13.html#partial-derivatives",
    "title": "Joint Distribution Functions",
    "section": "Partial Derivatives",
    "text": "Partial Derivatives\nFor a function \\(f(x,y)\\), the partial derivative with respect to \\(x\\) is taken by differentiating \\(f(x,y)\\) with respect to \\(x\\) while treating \\(y\\) as a constant. For example:\n\\(f(x,y) = x^2 + \\ln(y)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#multiple-integration",
    "href": "lectures/lecture_13.html#multiple-integration",
    "title": "Joint Distribution Functions",
    "section": "Multiple Integration",
    "text": "Multiple Integration\nMultiple integration is when you integrate a multivariate function by multiple variables. This is done by integrating the function by an individual variable at a time. For example:\n\\(f(x,y)=x^2 + y^2\\) which can be integrated as:"
  },
  {
    "objectID": "lectures/lecture_13.html#joint-distributions-1",
    "href": "lectures/lecture_13.html#joint-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Joint Distributions",
    "text": "Joint Distributions\nA joint distribution is a process where more than one random variable is generated; for example, collecting biomedical data, such as multiple biomarkers, are considered to follow a joint distribution. In mathematical terms, instead of dealing with a random variable, we are dealing with a random vector. Observing a particular random vector will have a probability attached to it."
  },
  {
    "objectID": "lectures/lecture_13.html#bivariate-discrete-distributions",
    "href": "lectures/lecture_13.html#bivariate-discrete-distributions",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Discrete Distributions",
    "text": "Bivariate Discrete Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, the joint distribution function of \\((X_1, X_2)\\) is defined as\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe properties of a bivariate discrete distribution are\n\n\\(p_{X_1,X_2}(x_1,x_2)\\ge 0\\) for all \\(x_1,\\ x_2\\)\n\\(\\sum_{x_1}\\sum_{x2}p(x_1,x_2)=1\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#bivariate-continuous-distribution",
    "href": "lectures/lecture_13.html#bivariate-continuous-distribution",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Continuous Distribution",
    "text": "Bivariate Continuous Distribution\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, the joint distribution function of \\((X_1, X_2)\\) is defined as\n\\[\nF_{X_1,X_2}(x_1, x_2) = P(X_1\\le x_1, X_2 \\le x_2).\n\\]\nThe properties of a bivariate continuous distribution are\n\n\\(f_{X_1,X_2}(x_1,x_2)=\\frac{\\partial^2F(x_1,x_2)}{\\partial x_1\\partial x_2}\\)\n\\(f_{X_1,X_2}(x_1, x_2)\\ge 0\\)\n\\(\\int_{x_1}\\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2dx_1=1\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#example",
    "href": "lectures/lecture_13.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf(x,y) \\left\\{\\begin{array}{cc}\n3x & 0\\le y\\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(P(0\\le X\\le 0.5,0.25\\le Y)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#marginal-density-functions",
    "href": "lectures/lecture_13.html#marginal-density-functions",
    "title": "Joint Distribution Functions",
    "section": "Marginal Density Functions",
    "text": "Marginal Density Functions\nA Marginal Density Function is density function of one random variable from a random vector."
  },
  {
    "objectID": "lectures/lecture_13.html#marginal-discrete-probability-mass-function",
    "href": "lectures/lecture_13.html#marginal-discrete-probability-mass-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Discrete Probability Mass Function",
    "text": "Marginal Discrete Probability Mass Function\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe marginal distribution of \\(X_1\\) is defined as\n\\[\np_{X_1}(x_1) = \\sum_{x_2}p_{X_1,X_2}(x_1,x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#marginal-continuous-density-function",
    "href": "lectures/lecture_13.html#marginal-continuous-density-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Continuous Density Function",
    "text": "Marginal Continuous Density Function\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The marginal distribution of \\(X_1\\) is defined as\n\\[\nf_{X_1}(x_1) = \\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#example-1",
    "href": "lectures/lecture_13.html#example-1",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf_{X,Y}(x,y) \\left\\{\\begin{array}{cc}\n2x & 0\\le y \\le 1;\\ 0 \\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(f_X(x)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#conditional-distributions-1",
    "href": "lectures/lecture_13.html#conditional-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable."
  },
  {
    "objectID": "lectures/lecture_13.html#discrete-conditional-distributions",
    "href": "lectures/lecture_13.html#discrete-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Discrete Conditional Distributions",
    "text": "Discrete Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#continuous-conditional-distributions",
    "href": "lectures/lecture_13.html#continuous-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Continuous Conditional Distributions",
    "text": "Continuous Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2=_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#example-2",
    "href": "lectures/lecture_13.html#example-2",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\nLet the joint density function of \\(X_1\\) and \\(X_2\\) be defined as\n\\[\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\nFind the conditional density function of \\(X_2|X_1=x_1\\)."
  },
  {
    "objectID": "lectures/lecture_13.html#independent-random-variables",
    "href": "lectures/lecture_13.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/lecture_13.html#discrete-independent-random-variables",
    "href": "lectures/lecture_13.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#continuous-independent-random-variables",
    "href": "lectures/lecture_13.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#matrix-algebra",
    "href": "lectures/lecture_13.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#example-3",
    "href": "lectures/lecture_13.html#example-3",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\nShow that \\(X\\perp Y\\).\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#expectations-1",
    "href": "lectures/lecture_13.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/lecture_13.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#conditional-expectations",
    "href": "lectures/lecture_13.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#conditional-expectations-1",
    "href": "lectures/lecture_13.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#covariance-1",
    "href": "lectures/lecture_13.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(E(X_1)=\\mu_1\\) and \\(E(X_2)=\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#correlation",
    "href": "lectures/lecture_13.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_15.html#learning-outcomes",
    "href": "lectures/lecture_15.html#learning-outcomes",
    "title": "Functions of Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nFunctions of Random Variables\nFinding PDFs using the distribution function\nFinding the PDF of a function of random variables\nUsing Moment Generating Functions"
  },
  {
    "objectID": "lectures/lecture_15.html#function-of-random-variables-1",
    "href": "lectures/lecture_15.html#function-of-random-variables-1",
    "title": "Functions of Random Variables",
    "section": "Function of Random Variables",
    "text": "Function of Random Variables"
  },
  {
    "objectID": "lectures/lecture_15.html#using-the-distribution-function",
    "href": "lectures/lecture_15.html#using-the-distribution-function",
    "title": "Functions of Random Variables",
    "section": "Using the Distribution Function",
    "text": "Using the Distribution Function\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), the density function for the random variable \\(Y=g(X)\\) can be found with the following steps\n\n\nFind the region of \\(Y\\) in the space of \\(X\\), find \\(g^{-1}(y)\\)\nFind the region of \\(Y\\le y\\)\nFind \\(F_Y(y)=P(Y\\le y)\\) using the probability density function of \\(X\\) over region \\(Y\\le y\\)\nFind \\(f_Y(y)\\) by differentiating \\(F_Y(y)\\)"
  },
  {
    "objectID": "lectures/lecture_15.html#example-1",
    "href": "lectures/lecture_15.html#example-1",
    "title": "Functions of Random Variables",
    "section": "Example 1",
    "text": "Example 1\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n2x & 0\\le x \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=3X-1\\)?"
  },
  {
    "objectID": "lectures/lecture_15.html#using-the-pdf",
    "href": "lectures/lecture_15.html#using-the-pdf",
    "title": "Functions of Random Variables",
    "section": "Using the PDF",
    "text": "Using the PDF\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), if the random variable \\(Y=g(X)\\) is either increasing or decreasing, than the probability density function can be found as\n\\[\nf_Y(y) = f_X\\{g^{-1}(y)\\}\\left|\\frac{dg^{-1}(y)}{dy}\\right|\n\\]"
  },
  {
    "objectID": "lectures/lecture_15.html#example-2",
    "href": "lectures/lecture_15.html#example-2",
    "title": "Functions of Random Variables",
    "section": "Example 2",
    "text": "Example 2\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n\\frac{3}{2}x^2 + x & 0\\le y \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=5-(X/2)\\)?"
  },
  {
    "objectID": "lectures/lecture_15.html#mgf-properties-linearity",
    "href": "lectures/lecture_15.html#mgf-properties-linearity",
    "title": "Functions of Random Variables",
    "section": "MGF Properties: Linearity",
    "text": "MGF Properties: Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "lectures/lecture_15.html#mgf-properties-uniqueness",
    "href": "lectures/lecture_15.html#mgf-properties-uniqueness",
    "title": "Functions of Random Variables",
    "section": "MGF Properties: Uniqueness",
    "text": "MGF Properties: Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "lectures/lecture_15.html#using-the-mgf",
    "href": "lectures/lecture_15.html#using-the-mgf",
    "title": "Functions of Random Variables",
    "section": "Using the MGF",
    "text": "Using the MGF\nUsing the uniqueness property of Moment Generating Functions, for a random variable \\(X\\) with a known distribution function \\(F_X(x)\\) and random variable \\(Y=g(X)\\), the distribution of \\(Y\\) can be found by:\n\nFind the moment generating function of \\(Y\\), \\(M_Y(t)\\).\nCompare \\(M_Y(t)\\), with known moment generating functions. If \\(M_Y(t)=M_V(t)\\), for all values \\(t\\), them \\(Y\\) and \\(V\\) have identical distributions."
  },
  {
    "objectID": "lectures/lecture_15.html#example-3",
    "href": "lectures/lecture_15.html#example-3",
    "title": "Functions of Random Variables",
    "section": "Example 3",
    "text": "Example 3\nLet \\(X\\) follow a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Find the distribution of \\(Z=\\frac{X-\\mu}{\\sigma}\\)."
  },
  {
    "objectID": "lectures/lecture_15.html#example-4",
    "href": "lectures/lecture_15.html#example-4",
    "title": "Functions of Random Variables",
    "section": "Example 4",
    "text": "Example 4\nLet \\(Z\\) follow a standard normal distribution with mean \\(0\\) and variance \\(1\\). Find the distribution of \\(Y=Z^2\\)"
  },
  {
    "objectID": "lectures/lecture_17.html#learning-outcomes",
    "href": "lectures/lecture_17.html#learning-outcomes",
    "title": "Maximum Likelihood Estimators",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMaximum Likelihood Estimators\nLog-Likelihood Functions"
  },
  {
    "objectID": "lectures/lecture_17.html#estimators",
    "href": "lectures/lecture_17.html#estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample."
  },
  {
    "objectID": "lectures/lecture_17.html#data",
    "href": "lectures/lecture_17.html#data",
    "title": "Maximum Likelihood Estimators",
    "section": "Data",
    "text": "Data\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)\\) where \\(F(\\cdot)\\) is a known distribution function and \\(\\boldsymbol\\theta\\) is a vector of parameters. Let \\(\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}\\), be the sample collected."
  },
  {
    "objectID": "lectures/lecture_17.html#likelihood-function",
    "href": "lectures/lecture_17.html#likelihood-function",
    "title": "Maximum Likelihood Estimators",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nUsing the joint pdf or pmf of the sample \\(\\boldsymbol X\\), the likelihood function is a function of \\(\\boldsymbol \\theta\\), given the observed data \\(\\boldsymbol X =\\boldsymbol x\\), defined as\n\\[\nL(\\boldsymbol \\theta|\\boldsymbol x)=f(\\boldsymbol x|\\boldsymbol \\theta)\n\\]\nIf the data is iid, then\n\\[\nf(\\boldsymbol x|\\boldsymbol \\theta) = \\prod^n_{i=1}f(x_i|\\boldsymbol\\theta)\n\\]"
  },
  {
    "objectID": "lectures/lecture_17.html#log-likelihood-function",
    "href": "lectures/lecture_17.html#log-likelihood-function",
    "title": "Maximum Likelihood Estimators",
    "section": "Log-Likelihood Function",
    "text": "Log-Likelihood Function\nIf \\(\\ln\\{L(\\boldsymbol \\theta)\\}\\) is monotone of \\(\\boldsymbol \\theta\\), then maximizing \\(\\ell(\\boldsymbol\\theta) = \\ln\\{L(\\boldsymbol \\theta)\\}\\) will yield the maximum likelihood estimators."
  },
  {
    "objectID": "lectures/lecture_17.html#maximum-log-likelihood-estimator",
    "href": "lectures/lecture_17.html#maximum-log-likelihood-estimator",
    "title": "Maximum Likelihood Estimators",
    "section": "Maximum log-Likelihood Estimator",
    "text": "Maximum log-Likelihood Estimator\nThe maximum likelihood estimator are the estimates of \\(\\boldsymbol \\theta\\) that maximize \\(\\ell(\\boldsymbol\\theta)\\)."
  },
  {
    "objectID": "lectures/lecture_17.html#poisson-distribution",
    "href": "lectures/lecture_17.html#poisson-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Pois}(\\lambda)\\), show that the MLE of \\(\\lambda\\) is \\(\\bar x\\)."
  },
  {
    "objectID": "lectures/lecture_17.html#normal-distribution",
    "href": "lectures/lecture_17.html#normal-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). Show that the MLE’s of \\(\\mu\\) and \\(\\sigma^2\\) are \\(\\bar x\\) and \\(\\frac{n-1}{n}s^2\\), respectively."
  },
  {
    "objectID": "lectures/lecture_17.html#exponential-distribution",
    "href": "lectures/lecture_17.html#exponential-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}Exp(\\lambda)\\). Find the MLE of \\(\\lambda\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#learning-outcomes",
    "href": "lectures/lecture_19.html#learning-outcomes",
    "title": "Linear Regression",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nScatter Plot\nLinear Regression\nOrdinary Least Squares\nUnbiasedness"
  },
  {
    "objectID": "lectures/lecture_19.html#scatter-plot-1",
    "href": "lectures/lecture_19.html#scatter-plot-1",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_19.html#scatter-plot-2",
    "href": "lectures/lecture_19.html#scatter-plot-2",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_19.html#linear-regression-1",
    "href": "lectures/lecture_19.html#linear-regression-1",
    "title": "Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is used to model the association between a set of predictor variables (x’s) and an outcome variable (y). Linear regression will fit a line that best describes the data points."
  },
  {
    "objectID": "lectures/lecture_19.html#simple-linear-regression",
    "href": "lectures/lecture_19.html#simple-linear-regression",
    "title": "Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nSimple linear regression will model the association between one predictor variable and an outcome:\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\\(\\beta_0\\): Intercept term\n\\(\\beta_1\\): Slope term\n\\(\\epsilon\\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#fitting-a-line",
    "href": "lectures/lecture_19.html#fitting-a-line",
    "title": "Linear Regression",
    "section": "Fitting a Line",
    "text": "Fitting a Line"
  },
  {
    "objectID": "lectures/lecture_19.html#interpretation",
    "href": "lectures/lecture_19.html#interpretation",
    "title": "Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\n\\hat y = 136.73 + 0.015 x\n\\]"
  },
  {
    "objectID": "lectures/lecture_19.html#ordinary-least-squares-1",
    "href": "lectures/lecture_19.html#ordinary-least-squares-1",
    "title": "Linear Regression",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nFor a data pair \\((X_i,Y_i)_{i=1}^n\\), the ordinary least squares estimator will find the estimates of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize the following function:\n\\[\n\\sum^n_{i=1}\\{y_i-(\\beta_0+\\beta_1x_i)\\}^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_19.html#estimating-betas",
    "href": "lectures/lecture_19.html#estimating-betas",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta\\)’s",
    "text": "Estimating \\(\\beta\\)’s"
  },
  {
    "objectID": "lectures/lecture_19.html#estimating-beta_1",
    "href": "lectures/lecture_19.html#estimating-beta_1",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta_1\\)",
    "text": "Estimating \\(\\beta_1\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#estimating-beta_0",
    "href": "lectures/lecture_19.html#estimating-beta_0",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta_0\\)",
    "text": "Estimating \\(\\beta_0\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#estimates",
    "href": "lectures/lecture_19.html#estimates",
    "title": "Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\n\\] \\[\n\\hat\\beta_1 = \\frac{\\sum^n_{i=1}(y_i-\\bar y)(x_i-\\bar x)}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\] \\[\n\\hat\\sigma^2 = \\frac{1}{n-2}\\sum^n_{i=1}(y_i-\\hat y_i)^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_19.html#unbiasedness-of-betas-1",
    "href": "lectures/lecture_19.html#unbiasedness-of-betas-1",
    "title": "Linear Regression",
    "section": "Unbiasedness of \\(\\beta\\)’s",
    "text": "Unbiasedness of \\(\\beta\\)’s\nBoth \\(\\beta_0\\) and \\(\\beta_1\\) are unbiased estimators."
  },
  {
    "objectID": "lectures/lecture_19.html#ebeta_0",
    "href": "lectures/lecture_19.html#ebeta_0",
    "title": "Linear Regression",
    "section": "\\(E(\\beta_0)\\)",
    "text": "\\(E(\\beta_0)\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#ebeta_1",
    "href": "lectures/lecture_19.html#ebeta_1",
    "title": "Linear Regression",
    "section": "\\(E(\\beta_1)\\)",
    "text": "\\(E(\\beta_1)\\)"
  },
  {
    "objectID": "lectures/lecture_20.html#learning-objectives",
    "href": "lectures/lecture_20.html#learning-objectives",
    "title": "Linear Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nMatrix Formulation\nMultiple Linear Regression\nModel Assumptions"
  },
  {
    "objectID": "lectures/lecture_20.html#matrix-version-of-model",
    "href": "lectures/lecture_20.html#matrix-version-of-model",
    "title": "Linear Regression",
    "section": "Matrix Version of Model",
    "text": "Matrix Version of Model\n\\[\nY_i = \\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta + \\epsilon_i\n\\]\n\n\\(Y_i\\): Outcome Variable\n\\(\\boldsymbol X_i=(1, X_i)^\\mathrm T\\): Predictors\n\\(\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T\\): Coefficients\n\\(\\epsilon_i\\): error term"
  },
  {
    "objectID": "lectures/lecture_20.html#data-matrix-formulation",
    "href": "lectures/lecture_20.html#data-matrix-formulation",
    "title": "Linear Regression",
    "section": "Data Matrix Formulation",
    "text": "Data Matrix Formulation\nFor \\(n\\) data points\n\\[\n\\boldsymbol Y = \\boldsymbol X^\\mathrm T\\boldsymbol \\beta + \\boldsymbol \\epsilon\n\\]\n\n\\(\\boldsymbol Y = (Y_1, \\cdots, Y_n)^\\mathrm T\\): Outcome Variable\n\\(\\boldsymbol X=(\\boldsymbol X_1, \\cdots, \\boldsymbol X_n)^\\mathrm T\\): Predictors\n\\(\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T\\): Coefficients\n\\(\\boldsymbol \\epsilon = (\\epsilon_1, \\cdots, \\epsilon_n)^\\mathrm T\\): Error terms"
  },
  {
    "objectID": "lectures/lecture_20.html#least-squares-formula",
    "href": "lectures/lecture_20.html#least-squares-formula",
    "title": "Linear Regression",
    "section": "Least Squares Formula",
    "text": "Least Squares Formula\n\\[\n(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)^\\mathrm T(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#estimates",
    "href": "lectures/lecture_20.html#estimates",
    "title": "Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat{\\boldsymbol \\beta} = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1}\\boldsymbol X ^\\mathrm T\\boldsymbol Y\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#mlr",
    "href": "lectures/lecture_20.html#mlr",
    "title": "Linear Regression",
    "section": "MLR",
    "text": "MLR\nMultivariable linear regression models are used when more than one explanatory variable is used to explain the outcome of interest."
  },
  {
    "objectID": "lectures/lecture_20.html#continuous-variable",
    "href": "lectures/lecture_20.html#continuous-variable",
    "title": "Linear Regression",
    "section": "Continuous Variable",
    "text": "Continuous Variable\nTo fit an additional continuous random variable to the model, we will only need to add it to the model:\n\\[\nY = \\beta_0 +\\beta_1 X_1 + \\beta_2 X_2\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#categorical-variable",
    "href": "lectures/lecture_20.html#categorical-variable",
    "title": "Linear Regression",
    "section": "Categorical Variable",
    "text": "Categorical Variable\nA categorical variable can be included in a model, but a reference category must be specified."
  },
  {
    "objectID": "lectures/lecture_20.html#fitting-a-model-with-categorical-variables",
    "href": "lectures/lecture_20.html#fitting-a-model-with-categorical-variables",
    "title": "Linear Regression",
    "section": "Fitting a model with categorical variables",
    "text": "Fitting a model with categorical variables\nTo fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use \\(C-1\\) dummy variables where \\(C\\) indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables."
  },
  {
    "objectID": "lectures/lecture_20.html#example",
    "href": "lectures/lecture_20.html#example",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nIf we have 4 categories, we will need 3 dummy variables:\n\n\n\n\nCat 1\nCat 2\nCat 3\nCat 4\n\n\n\n\nDummy 1\n1\n0\n0\n0\n\n\nDummy 2\n0\n1\n0\n0\n\n\nDummy 3\n0\n0\n1\n0\n\n\n\nWhich one is the reference category?"
  },
  {
    "objectID": "lectures/lecture_20.html#matrix-notation",
    "href": "lectures/lecture_20.html#matrix-notation",
    "title": "Linear Regression",
    "section": "Matrix Notation",
    "text": "Matrix Notation\n\\[\nY = \\boldsymbol \\beta^T\\boldsymbol X\n\\]\n\n\\(\\boldsymbol \\beta\\): a column vector of regression coefficients\n\\(\\boldsymbol X\\): a column vector of predictor variables"
  },
  {
    "objectID": "lectures/lecture_20.html#model",
    "href": "lectures/lecture_20.html#model",
    "title": "Linear Regression",
    "section": "Model",
    "text": "Model\n\\[\nY = \\boldsymbol \\beta^T\\boldsymbol X\n\\]\n\n\\(\\epsilon \\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "lectures/lecture_20.html#model-scatter-plot",
    "href": "lectures/lecture_20.html#model-scatter-plot",
    "title": "Linear Regression",
    "section": "Model Scatter Plot",
    "text": "Model Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_20.html#model-assumptions-1",
    "href": "lectures/lecture_20.html#model-assumptions-1",
    "title": "Linear Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nErrors are normally distributed\nConstant Variance\nLinearity\nIndependence\nNo outliers"
  },
  {
    "objectID": "lectures/lecture_20.html#errors-normally-distributed",
    "href": "lectures/lecture_20.html#errors-normally-distributed",
    "title": "Linear Regression",
    "section": "Errors Normally Distributed",
    "text": "Errors Normally Distributed"
  },
  {
    "objectID": "lectures/lecture_20.html#constant-variance",
    "href": "lectures/lecture_20.html#constant-variance",
    "title": "Linear Regression",
    "section": "Constant Variance",
    "text": "Constant Variance"
  },
  {
    "objectID": "lectures/lecture_20.html#linearity",
    "href": "lectures/lecture_20.html#linearity",
    "title": "Linear Regression",
    "section": "Linearity",
    "text": "Linearity"
  },
  {
    "objectID": "lectures/lecture_20.html#linearity-1",
    "href": "lectures/lecture_20.html#linearity-1",
    "title": "Linear Regression",
    "section": "Linearity",
    "text": "Linearity"
  },
  {
    "objectID": "lectures/lecture_20.html#no-outliers",
    "href": "lectures/lecture_20.html#no-outliers",
    "title": "Linear Regression",
    "section": "No Outliers",
    "text": "No Outliers"
  },
  {
    "objectID": "lectures/lecture_20.html#residual-analysis",
    "href": "lectures/lecture_20.html#residual-analysis",
    "title": "Linear Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nA residual analysis is used to assess the validity of the assumptions."
  },
  {
    "objectID": "lectures/lecture_22.html#learning-outcomes",
    "href": "lectures/lecture_22.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nEstimation Procedures\n\nRegression Coefficients\nDispersion Parameter\n\nNewton-Raphson Algorithm"
  },
  {
    "objectID": "lectures/lecture_22.html#estimating-boldsymbolbeta",
    "href": "lectures/lecture_22.html#estimating-boldsymbolbeta",
    "title": "Generalized Linear Models",
    "section": "Estimating \\(\\boldsymbol\\beta\\)",
    "text": "Estimating \\(\\boldsymbol\\beta\\)\nTo obtain the estimates of \\(\\boldsymbol \\beta\\) we can use the maximum log-likelihood approach to obtain \\(\\hat{\\boldsymbol\\beta}\\).\n\\[\nL(\\boldsymbol \\beta) =  \\prod^n_{i=1}f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#maximum-likelihood-approach",
    "href": "lectures/lecture_22.html#maximum-likelihood-approach",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood Approach",
    "text": "Maximum Likelihood Approach\n\\[\n\\ell(\\boldsymbol \\beta) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#numerical-approaches",
    "href": "lectures/lecture_22.html#numerical-approaches",
    "title": "Generalized Linear Models",
    "section": "Numerical Approaches",
    "text": "Numerical Approaches\n\nNewton-Rhapson Algorithm\nFisher-Scoring Algorithm\nNelder-Mead\nBFGS"
  },
  {
    "objectID": "lectures/lecture_22.html#estimating-phi-1",
    "href": "lectures/lecture_22.html#estimating-phi-1",
    "title": "Generalized Linear Models",
    "section": "Estimating \\(\\phi\\)",
    "text": "Estimating \\(\\phi\\)\nDepending on the random variable, the dispersion parameter will need to be estimated to conduct inference procedures. There are 4 methods to estimate the dispersion parameter:\n\nMaximum Likelihood\nMaximum (Modified) Profile Likelihood Approach\nMean Deviance Estimator\nPearson Estimator"
  },
  {
    "objectID": "lectures/lecture_22.html#maximum-likelihood-approach-1",
    "href": "lectures/lecture_22.html#maximum-likelihood-approach-1",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood Approach",
    "text": "Maximum Likelihood Approach\n\\[\n\\ell(\\phi) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#maximum-modified-profile-likelihood-approach",
    "href": "lectures/lecture_22.html#maximum-modified-profile-likelihood-approach",
    "title": "Generalized Linear Models",
    "section": "Maximum (Modified) Profile Likelihood Approach",
    "text": "Maximum (Modified) Profile Likelihood Approach\n\\[\n\\ell_p(\\phi) = \\frac{p}{2}\\log \\phi + \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\hat{\\boldsymbol \\beta},\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#mean-deviance-estimator",
    "href": "lectures/lecture_22.html#mean-deviance-estimator",
    "title": "Generalized Linear Models",
    "section": "Mean Deviance Estimator",
    "text": "Mean Deviance Estimator\n\\[\n\\tilde \\phi = \\frac{D(y,\\hat\\mu)}{n-p}\n\\]\n\n\\(D(y,\\hat\\mu)=2\\sum^n_{i=1}\\left\\{t(y,y) - t(y,\\mu) \\right\\}\\)\n\\(t(y,\\mu)=y\\theta-\\kappa(\\theta)\\)\n\\(p\\): number of regression coefficients"
  },
  {
    "objectID": "lectures/lecture_22.html#pearson-estimator",
    "href": "lectures/lecture_22.html#pearson-estimator",
    "title": "Generalized Linear Models",
    "section": "Pearson Estimator",
    "text": "Pearson Estimator\n\\[\n\\bar \\phi = \\frac{\\Lambda^2}{n-p}\n\\]\n\n\\(\\Lambda^2=\\sum^n_{i=1}\\frac{y_i-\\hat\\mu_i}{V(\\hat\\mu_i)}\\)\n\\(\\hat \\mu_i = g^{-1}(\\hat\\beta_0 + \\sum^n_{j=1}{X_{ij}\\hat\\beta_j})\\)\n\\(V(\\hat\\mu_i)=\\frac{d^2\\kappa(\\hat\\theta_i)}{d\\theta_i^2}\\)"
  },
  {
    "objectID": "lectures/lecture_22.html#numerical-algorithm",
    "href": "lectures/lecture_22.html#numerical-algorithm",
    "title": "Generalized Linear Models",
    "section": "Numerical Algorithm",
    "text": "Numerical Algorithm\nIn Mathematics and Statistics, numerical algorithms are used to approximate the value of different functions:\n\nRoot Finding:\n\nNewton’s Method\n\nDerivatives\n\nSecant Step-size\n\nIntegrals\n\nReimman Sums\n\nMaximization\n\nNewton-Raphson"
  },
  {
    "objectID": "lectures/lecture_22.html#optimization",
    "href": "lectures/lecture_22.html#optimization",
    "title": "Generalized Linear Models",
    "section": "Optimization",
    "text": "Optimization\nOptimization is the techniques used to find the values that maximizes the a function:\n\\[\nx_0 = \\mathrm{argmax}_{x}f(x)\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#newton-raphson",
    "href": "lectures/lecture_22.html#newton-raphson",
    "title": "Generalized Linear Models",
    "section": "Newton-Raphson",
    "text": "Newton-Raphson\nThe Newton-Raphson algorithm is used to estimate the parameters using an iterative algorithm. Given initial estimates, it will update the estimates of the parameters using the Newton step. It will continue iterating and updating the steps until the function converges to the maximum value."
  },
  {
    "objectID": "lectures/lecture_22.html#newton-raphson-1",
    "href": "lectures/lecture_22.html#newton-raphson-1",
    "title": "Generalized Linear Models",
    "section": "Newton-Raphson",
    "text": "Newton-Raphson\n\\[\n\\beta_j^{(it+1)} = \\beta_j^{(it)} - \\frac{G_{\\beta_j}^{(it)}}{H_{\\beta_j}^{(it)}}\n\\]\n\n\\(\\beta_j^{(it)}\\): current estimate of \\(\\beta_j\\)\n\\(G_{\\beta_j}^{(it)}=d\\ell(\\boldsymbol \\beta)/d\\beta_j|_{\\beta_j=\\beta_j^{(it)}}\\)\n\\(H_{\\beta_j}^{(it)}=d^2\\ell(\\boldsymbol \\beta)/d\\beta_j^2|_{\\beta_j=\\beta_j^{(it)}}\\)\n\\(\\beta_j^{(it+1)}\\): Updated estimate of \\(\\beta_j\\)"
  },
  {
    "objectID": "lectures/lecture_22.html#logistic-regression",
    "href": "lectures/lecture_22.html#logistic-regression",
    "title": "Generalized Linear Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet \\((Y_i,X_i)_{i=1}^n\\) be a data set where \\(Y_i\\overset{iid}{\\sim}Bernoulli(p)\\). Find the first and second derivative for \\(\\beta_1\\), when a GLM is fitted to the model."
  },
  {
    "objectID": "lectures/lecture_22.html#poisson-regression",
    "href": "lectures/lecture_22.html#poisson-regression",
    "title": "Generalized Linear Models",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nLet \\((Y_i,X_i)_{i=1}^n\\) be a data set where \\(Y_i\\overset{iid}{\\sim}Pois(\\lambda)\\). Find the first and second derivative for \\(\\beta_0\\), when a GLM is fitted to the model."
  },
  {
    "objectID": "lectures/lecture_25.html#learning-objectives",
    "href": "lectures/lecture_25.html#learning-objectives",
    "title": "Interpreting Output",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nR\nPython\nJulia\nSPSS\nStata\nSAS\nExcel\nMinitab"
  },
  {
    "objectID": "lectures/lecture_25.html#r",
    "href": "lectures/lecture_25.html#r",
    "title": "Interpreting Output",
    "section": "R",
    "text": "R"
  },
  {
    "objectID": "lectures/lecture_25.html#python",
    "href": "lectures/lecture_25.html#python",
    "title": "Interpreting Output",
    "section": "Python",
    "text": "Python"
  },
  {
    "objectID": "lectures/lecture_25.html#julia",
    "href": "lectures/lecture_25.html#julia",
    "title": "Interpreting Output",
    "section": "Julia",
    "text": "Julia"
  },
  {
    "objectID": "lectures/lecture_25.html#sas",
    "href": "lectures/lecture_25.html#sas",
    "title": "Interpreting Output",
    "section": "SAS",
    "text": "SAS"
  },
  {
    "objectID": "lectures/lecture_25.html#stata",
    "href": "lectures/lecture_25.html#stata",
    "title": "Interpreting Output",
    "section": "Stata",
    "text": "Stata"
  },
  {
    "objectID": "lectures/lecture_25.html#spss",
    "href": "lectures/lecture_25.html#spss",
    "title": "Interpreting Output",
    "section": "SPSS",
    "text": "SPSS"
  },
  {
    "objectID": "lectures/lecture_25.html#excel",
    "href": "lectures/lecture_25.html#excel",
    "title": "Interpreting Output",
    "section": "Excel",
    "text": "Excel"
  },
  {
    "objectID": "lectures/lecture_25.html#minitab",
    "href": "lectures/lecture_25.html#minitab",
    "title": "Interpreting Output",
    "section": "Minitab",
    "text": "Minitab"
  },
  {
    "objectID": "lectures/lecture_25.html#output",
    "href": "lectures/lecture_25.html#output",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1058.80  -259.27   -26.88   247.33  1288.69 \n#&gt; \n#&gt; Coefficients:\n#&gt;                    Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -5780.831    305.815  -18.90   &lt;2e-16 ***\n#&gt; flipper_length_mm    49.686      1.518   32.72   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 394.3 on 340 degrees of freedom\n#&gt;   (2 observations deleted due to missingness)\n#&gt; Multiple R-squared:  0.759,  Adjusted R-squared:  0.7583 \n#&gt; F-statistic:  1071 on 1 and 340 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/lecture_25.html#output-1",
    "href": "lectures/lecture_25.html#output-1",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-2",
    "href": "lectures/lecture_25.html#output-2",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-3",
    "href": "lectures/lecture_25.html#output-3",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-4",
    "href": "lectures/lecture_25.html#output-4",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-5",
    "href": "lectures/lecture_25.html#output-5",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-6",
    "href": "lectures/lecture_25.html#output-6",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-7",
    "href": "lectures/lecture_25.html#output-7",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_4.html#learning-outcomes",
    "href": "lectures/lecture_4.html#learning-outcomes",
    "title": "Introduction to Probability",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDescribe disjoint events\nDescribe a conditional probability\nDefine independent events\nLaw of Total Probability\nBaye’s Theorem"
  },
  {
    "objectID": "lectures/lecture_4.html#disjoint-events-1",
    "href": "lectures/lecture_4.html#disjoint-events-1",
    "title": "Introduction to Probability",
    "section": "Disjoint Events",
    "text": "Disjoint Events\nTwo events A and B are considered disjoint if \\(P(A\\cap B)=0\\). In general terms, only one event can occur, not both."
  },
  {
    "objectID": "lectures/lecture_4.html#diagram",
    "href": "lectures/lecture_4.html#diagram",
    "title": "Introduction to Probability",
    "section": "Diagram",
    "text": "Diagram"
  },
  {
    "objectID": "lectures/lecture_4.html#conditional-probability-1",
    "href": "lectures/lecture_4.html#conditional-probability-1",
    "title": "Introduction to Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nLet there be 2 events A and B. Given that B has occurred, what is the probability that A occurs? The conditional probability requires there to be at least 2 events and one event must have occurred. Additionally, the events cannot be disjoint. Conditional probabilities are denoted as \\(P(A|B)\\), the probability of A given B has occurred.\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_4.html#diagram-1",
    "href": "lectures/lecture_4.html#diagram-1",
    "title": "Introduction to Probability",
    "section": "Diagram",
    "text": "Diagram"
  },
  {
    "objectID": "lectures/lecture_4.html#example",
    "href": "lectures/lecture_4.html#example",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\n\n\n\nUses Eye Glasses\n\n\n\nNeeds Glasses\nYes\nNo\n\n\nYes\n44\n14\n\n\nNo\n2\n40\n\n\n\n\nFind the probability of needing glasses\nFind the probability of not using glasses\nFind the probability of not using glasses and needing glasses\nFind the probability of not using glasses, given they need glasses"
  },
  {
    "objectID": "lectures/lecture_4.html#work",
    "href": "lectures/lecture_4.html#work",
    "title": "Introduction to Probability",
    "section": "Work",
    "text": "Work"
  },
  {
    "objectID": "lectures/lecture_4.html#independent-events-1",
    "href": "lectures/lecture_4.html#independent-events-1",
    "title": "Introduction to Probability",
    "section": "Independent Events",
    "text": "Independent Events\nEvents A and B are considered independent if \\(P(A\\cap B)=P(A)P(B)\\). In other words, The occurrence of one event will not have an effect on the occurrence of the other event."
  },
  {
    "objectID": "lectures/lecture_4.html#example-1",
    "href": "lectures/lecture_4.html#example-1",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\n\n\n\nUses Eye Glasses\n\n\n\n\n\nNeeds Glasses\nYes\nNo\n\n\nYes\n44\n14\n\n\nNo\n2\n40"
  },
  {
    "objectID": "lectures/lecture_4.html#law-of-total-probability-1",
    "href": "lectures/lecture_4.html#law-of-total-probability-1",
    "title": "Introduction to Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\nThe law of total probability allows you to compute the probability of an event A given that the sets {\\(B_1\\), … , \\(B_n\\)} partitions event A. The law of total probability is given as\n\\[\nP(A)= \\sum^n_{i=1}P(A\\cap B_i)\n\\]\n\n\\[\nP(A)=\\sum^n_{i=1}P(A|B_i)P(B_i)\n\\]"
  },
  {
    "objectID": "lectures/lecture_4.html#diagrams",
    "href": "lectures/lecture_4.html#diagrams",
    "title": "Introduction to Probability",
    "section": "Diagrams",
    "text": "Diagrams"
  },
  {
    "objectID": "lectures/lecture_4.html#example-2",
    "href": "lectures/lecture_4.html#example-2",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nThe probability of an individual having a disease, given that they test positive for the disease, is 0.82. The probability of an individual having a disease, given they tested negative, is 0.14. The probability of testing positive is 0.6. What is the prevalence of a disease (probability of having a disease)?"
  },
  {
    "objectID": "lectures/lecture_4.html#bayes-theorem-1",
    "href": "lectures/lecture_4.html#bayes-theorem-1",
    "title": "Introduction to Probability",
    "section": "Baye’s Theorem",
    "text": "Baye’s Theorem\nBaye’s theorem computes the probability of an event \\(B_i\\) given event A\n\\[\nP(B_i|A) = \\frac{P(A\\cap B_i)}{P(A)}=\\frac{P(A|B_i)P(B_i)}{\\sum^n_{i=1}P(A|B_i)P(B_i)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_4.html#example-3",
    "href": "lectures/lecture_4.html#example-3",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nThe probability of an having a disease, given that they test positive for the disease, is 0.82. The probability of and individual having a disease, given they tested negative, is 0.14. The probability of testing positive is 0.6. What is the probability of resulting in a false negative?"
  },
  {
    "objectID": "lectures/lecture_6.html#learning-outcomes",
    "href": "lectures/lecture_6.html#learning-outcomes",
    "title": "Expectation of a Random Variable",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExpected Values\nVariances\nProperties of Expected Values"
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value",
    "href": "lectures/lecture_6.html#expected-value",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of \\(Y\\) is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]\nwhere \\(P(y)\\) is the PMF of \\(Y\\)."
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value-1",
    "href": "lectures/lecture_6.html#expected-value-1",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value of a function of a random variable \\(Y\\) is provided as\n\\[\nE\\{g(Y)\\} = \\sum_y g(y)P(y)\n\\]"
  },
  {
    "objectID": "lectures/lecture_6.html#variance",
    "href": "lectures/lecture_6.html#variance",
    "title": "Expectation of a Random Variable",
    "section": "Variance",
    "text": "Variance\nThe variance is the expected squared difference between the random variable and expected value.\n\\[\nVar(Y)= E[\\{Y-E(Y)\\}^2]=\\sum_y\\{y-E(Y)\\}^2P(y)\n\\]\n\\[\nVar(Y) = E(Y^2) - E(Y)^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value-2",
    "href": "lectures/lecture_6.html#expected-value-2",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/lecture_6.html#variance-1",
    "href": "lectures/lecture_6.html#variance-1",
    "title": "Expectation of a Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value-3",
    "href": "lectures/lecture_6.html#expected-value-3",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/lecture_6.html#variance-2",
    "href": "lectures/lecture_6.html#variance-2",
    "title": "Expectation of a Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value-4",
    "href": "lectures/lecture_6.html#expected-value-4",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/lecture_6.html#variance-3",
    "href": "lectures/lecture_6.html#variance-3",
    "title": "Expectation of a Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/lecture_6.html#properties",
    "href": "lectures/lecture_6.html#properties",
    "title": "Expectation of a Random Variable",
    "section": "Properties",
    "text": "Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(Y)\\}=cE\\{g(Y)\\}\\)\n\\(E\\{g_1(Y)+\\cdots+g_n(Y)\\}=E\\{g_1(Y)\\}+\\cdots+E\\{g_n(Y)\\}\\)"
  },
  {
    "objectID": "lectures/lecture_9.html#learning-outcomes",
    "href": "lectures/lecture_9.html#learning-outcomes",
    "title": "Continuous Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nContinuous Random Variables\n\nProbability Density Functions\nCumulative Density/Distribution Function\n\nCommon Distributions"
  },
  {
    "objectID": "lectures/lecture_9.html#continuous-random-variable",
    "href": "lectures/lecture_9.html#continuous-random-variable",
    "title": "Continuous Random Variables",
    "section": "Continuous Random Variable",
    "text": "Continuous Random Variable\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist.\n\n\n\n\n\n\n\n\n\nDistribution\nParameters\nPDF\n\n\n\n\nUniform\n\\(a\\) and \\(b\\)\n\\(\\frac{1}{b-a}\\)\n\n\nNormal\n\\(\\mu\\) and \\(\\sigma^2\\)\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)\n\n\nExponential\n\\(\\lambda\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\nGamma\n\\(\\alpha\\) and \\(\\beta\\)\n\\(\\frac{x^{\\alpha-1}e^{-x/\\beta}}{\\beta^\\alpha\\Gamma(\\alpha)}\\)\n\n\nBeta\n\\(\\alpha\\) and \\(\\beta\\)\n\\(\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\int^1_0x^{\\alpha-1}(1-x)^{\\beta-1}dx}\\)"
  },
  {
    "objectID": "lectures/lecture_9.html#cumulative-density-function",
    "href": "lectures/lecture_9.html#cumulative-density-function",
    "title": "Continuous Random Variables",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function"
  },
  {
    "objectID": "lectures/lecture_9.html#probability-density-function",
    "href": "lectures/lecture_9.html#probability-density-function",
    "title": "Continuous Random Variables",
    "section": "Probability Density Function",
    "text": "Probability Density Function\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)"
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-to-cdf",
    "href": "lectures/lecture_9.html#pdf-to-cdf",
    "title": "Continuous Random Variables",
    "section": "PDF to CDF",
    "text": "PDF to CDF"
  },
  {
    "objectID": "lectures/lecture_9.html#obtaining-probability",
    "href": "lectures/lecture_9.html#obtaining-probability",
    "title": "Continuous Random Variables",
    "section": "Obtaining Probability",
    "text": "Obtaining Probability"
  },
  {
    "objectID": "lectures/lecture_9.html#uniform-distribution-1",
    "href": "lectures/lecture_9.html#uniform-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nA random variable is said to follow uniform distribution if the density function is constant between two parameters."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf",
    "href": "lectures/lecture_9.html#pdf",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/lecture_9.html#cdf",
    "href": "lectures/lecture_9.html#cdf",
    "title": "Continuous Random Variables",
    "section": "CDF",
    "text": "CDF"
  },
  {
    "objectID": "lectures/lecture_9.html#exponential-distribution-1",
    "href": "lectures/lecture_9.html#exponential-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nAn exponential distribution is used to model positive continuous random variables, commonly used for time."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-1",
    "href": "lectures/lecture_9.html#pdf-1",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/lecture_9.html#cdf-1",
    "href": "lectures/lecture_9.html#cdf-1",
    "title": "Continuous Random Variables",
    "section": "CDF",
    "text": "CDF"
  },
  {
    "objectID": "lectures/lecture_9.html#normal-distribution-1",
    "href": "lectures/lecture_9.html#normal-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-2",
    "href": "lectures/lecture_9.html#pdf-2",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/lecture_9.html#gamma-distribution-1",
    "href": "lectures/lecture_9.html#gamma-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\nA gamma distribution is the general form of distribution for a \\(\\chi^2\\)-distribution and exponential distribution."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-3",
    "href": "lectures/lecture_9.html#pdf-3",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/lecture_9.html#beta-distribution-1",
    "href": "lectures/lecture_9.html#beta-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nA beta distribution models a continuous random variable that only has support between the 0 and 1."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-4",
    "href": "lectures/lecture_9.html#pdf-4",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "posts/week_10.html",
    "href": "posts/week_10.html",
    "title": "Week 10",
    "section": "",
    "text": "Maximum Likelihood Estimators\n\n\n\n\n\nMLE: Properties\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\n\n\n\n002\nVideo"
  },
  {
    "objectID": "posts/week_10.html#learning-outcomes",
    "href": "posts/week_10.html#learning-outcomes",
    "title": "Week 10",
    "section": "",
    "text": "Maximum Likelihood Estimators\n\n\n\n\n\nMLE: Properties\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\n\n\n\n002\nVideo"
  },
  {
    "objectID": "posts/week_2.html",
    "href": "posts/week_2.html",
    "title": "Week 2",
    "section": "",
    "text": "Describe disjoint events\nDescribe a conditional probability\nDefine independent events\nLaw of Total Probability\nBaye’s Theorem\n\n\n\n\n\nDefine Random Variables\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nUnavailable"
  },
  {
    "objectID": "posts/week_2.html#learning-outcomes",
    "href": "posts/week_2.html#learning-outcomes",
    "title": "Week 2",
    "section": "",
    "text": "Describe disjoint events\nDescribe a conditional probability\nDefine independent events\nLaw of Total Probability\nBaye’s Theorem\n\n\n\n\n\nDefine Random Variables\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nUnavailable"
  },
  {
    "objectID": "posts/week_2.html#important-concepts",
    "href": "posts/week_2.html#important-concepts",
    "title": "Week 2",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nExperiment\nA process that yields an outcome randomly.\n\n\nSample Space\nThe sample space (S) is all the possible outcomes from an experiment.\n\n\nEvent\nAn event is a collection of outcomes of an experiment that is any subset of the sample space S (including S).\n\n\nRandom Variable\nA random variable (rv) is a function that maps a sample space to the real numbers. A rv is denoted by a capital letter. The observed value is denoted with a lower-case letter.\n\n\nProbability (Cumulative) Distribution Function\nThe cumulative distribution function of a random variable \\(X\\), expressed as \\(F_X(x)\\), is defined as\n\\[\nF_X(x) = P(X\\leq x),\n\\]\nproviding the probability of observing \\(x\\) or lower.\n\nProbability Mass Function (Categorical Variable1)\nThe probability mass function of a random variable, expressed as \\(f_X(x)\\), is expressed as\n\\[\nf_X(x)=P(X=x),\n\\]\nproviding the probability of observing \\(x\\).\n\n\n\nSecond Lecture\n\nDisjoint Events\nTwo events A and B are considered disjoint if \\(P(A\\cap B)=0\\). In general terms, only one event can occur, not both.\n\n\nConditional Probability\nLet there be 2 events A and B. Given that B has occurred, what is the probability that A occurs? The conditional probability requires there to be at least 2 events and one event must have occurred. Additionally, the events cannot be disjoint. Conditional probabilities are denoted as $P(A|B)$, the probability of A given B has occurred.\n\n\\(P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\)\n\n\n\nIndependent Events\nEvents A and B are considered independent if \\(P(A\\cap B)=P(A)P(B)\\). In other words, The occurrence of one event will not affect the occurrence of the other event.\n\n\nLaw of Total Probability\nThe law of total probability allows you to compute the probability of an event A given that the sets {\\(B_1\\), …, \\(B_n\\)} partitions event A. The law of total probability is given as\n\\[\nP(A) = \\sum^n_{i=1}P(A\\cap B_i) = \\sum^n_{i=1}P(A|B_i)P(B_i)\n\\]\n\n\nBaye’s Theorem\nBaye’s theorem computes the probability of an event \\(B_i\\) given event A\n\\[\nP(B_i|A) = \\frac{P(A\\cap B_i)}{P(A)}=\\frac{P(A|B_i)P(B_i)}{\\sum^n_{i=1}P(A|B_i)P(B_i)}\n\\]"
  },
  {
    "objectID": "posts/week_2.html#footnotes",
    "href": "posts/week_2.html#footnotes",
    "title": "Week 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe will define this later on in the semester.↩︎"
  },
  {
    "objectID": "posts/week_4.html",
    "href": "posts/week_4.html",
    "title": "Week 4",
    "section": "",
    "text": "Moment Generating Functions\nExpected Value Properties\n\n\n\n\n\nExpected Values\nMGF\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_4.html#learning-outcomes",
    "href": "posts/week_4.html#learning-outcomes",
    "title": "Week 4",
    "section": "",
    "text": "Moment Generating Functions\nExpected Value Properties\n\n\n\n\n\nExpected Values\nMGF\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_4.html#important-concepts",
    "href": "posts/week_4.html#important-concepts",
    "title": "Week 4",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nFirst Lecture\n\nMoments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\).\n\n\nMoment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(t) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(t\\), and setting \\(t\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(t)}{dt}\\Bigg|_{t=0}\n\\]"
  },
  {
    "objectID": "posts/week_6.html",
    "href": "posts/week_6.html",
    "title": "Week 6",
    "section": "",
    "text": "Moment Generating Functions\nMGF Properties\n\n\n\n\n\nR Lab Continuous Random Variables\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_6.html#learning-outcomes",
    "href": "posts/week_6.html#learning-outcomes",
    "title": "Week 6",
    "section": "",
    "text": "Moment Generating Functions\nMGF Properties\n\n\n\n\n\nR Lab Continuous Random Variables\n\n\n\n\nTuesday Slides | Thursday Slides\n\n\n\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\nVideo\n\n\n002\nVideo\nVideo"
  },
  {
    "objectID": "posts/week_6.html#important-concepts",
    "href": "posts/week_6.html#important-concepts",
    "title": "Week 6",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nFirst Lecture\n\nMoments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\).\n\n\nMoment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(c) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(c\\), and setting \\(c\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(c)}{dc}\\Bigg|_{c=0}\n\\]\n\n\nMGF Properties\n\nLinearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]\nLet \\(X\\) and \\(Y\\) be two random variables with MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively, and are independent. The MGF of \\(U=X-Y\\)\n\\[\nM_U(t) = M_X(t)M_Y(-t)\n\\]\n\n\nUniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "posts/week_8.html",
    "href": "posts/week_8.html",
    "title": "Week 8",
    "section": "",
    "text": "Sampling Statistics\n\n\n\n\n\nExam\n\n\n\n\nTuesday Slides ### Videos\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\n\n\n\n002\nVideo"
  },
  {
    "objectID": "posts/week_8.html#learning-outcomes",
    "href": "posts/week_8.html#learning-outcomes",
    "title": "Week 8",
    "section": "",
    "text": "Sampling Statistics\n\n\n\n\n\nExam\n\n\n\n\nTuesday Slides ### Videos\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\nVideo\n\n\n\n002\nVideo"
  },
  {
    "objectID": "r.html",
    "href": "r.html",
    "title": "R Labs",
    "section": "",
    "text": "Below are the R Labs for the course. Make sure you complete and submit your QMD file by the deadline on Canvas.\n\n\n\n\n\n\n\n\nNo matching items"
  }
]