[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Math 352: Probability and Statistics",
    "section": "",
    "text": "Term: Fall 2025\nInstructor: Isaac Quintanilla Salinas\nEmail: isaac.qs@csuci.edu\nOffice Location: Marin Hall 2326\nOffice Hours:\n\nTue/Thur 5-6 PM\nWed 2-4 PM\n\nOr by Zoom appointment.\nLecture:\n\nSec 01: T/TH 10:30-11:45 AM\nSec 02: T/TH 3-4:45 PM\n\nPre-Requisites: MATH 151"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Math 352: Probability and Statistics",
    "section": "Course Description",
    "text": "Course Description\nStatistics is the science of reasoning from data. It is both an exciting intellectual discipline and a powerful scientific tool. Statistics is a mathematical science, in the sense that it makes use of mathematics extensively, but it is not a branch of mathematics.\nThe practice of statistics involves collecting data, analyzing data, and drawing inferences from data. The mathematical foundations of statistical inference lie in probability, the study of randomness and uncertainty. This course introduces you to fundamental ideas and methods of probability and statistics."
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Math 352: Probability and Statistics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand and apply basic ideas and methods of probability, including conditional probability and random variables.\nApply and interpret the results of a variety of statistical techniques, including both descriptive and inferential methods;\nUnderstand many of the fundamental ideas of statistics, such as variability, distribution, sampling, confidence, and significance;\nUse statistical software to conduct simulations and analyze data;\nCommunicate your knowledge of probability and statistics effectively."
  },
  {
    "objectID": "syllabus.html#recommended-texts",
    "href": "syllabus.html#recommended-texts",
    "title": "Math 352: Probability and Statistics",
    "section": "Recommended Texts",
    "text": "Recommended Texts\nProbability for Data Science and Course Materials"
  },
  {
    "objectID": "syllabus.html#required-software",
    "href": "syllabus.html#required-software",
    "title": "Math 352: Probability and Statistics",
    "section": "Required Software",
    "text": "Required Software\nFor this course, we will use R, RMarkdown, and RStudio. Please download and install on your computer.\n\nR is a free statistical software program that is available for download at: https://www.r-project.org/.\nQuarto is a scientific documentation system used to write scientific documentation. Quarto is freely available at: https://quarto.org/\nRStudio provides free and open source tools for your data analysis in R: https://www.rstudio.com/"
  },
  {
    "objectID": "syllabus.html#course-grading",
    "href": "syllabus.html#course-grading",
    "title": "Math 352: Probability and Statistics",
    "section": "Course Grading",
    "text": "Course Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n20%\n\n\nR Assignments\n20%\n\n\nIn-Person Exam 1\n15%\n\n\nIn-Person Exam 2\n15%\n\n\nTake-Home Exam 1\n15%\n\n\nTake-Home Exam 2\n15%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\nHomework\nHomework will be assigned on a regular basis and posted on https://m352.inqs.info/hw.html and CANVAS. The homework is to help you practice the concepts learned in lecture and to help you study. You must turn in your own individual homework and show your understanding of the material. At the end of the semester, the two lowest homework grades will be dropped. Late work will be accepted, but with a 25% penalty. The last day late work will be accepted is on 12/5/2025 at 11:59 PM.\n\n\nR Labs\nThe objective of the labs are to develop both your statistical and programming skills. We will work on these labs in class. The lowest R Lab will be dropped.\n\n\nExams\nThere will be four exams this semester, two in-person exams and two take-home exams. While the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success. At the end of the semester, your lowest exam grade will be replaced by the median of all exam grades.\nThis course will operate under a zero-tolerance policy. Talking during the time of the exam, sharing materials, looking at another students’ exam, or not following directions given will be subject to the University’s academic integrity policy.\n\n\nExtra Credit\nThere will be 4 extra credit opportunities worth a total of 10% of your overall grade. There are no make-ups for missed extra credit assignments!"
  },
  {
    "objectID": "syllabus.html#class-schedule",
    "href": "syllabus.html#class-schedule",
    "title": "Math 352: Probability and Statistics",
    "section": "Class Schedule",
    "text": "Class Schedule\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nAssignments/Exams\nReading\n\n\n\n\n8/25\nIntro to Course/Intro to R\nHW #0\nHandouts\n\n\n9/1\nIntroduction to Probability\nHW #1\n2.1 - 2.4\n\n\n9/8\nDiscrete Random Variables\nHW #2\n3.1 - 3.5\n\n\n9/15\nDiscrete Random Variables\nTake-Home Exam #1\n4.1 - 4.3, 4.5, 4.6\n\n\n9/22\nContinuous Random Variables\nHW #3\n5.1 - 5.4\n\n\n9/29\nContinuous Random Variables\nHW #4\n4.7, 5.5, 5.6, 6.1\n\n\n10/6\nJoint Distributions\nHW #5\n5.1 - 5.4\n\n\n10/13\nExam # 1/Sampling Statistics\n\n\n\n\n10/20\nSampling Statistics\n\n6.3 - 6.4\n\n\n10/27\nMaximum Likelihood Estimation\nHW #6\n8.1 - 8.2\n\n\n11/3\nRegression\nHW #7\n7.1\n\n\n11/10\nRegression\nHW #8\n7.2\n\n\n11/17\nStandard Errors\nTake-Home Exam #2\nHandouts\n\n\n11/24\nHypothesis Testing\nHW #9\n9.3\n\n\n12/1\nConfidence Intervals\nHW #10\n9.2 - 9.4\n\n\n12/8\nExam #2\n\n\n\n\n\n\nGenerative Artificial Intelligence Policy\nThe use of generative artificial intelligence (AI) in an ethical manner is permitted for this course.\n\nPermitted Uses\nYou may use AI for: - Obtain clarification - Brainstorming ideas, examples, outlines, and strategies - Generating questions for practice or exploration - Identifying keywords or phrasing to match professional goals\n\n\nProhibited Uses\nYou may not: - Submit AI-generated work - Use AI to complete assignments, quizzes, exams, or other assessments meant to reflect only your own work - Use AI to generate code\nAny AI-generated work will receive a 0 in the class. Severe cases will be reported to Academic Misconduct."
  },
  {
    "objectID": "syllabus.html#university-policies",
    "href": "syllabus.html#university-policies",
    "title": "Math 352: Probability and Statistics",
    "section": "University Policies",
    "text": "University Policies\n\nAcademic Honesty:\nPlease conduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials are allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\n\n\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\n\n\nDisruption\n\nIf I Am Out: I will communicate via email and will hold classes asynchronously.\nIf You Are Out: Contact me as soon as possible to talk about your options. Reasonable accommodations will be provided for a brief absence. With proper documentation, extended accommodations will be provided."
  },
  {
    "objectID": "posts/week_1.html",
    "href": "posts/week_1.html",
    "title": "Week 1",
    "section": "",
    "text": "Define Population\nDefine Sample\nDefine Inference\nDefine Association\n\n\n\n\n\nInstalling R and RStudio\nScripts\nR Packages\nR Environment"
  },
  {
    "objectID": "posts/week_1.html#learning-outcomes",
    "href": "posts/week_1.html#learning-outcomes",
    "title": "Week 1",
    "section": "",
    "text": "Define Population\nDefine Sample\nDefine Inference\nDefine Association\n\n\n\n\n\nInstalling R and RStudio\nScripts\nR Packages\nR Environment"
  },
  {
    "objectID": "posts/week_1.html#resources",
    "href": "posts/week_1.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\n\nLecture\nTuesday Slides Thursday Slides\n\n\nVideos\n\n\n\nSection\nTuesday\nThursday\n\n\n\n\n001\n\n\n\n\n002"
  },
  {
    "objectID": "posts/week_1.html#important-concepts",
    "href": "posts/week_1.html#important-concepts",
    "title": "Week 1",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nFirst Lecture\n\nPopulation\nA set of all measurements of interest to the sample collector.\n\n\nSample\nA sample is any subset of measurements selected from the population.\n\n\nInference\n\nParameter: a measurement describing the population\nStatistic: a measurement describing the sample\n\n\n\nAssociation\nAn association describes the relationship between two characteristics of a population.\n\n\n\nSecond Lecture\n\nAccessing R & RStudio\nIf you are on a tablet or Chromebook, you can access R & RStudio via rstudio.cloud for free. However, they have limited computing resources. Be mindful of your experimentation. You may also be able to use Quarto in Rstudio cloud.\nYou can install R via their website: https://www.r-project.org/.\nYou can install RStudio for free from their website: https://www.rstudio.com/products/rstudio/download/\n\n\nUsing R\nR can be used as a calculator; below are a few examples:\n\n1+2\n\n[1] 3\n\n3/4\n\n[1] 0.75\n\n9*8\n\n[1] 72\n\nexp(4)\n\n[1] 54.59815\n\n\n\n\nR Functions\nR has specialized functions that can compute specific values. R functions require inputs, known as arguments, to produce a specific output.\nFor example, the log() function can be used to compute the natural logarithm of a specified input:\n\nlog(34)\n\n[1] 3.526361\n\n\nIf you want to know information about a specific function, you can use the ? operator:\n\n?log\n\nwhich will open the help tab. Notice there are 2 arguments: x and base. This means that the log() function can be extended to other base. To use common log1, specify the arguments:\n\nlog(x=34, base=10)\n\n[1] 1.531479\n\n\nNotice that I specified the arguments. You can also type this:\n\nlog(34, 10)\n\n[1] 1.531479\n\n\nwhich produces the same results. This is because R uses positions in the function to determine argument values; therefore, if the positions are correct, you do not need to specify the argument name.\nGoing back to the First Lecture example, log(34), we did not specify the base. This is because functions have default values for arguments. The help documentation tells us what arguments have defaults and do not need to be specified.\n\n\nInstall packages\nYou can extend the functionality of R. The tidyverse package includes a popular set of R packages for data wrangling and analysis. To install tidyverse, use the install.packages() function2:\n\ninstall.packages('tidyverse')\n\nOnce you installed the R package, you will need to load with every R session using the library() function:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/week_1.html#footnotes",
    "href": "posts/week_1.html#footnotes",
    "title": "Week 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(\\log_{10}(x)\\)↩︎\nThe package name must be inputted with quotes in the function.↩︎"
  },
  {
    "objectID": "lectures/lecture_7.html#learning-outcomes",
    "href": "lectures/lecture_7.html#learning-outcomes",
    "title": "Moment Generating Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMoments\nMoment Generating Functions\nProperties"
  },
  {
    "objectID": "lectures/lecture_7.html#moments",
    "href": "lectures/lecture_7.html#moments",
    "title": "Moment Generating Functions",
    "section": "Moments",
    "text": "Moments\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of \\(Y\\) is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]\nwhere \\(P(y)\\) is the PMF of \\(Y\\)."
  },
  {
    "objectID": "lectures/lecture_7.html#moment-generating-functions",
    "href": "lectures/lecture_7.html#moment-generating-functions",
    "title": "Moment Generating Functions",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(c) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(c\\), and setting \\(c\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(c)}{dc}\\Bigg|_{c=0}\n\\]"
  },
  {
    "objectID": "lectures/lecture_7.html#mgf",
    "href": "lectures/lecture_7.html#mgf",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/lecture_7.html#ey2",
    "href": "lectures/lecture_7.html#ey2",
    "title": "Moment Generating Functions",
    "section": "\\(E(Y^2)\\)",
    "text": "\\(E(Y^2)\\)"
  },
  {
    "objectID": "lectures/lecture_7.html#mgf-1",
    "href": "lectures/lecture_7.html#mgf-1",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/lecture_7.html#ey3",
    "href": "lectures/lecture_7.html#ey3",
    "title": "Moment Generating Functions",
    "section": "\\(E(Y^3)\\)",
    "text": "\\(E(Y^3)\\)"
  },
  {
    "objectID": "lectures/lecture_7.html#mgf-2",
    "href": "lectures/lecture_7.html#mgf-2",
    "title": "Moment Generating Functions",
    "section": "MGF",
    "text": "MGF"
  },
  {
    "objectID": "lectures/lecture_7.html#ey",
    "href": "lectures/lecture_7.html#ey",
    "title": "Moment Generating Functions",
    "section": "\\(E(Y)\\)",
    "text": "\\(E(Y)\\)"
  },
  {
    "objectID": "lectures/lecture_7.html#properties",
    "href": "lectures/lecture_7.html#properties",
    "title": "Moment Generating Functions",
    "section": "Properties",
    "text": "Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(Y)\\}=cE\\{g(Y)\\}\\)\n\\(E\\{g_1(Y)+\\cdots+g_n(Y)\\}=E\\{g_1(Y)\\}+\\cdots+E\\{g_n(Y)\\}\\)"
  },
  {
    "objectID": "lectures/lecture_5.html#learning-outcomes",
    "href": "lectures/lecture_5.html#learning-outcomes",
    "title": "Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDiscrete Random Variables\n\nObtain Probabilities"
  },
  {
    "objectID": "lectures/lecture_5.html#discrete-random-variables-1",
    "href": "lectures/lecture_5.html#discrete-random-variables-1",
    "title": "Distribution Functions",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values.\n\n\n\n\n\n\n\n\n\nDistribution\nParameter(s)\nPMF \\(P(Y=y)\\)\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p^y(1-p)^{1-y}\\)\n\n\nBinomial\n\\(n\\) and \\(p\\)\n\\((^n_y)p^y(1-p)^{n-y}\\)\n\n\nGeometric\n\\(p\\)\n\\((1-p)^{y-1}p\\)\n\n\nNegative Binomial\n\\(r\\) and \\(p\\)\n\\((^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{\\lambda^y}{y!} e^{-\\lambda}\\)"
  },
  {
    "objectID": "lectures/lecture_5.html#probability-mass-function",
    "href": "lectures/lecture_5.html#probability-mass-function",
    "title": "Distribution Functions",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\)."
  },
  {
    "objectID": "lectures/lecture_5.html#cumulative-density-function",
    "href": "lectures/lecture_5.html#cumulative-density-function",
    "title": "Distribution Functions",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\)."
  },
  {
    "objectID": "lectures/lecture_5.html#bernoulli-distribution-1",
    "href": "lectures/lecture_5.html#bernoulli-distribution-1",
    "title": "Distribution Functions",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nA Bernoulli distribution is probability of having a success out of two outcomes. In essence, a coin flip with probability of success \\(p\\)."
  },
  {
    "objectID": "lectures/lecture_5.html#binomial-distribution-1",
    "href": "lectures/lecture_5.html#binomial-distribution-1",
    "title": "Distribution Functions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nFor a given sample, the number of success is represent with a binomial distribution. Binary outcomes are represented with either a Bernoulli of binomial distribution. For a binomial experiment, the following must be satisfied:\n\n\nThere is a fixed \\(n\\) trials\nThe there are two outcomes for each trial\nThe probability of success (\\(p\\)) is constant for each trial\nThe trials are independent of each other"
  },
  {
    "objectID": "lectures/lecture_5.html#pmf",
    "href": "lectures/lecture_5.html#pmf",
    "title": "Distribution Functions",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/lecture_5.html#example",
    "href": "lectures/lecture_5.html#example",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.3. An arborist decides to plant 8 seeds in the community. What is the probability that 3 to 5 seeds will germinate?"
  },
  {
    "objectID": "lectures/lecture_5.html#example-1",
    "href": "lectures/lecture_5.html#example-1",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.6. An arborist decides to plant 8 seeds in the community. What is the probability that at least 1 seed germinates?"
  },
  {
    "objectID": "lectures/lecture_5.html#example-2",
    "href": "lectures/lecture_5.html#example-2",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe probability of a planted seed to germinate is 0.8. An arborist decides to plant 6 seeds in the community. What is the probability that an even number of seed will germinate?"
  },
  {
    "objectID": "lectures/lecture_5.html#poisson-distribution-1",
    "href": "lectures/lecture_5.html#poisson-distribution-1",
    "title": "Distribution Functions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe poisson distribution describes an experiment that measures that occurrence of an event at specific point and/or time period."
  },
  {
    "objectID": "lectures/lecture_5.html#pmf-1",
    "href": "lectures/lecture_5.html#pmf-1",
    "title": "Distribution Functions",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/lecture_5.html#example-3",
    "href": "lectures/lecture_5.html#example-3",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe number of industrial accidents at a particular manufacturing plant is found to be an average of 1 accident per month. In the previous month, there were 3 accidents. What is the probability of observing 3 or more accidents in a given month?"
  },
  {
    "objectID": "lectures/lecture_5.html#example-4",
    "href": "lectures/lecture_5.html#example-4",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nThe number of industrial accidents at a particular manufacturing plant is found to be an average of 8 accidents per year. What is the probability of observing 15 or more accidents in a given year?"
  },
  {
    "objectID": "lectures/lecture_5.html#geometric-distribution-1",
    "href": "lectures/lecture_5.html#geometric-distribution-1",
    "title": "Distribution Functions",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\nA geometric distribution models the number of Bernoulli trials until the first success (or failure)."
  },
  {
    "objectID": "lectures/lecture_5.html#pmf-2",
    "href": "lectures/lecture_5.html#pmf-2",
    "title": "Distribution Functions",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/lecture_5.html#example-5",
    "href": "lectures/lecture_5.html#example-5",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nSuppose 30% of the applicants for a certain industrial job possess the necessary skills. What is the probability that the first applicant with the necessary skills is found on the 5th interview."
  },
  {
    "objectID": "lectures/lecture_5.html#negative-binomial-distribution-1",
    "href": "lectures/lecture_5.html#negative-binomial-distribution-1",
    "title": "Distribution Functions",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\nA negative binomial distribution models the number of successful Bernoulli trials until the \\(r\\)th failure."
  },
  {
    "objectID": "lectures/lecture_5.html#pmf-3",
    "href": "lectures/lecture_5.html#pmf-3",
    "title": "Distribution Functions",
    "section": "PMF",
    "text": "PMF"
  },
  {
    "objectID": "lectures/lecture_5.html#example-6",
    "href": "lectures/lecture_5.html#example-6",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nTen percent of engines manufactured on an assembly line are defective. If engines are randomly selected one at a time and tested, what is the probability that third nondefective engine is found on the 4th trial."
  },
  {
    "objectID": "lectures/lecture_3.html#learning-objectives",
    "href": "lectures/lecture_3.html#learning-objectives",
    "title": "Introduction to Probability",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine sample space and experiment\nDefine probabilities\nDefine random variable and distribution function"
  },
  {
    "objectID": "lectures/lecture_3.html#sample-space",
    "href": "lectures/lecture_3.html#sample-space",
    "title": "Introduction to Probability",
    "section": "Sample Space",
    "text": "Sample Space"
  },
  {
    "objectID": "lectures/lecture_3.html#event",
    "href": "lectures/lecture_3.html#event",
    "title": "Introduction to Probability",
    "section": "Event",
    "text": "Event\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/lecture_3.html#set-rules",
    "href": "lectures/lecture_3.html#set-rules",
    "title": "Introduction to Probability",
    "section": "Set Rules",
    "text": "Set Rules\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/lecture_3.html#set-rules-1",
    "href": "lectures/lecture_3.html#set-rules-1",
    "title": "Introduction to Probability",
    "section": "Set Rules",
    "text": "Set Rules\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/lecture_3.html#enumerating-outcomes",
    "href": "lectures/lecture_3.html#enumerating-outcomes",
    "title": "Introduction to Probability",
    "section": "Enumerating outcomes",
    "text": "Enumerating outcomes\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/lecture_3.html#probability-rules",
    "href": "lectures/lecture_3.html#probability-rules",
    "title": "Introduction to Probability",
    "section": "Probability Rules",
    "text": "Probability Rules"
  },
  {
    "objectID": "lectures/lecture_3.html#probability-rules-1",
    "href": "lectures/lecture_3.html#probability-rules-1",
    "title": "Introduction to Probability",
    "section": "Probability Rules",
    "text": "Probability Rules"
  },
  {
    "objectID": "lectures/lecture_3.html#random-variable-1",
    "href": "lectures/lecture_3.html#random-variable-1",
    "title": "Introduction to Probability",
    "section": "Random Variable",
    "text": "Random Variable"
  },
  {
    "objectID": "lectures/lecture_3.html#probability-mass-function",
    "href": "lectures/lecture_3.html#probability-mass-function",
    "title": "Introduction to Probability",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function"
  },
  {
    "objectID": "lectures/lecture_3.html#cumulative-density-function",
    "href": "lectures/lecture_3.html#cumulative-density-function",
    "title": "Introduction to Probability",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function"
  },
  {
    "objectID": "lectures/lecture_3.html#example-1",
    "href": "lectures/lecture_3.html#example-1",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nSuppose we want to understand the efficacy of a test for a certain disease. Consider the following table:\n\n\n\n\n\nDisease\nPresence\nTotal\n\n\n\n\n\n\nYes\nNo\n\n\n\nTest Result\nYes\n42\n6\n\n\n\n\nNo\n17\n35\n\n\n\n\n\n\n\n100"
  },
  {
    "objectID": "lectures/lecture_3.html#example-2",
    "href": "lectures/lecture_3.html#example-2",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\nFind the probability that an individual has a disease\nFind the probability that an individual tests negative for a disease\nFind the probability that and tests positive for a disease or they don’t have the disease\nFind the probability that the test gives an accurate result\nFind the probability that the test gives and inaccurate result"
  },
  {
    "objectID": "lectures/lecture_23.html#learning-objectives",
    "href": "lectures/lecture_23.html#learning-objectives",
    "title": "Standard Errors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nStandard Errors\n\nLinear Regression\nGLM\n\nSampling Distributions"
  },
  {
    "objectID": "lectures/lecture_23.html#standard-errors",
    "href": "lectures/lecture_23.html#standard-errors",
    "title": "Standard Errors",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nFind the variance of the estimate\nFind the information matrix\nUse for Inference"
  },
  {
    "objectID": "lectures/lecture_23.html#finding-the-variance",
    "href": "lectures/lecture_23.html#finding-the-variance",
    "title": "Standard Errors",
    "section": "Finding the Variance",
    "text": "Finding the Variance"
  },
  {
    "objectID": "lectures/lecture_23.html#estimate-for-sigma2",
    "href": "lectures/lecture_23.html#estimate-for-sigma2",
    "title": "Standard Errors",
    "section": "Estimate for \\(\\sigma^2\\)",
    "text": "Estimate for \\(\\sigma^2\\)\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum^n_{i=1} (Y_i-\\boldsymbol X_i^\\mathrm T\\hat{\\boldsymbol \\beta})^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#standard-errors-of-betas",
    "href": "lectures/lecture_23.html#standard-errors-of-betas",
    "title": "Standard Errors",
    "section": "Standard Errors of \\(\\beta\\)’s",
    "text": "Standard Errors of \\(\\beta\\)’s\n\\[\nSE(\\hat\\beta_0)=\\sqrt{\\frac{\\sum^n_{i=1}x_i^2\\hat\\sigma^2}{n\\sum^n_{i=1}(x_i-\\bar x)^2}}\n\\]\n\\[\nSE(\\hat\\beta_1)=\\sqrt\\frac{\\hat\\sigma^2}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#standard-errors-matrix-form",
    "href": "lectures/lecture_23.html#standard-errors-matrix-form",
    "title": "Standard Errors",
    "section": "Standard Errors Matrix Form",
    "text": "Standard Errors Matrix Form\n\\[\nVar(\\hat {\\boldsymbol \\beta}) = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1} \\hat \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#large-sample-theory",
    "href": "lectures/lecture_23.html#large-sample-theory",
    "title": "Standard Errors",
    "section": "Large Sample Theory",
    "text": "Large Sample Theory\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be the MLE estimator for a parameter \\(\\theta\\). As \\(n\\rightarrow\\infty\\), then \\(\\hat \\theta\\) has a normal distribution with mean \\(\\theta\\) and variance \\(1/nI(\\theta)\\), where\n\\[\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right]\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#sampling-distributions-1",
    "href": "lectures/lecture_23.html#sampling-distributions-1",
    "title": "Standard Errors",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\n\n\\(\\phi\\) known\n\\[\n\\frac{\\hat\\beta_j - \\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]\n\n\\(\\phi\\) unknown\n\\[\n\\frac{\\hat\\beta_j-\\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#learning-outcomes",
    "href": "lectures/lecture_21.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExponential Family of Distributions\nGeneralized Linear Models"
  },
  {
    "objectID": "lectures/lecture_21.html#exponential-family-of-distributions-1",
    "href": "lectures/lecture_21.html#exponential-family-of-distributions-1",
    "title": "Generalized Linear Models",
    "section": "Exponential Family of Distributions",
    "text": "Exponential Family of Distributions\nAn exponential family of distributions are random variables that allow their probability density function to have the following form:\n\\[\nf(y; \\theta,\\phi) = a(y,\\phi)\\exp\\left\\{\\frac{y\\theta-\\kappa(\\theta)}{\\phi}\\right\\}\n\\]\n\n\\(\\theta\\): is the canonical parameter (also a function of other parameters)\n\\(\\kappa(\\theta)\\): is a known cumulant function\n\\(\\phi&gt;0\\): dispersion parameter function\n\\(a(y,\\phi)\\): normalizing constant"
  },
  {
    "objectID": "lectures/lecture_21.html#canonical-parameter",
    "href": "lectures/lecture_21.html#canonical-parameter",
    "title": "Generalized Linear Models",
    "section": "Canonical Parameter",
    "text": "Canonical Parameter\nThe canonical parameter represents the relationship between the random variable and the \\(E(Y)=\\mu\\)"
  },
  {
    "objectID": "lectures/lecture_21.html#normal-distribution",
    "href": "lectures/lecture_21.html#normal-distribution",
    "title": "Generalized Linear Models",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\\[\nf(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#binomial-distribution",
    "href": "lectures/lecture_21.html#binomial-distribution",
    "title": "Generalized Linear Models",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\\[\nf(x;n,p)=\\big(^n_x\\big) p^x(1-p)^{n-x}\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#poisson-distribution",
    "href": "lectures/lecture_21.html#poisson-distribution",
    "title": "Generalized Linear Models",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\\[\nf(x;\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#common-distributions-and-canonical-parameters",
    "href": "lectures/lecture_21.html#common-distributions-and-canonical-parameters",
    "title": "Generalized Linear Models",
    "section": "Common Distributions and Canonical Parameters",
    "text": "Common Distributions and Canonical Parameters\n\n\n\nRandom Variable\nCanonical Parameter\n\n\n\n\nNormal\n\\(\\mu\\)\n\n\nBinomial\n\\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\n\nNegative Binomial\n\\(\\log\\left(\\frac{\\mu}{\\mu+k}\\right)\\)\n\n\nPoisson\n\\(\\log(\\mu)\\)\n\n\nGamma\n\\(-\\frac{1}{\\mu}\\)\n\n\nInverse Gaussian\n\\(-\\frac{1}{2\\mu^2}\\)"
  },
  {
    "objectID": "lectures/lecture_21.html#generalized-linear-models-1",
    "href": "lectures/lecture_21.html#generalized-linear-models-1",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA generalized linear model (GLM) is used to model the association between an outcome variable (of any data type) and a set of predictor values. We estimate a set of regression coefficients \\(\\boldsymbol \\beta\\) to explain how each predictor is related to the expected value of the outcome."
  },
  {
    "objectID": "lectures/lecture_21.html#generalized-linear-models-2",
    "href": "lectures/lecture_21.html#generalized-linear-models-2",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA GLM is composed of a systematic and random component."
  },
  {
    "objectID": "lectures/lecture_21.html#random-component",
    "href": "lectures/lecture_21.html#random-component",
    "title": "Generalized Linear Models",
    "section": "Random Component",
    "text": "Random Component\nThe random component is the random variable that defines the randomness and variation of the outcome variable."
  },
  {
    "objectID": "lectures/lecture_21.html#systematic-component",
    "href": "lectures/lecture_21.html#systematic-component",
    "title": "Generalized Linear Models",
    "section": "Systematic Component",
    "text": "Systematic Component\nThe systematic component is the linear model that models the association between a set of predictors and the expected value of Y:\n\\[\ng(\\mu)=\\eta=\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta\n\\]\n\n\\(\\boldsymbol\\beta\\): regression coefficients\n\\(\\boldsymbol X_i=(1, X_{i1}, \\ldots, X_{ip})^\\mathrm T\\): design vector\n\\(\\eta\\): linear model\n\\(\\mu=E(Y)\\)\n\\(g(\\cdot)\\): link function"
  },
  {
    "objectID": "lectures/lecture_2.html#r-programming",
    "href": "lectures/lecture_2.html#r-programming",
    "title": "Introduction to R/RStudio",
    "section": "R Programming",
    "text": "R Programming\nR is a statistical programming package that allows you to conduct different types of analysis.\nR"
  },
  {
    "objectID": "lectures/lecture_2.html#rstudio",
    "href": "lectures/lecture_2.html#rstudio",
    "title": "Introduction to R/RStudio",
    "section": "RStudio",
    "text": "RStudio\nA piece of software that organizes how you conduct statistical analysis in R.\nRStudio"
  },
  {
    "objectID": "lectures/lecture_2.html#posit-cloud",
    "href": "lectures/lecture_2.html#posit-cloud",
    "title": "Introduction to R/RStudio",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nA web version of RStudio.\nPosit Cloud"
  },
  {
    "objectID": "lectures/lecture_2.html#r-packages",
    "href": "lectures/lecture_2.html#r-packages",
    "title": "Introduction to R/RStudio",
    "section": "R Packages",
    "text": "R Packages\n\nTidyverse\nINQS Tools"
  },
  {
    "objectID": "lectures/lecture_2.html#r-as-a-calculator",
    "href": "lectures/lecture_2.html#r-as-a-calculator",
    "title": "Introduction to R/RStudio",
    "section": "R as a calculator",
    "text": "R as a calculator\nR can evaluate different expressions in the console tab.\nTry the following:\n\n\\(4(4+2)/34\\)\n\\(6^3\\)\n\\(3-1\\)\n\\(4+4/3+45(32*34-54)\\)"
  },
  {
    "objectID": "lectures/lecture_2.html#r-functions",
    "href": "lectures/lecture_2.html#r-functions",
    "title": "Introduction to R/RStudio",
    "section": "R Functions",
    "text": "R Functions\nR functions performs tasks to specific data values.\nEvaluate the following values in R:\n\n\\(\\sqrt{3}\\)\n\\(e^3\\)\n\\(\\ln(53)\\)\n\\(\\log(324)\\)\n\\(\\sin(3)\\)\n\\(\\sin(3\\pi)\\)"
  },
  {
    "objectID": "lectures/lecture_2.html#types-of-data",
    "href": "lectures/lecture_2.html#types-of-data",
    "title": "Introduction to R/RStudio",
    "section": "Types of Data",
    "text": "Types of Data\n\nNumeric\nCharacter\nLogical\nMissing\n\nEvaluate the following code:"
  },
  {
    "objectID": "lectures/lecture_2.html#types-of-objects",
    "href": "lectures/lecture_2.html#types-of-objects",
    "title": "Introduction to R/RStudio",
    "section": "Types of Objects",
    "text": "Types of Objects\nIn R, an object contains a set of data. The most common types are vectors and matrix.\nRun this code and print out the objects in the console:"
  },
  {
    "objectID": "lectures/lecture_2.html#data-frames",
    "href": "lectures/lecture_2.html#data-frames",
    "title": "Introduction to R/RStudio",
    "section": "Data Frames",
    "text": "Data Frames\nData frames can be thought of as R’s version of a data set.\nPlay around with mtcars:\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "lectures/lecture_2.html#lists",
    "href": "lectures/lecture_2.html#lists",
    "title": "Introduction to R/RStudio",
    "section": "Lists",
    "text": "Lists\nList can be thought as an extended vector, but each element is a different R object.\nTry playing with this R object:"
  },
  {
    "objectID": "lectures/lecture_18.html#learning-outcomes",
    "href": "lectures/lecture_18.html#learning-outcomes",
    "title": "Maximum Likelihood Estimators",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMaximum Likelihood Estimators\nProperties"
  },
  {
    "objectID": "lectures/lecture_18.html#estimators",
    "href": "lectures/lecture_18.html#estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample."
  },
  {
    "objectID": "lectures/lecture_18.html#data",
    "href": "lectures/lecture_18.html#data",
    "title": "Maximum Likelihood Estimators",
    "section": "Data",
    "text": "Data\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)\\) where \\(F(\\cdot)\\) is a known distribution function and \\(\\boldsymbol\\theta\\) is a vector of parameters. Let \\(\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}\\), be the sample collected."
  },
  {
    "objectID": "lectures/lecture_18.html#unbiased-estimators",
    "href": "lectures/lecture_18.html#unbiased-estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Unbiased Estimators",
    "text": "Unbiased Estimators\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be an estimator for a parameter \\(\\theta\\). Then \\(\\hat \\theta\\) is an unbiased estimator if \\(E(\\hat \\theta) = \\theta\\). Otherwise, \\(\\hat\\theta\\) is considered biased."
  },
  {
    "objectID": "lectures/lecture_18.html#consistent-estimators",
    "href": "lectures/lecture_18.html#consistent-estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Consistent Estimators",
    "text": "Consistent Estimators\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). The estimator \\(\\hat \\theta\\) is a consistent estimator of the \\(\\theta\\) if\n\n\\(E\\{(\\hat\\theta-\\theta)^2\\}\\rightarrow0\\) as \\(n\\rightarrow \\infty\\)\n\\(P(|\\hat\\theta-\\theta|\\ge \\epsilon)\\rightarrow0\\) as \\(n\\rightarrow \\infty\\) for every \\(\\epsilon&gt;0\\)"
  },
  {
    "objectID": "lectures/lecture_18.html#invariance-property",
    "href": "lectures/lecture_18.html#invariance-property",
    "title": "Maximum Likelihood Estimators",
    "section": "Invariance Property",
    "text": "Invariance Property\nIf \\(\\hat \\theta\\) is an ML estimator of \\(\\theta\\), then for any one-to-one function \\(g\\), the ML estimator for \\(g(\\theta)\\) is \\(g(\\hat\\theta)\\)."
  },
  {
    "objectID": "lectures/lecture_18.html#large-sample-theory",
    "href": "lectures/lecture_18.html#large-sample-theory",
    "title": "Maximum Likelihood Estimators",
    "section": "Large Sample Theory",
    "text": "Large Sample Theory\nLet \\(X_1,\\ldots,X_n\\) be a random sample from a distribution with parameter \\(\\theta\\). Let \\(\\hat \\theta\\) be the MLE estimator for a parameter \\(\\theta\\). As \\(n\\rightarrow\\infty\\), then \\(\\hat \\theta\\) has a normal distribution with mean \\(\\theta\\) and variance \\(1/nI(\\theta)\\), where\n\\[\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right]\n\\]"
  },
  {
    "objectID": "lectures/lecture_18.html#exponential-distribution",
    "href": "lectures/lecture_18.html#exponential-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}Exp(\\lambda)\\). Find the sampling distribution of the MLE of \\(\\lambda\\)"
  },
  {
    "objectID": "lectures/lecture_18.html#poisson-distribution",
    "href": "lectures/lecture_18.html#poisson-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Pois}(\\lambda)\\), Find the sampling distribution of the MLE."
  },
  {
    "objectID": "lectures/lecture_18.html#normal-distribution",
    "href": "lectures/lecture_18.html#normal-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). Are the MLE’s of \\(\\mu\\) and \\(\\sigma^2\\) unbiased?"
  },
  {
    "objectID": "lectures/lecture_16.html#learning-outcomes",
    "href": "lectures/lecture_16.html#learning-outcomes",
    "title": "Covaraince and Sampling Distributions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCovariance\nStatistics and Inference\nSampling Distributions\nCentral Limit Theorem"
  },
  {
    "objectID": "lectures/lecture_16.html#covariance-1",
    "href": "lectures/lecture_16.html#covariance-1",
    "title": "Covaraince and Sampling Distributions",
    "section": "Covariance",
    "text": "Covariance\nThe covariance measures the average dependence between multiple random variables. Let \\(W=(^X_Y)\\) be a random vector. The variance of \\(W\\) is defined as\n\\[\nVar(W) = \\left(\\begin{array}{cc}\n\\sigma^2_X & \\sigma_{XY} \\\\\n\\sigma_{XY} & \\sigma^2_{Y}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#covariance-2",
    "href": "lectures/lecture_16.html#covariance-2",
    "title": "Covaraince and Sampling Distributions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(\\mu_1\\) and \\(\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#correlation",
    "href": "lectures/lecture_16.html#correlation",
    "title": "Covaraince and Sampling Distributions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#mgf-property-independence",
    "href": "lectures/lecture_16.html#mgf-property-independence",
    "title": "Covaraince and Sampling Distributions",
    "section": "MGF Property: Independence",
    "text": "MGF Property: Independence\nLet \\(X\\) and \\(Y\\) be independent random variables. Let \\(Z = X+Y\\), the MGF of Z is\n\\[\nM_Z(t) = M_X(t)M_Y(t)\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#sample",
    "href": "lectures/lecture_16.html#sample",
    "title": "Covaraince and Sampling Distributions",
    "section": "Sample",
    "text": "Sample"
  },
  {
    "objectID": "lectures/lecture_16.html#statistics",
    "href": "lectures/lecture_16.html#statistics",
    "title": "Covaraince and Sampling Distributions",
    "section": "Statistics",
    "text": "Statistics"
  },
  {
    "objectID": "lectures/lecture_16.html#inference",
    "href": "lectures/lecture_16.html#inference",
    "title": "Covaraince and Sampling Distributions",
    "section": "Inference",
    "text": "Inference"
  },
  {
    "objectID": "lectures/lecture_16.html#iid-random-variables",
    "href": "lectures/lecture_16.html#iid-random-variables",
    "title": "Covaraince and Sampling Distributions",
    "section": "iid Random Variables",
    "text": "iid Random Variables"
  },
  {
    "objectID": "lectures/lecture_16.html#sampling-distributions-1",
    "href": "lectures/lecture_16.html#sampling-distributions-1",
    "title": "Covaraince and Sampling Distributions",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\nA sampling distribution is the distribution of a statistic. Many known statistics have a known distribution."
  },
  {
    "objectID": "lectures/lecture_16.html#bar-x",
    "href": "lectures/lecture_16.html#bar-x",
    "title": "Covaraince and Sampling Distributions",
    "section": "\\(\\bar X\\)",
    "text": "\\(\\bar X\\)"
  },
  {
    "objectID": "lectures/lecture_16.html#s2",
    "href": "lectures/lecture_16.html#s2",
    "title": "Covaraince and Sampling Distributions",
    "section": "\\(s^2\\)",
    "text": "\\(s^2\\)"
  },
  {
    "objectID": "lectures/lecture_16.html#t-distribution",
    "href": "lectures/lecture_16.html#t-distribution",
    "title": "Covaraince and Sampling Distributions",
    "section": "t-distribution",
    "text": "t-distribution\nLet \\(Z\\sim N(0,1)\\), \\(W\\sim \\chi^2_\\nu\\), \\(Z\\perp W\\); therefore:\n\\[\nT=\\frac{Z}{\\sqrt{W/\\nu}} \\sim t_\\nu\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#f-distribution",
    "href": "lectures/lecture_16.html#f-distribution",
    "title": "Covaraince and Sampling Distributions",
    "section": "F-distribution",
    "text": "F-distribution\nLet \\(W_1\\sim\\chi^2_{\\nu_1}\\) \\(W_2\\sim\\chi^2_{\\nu_2}\\), and \\(W_1\\perp W_2\\); therefore:\n\\[\nF = \\frac{W_1/\\nu_1}{W_2/\\nu_2}\\sim F_{\\nu_1,\\nu_2}\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#example",
    "href": "lectures/lecture_16.html#example",
    "title": "Covaraince and Sampling Distributions",
    "section": "Example",
    "text": "Example\nLet \\(X_1, X_2, \\ldots, X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) , show that \\(\\bar X \\sim N(\\mu,\\sigma^2/n)\\). Note: the MGF of \\(X_i\\) is \\(e^{\\mu t + \\frac{t^2\\sigma^2}{2}}\\)."
  },
  {
    "objectID": "lectures/lecture_16.html#central-limit-theorem-1",
    "href": "lectures/lecture_16.html#central-limit-theorem-1",
    "title": "Covaraince and Sampling Distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nLet \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define\n\\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\)."
  },
  {
    "objectID": "lectures/lecture_16.html#central-limit-theorem-2",
    "href": "lectures/lecture_16.html#central-limit-theorem-2",
    "title": "Covaraince and Sampling Distributions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\\[\n\\bar X \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#example-1",
    "href": "lectures/lecture_16.html#example-1",
    "title": "Covaraince and Sampling Distributions",
    "section": "Example",
    "text": "Example\nLet \\(X_1, \\ldots, X_n \\overset{iid}{\\sim} \\chi^2_p\\), the MGF is \\(M(t)=(1-2t)^{-p/2}\\). Find the distribution of \\(\\bar X\\)."
  },
  {
    "objectID": "lectures/lecture_14.html#learning-outcomes",
    "href": "lectures/lecture_14.html#learning-outcomes",
    "title": "Joint Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConditional Distributions\nIndependence\nExpectations\nCovariance"
  },
  {
    "objectID": "lectures/lecture_14.html#conditional-distributions-1",
    "href": "lectures/lecture_14.html#conditional-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable."
  },
  {
    "objectID": "lectures/lecture_14.html#discrete-conditional-distributions",
    "href": "lectures/lecture_14.html#discrete-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Discrete Conditional Distributions",
    "text": "Discrete Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#continuous-conditional-distributions",
    "href": "lectures/lecture_14.html#continuous-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Continuous Conditional Distributions",
    "text": "Continuous Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#example",
    "href": "lectures/lecture_14.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\nLet the joint density function of \\(X_1\\) and \\(X_2\\) be defined as\n\\[\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\nFind the conditional density function of \\(X_2|X_1=x_1\\)."
  },
  {
    "objectID": "lectures/lecture_14.html#independent-random-variables",
    "href": "lectures/lecture_14.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/lecture_14.html#discrete-independent-random-variables",
    "href": "lectures/lecture_14.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#continuous-independent-random-variables",
    "href": "lectures/lecture_14.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#matrix-algebra",
    "href": "lectures/lecture_14.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#bivariate-normal-distribution",
    "href": "lectures/lecture_14.html#bivariate-normal-distribution",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Normal Distribution",
    "text": "Bivariate Normal Distribution\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/lecture_14.html#expectations-1",
    "href": "lectures/lecture_14.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/lecture_14.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/lecture_14.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/lecture_14.html#conditional-expectations",
    "href": "lectures/lecture_14.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#conditional-expectations-1",
    "href": "lectures/lecture_14.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#covariance-1",
    "href": "lectures/lecture_14.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(E(X_1)=\\mu_1\\) and \\(E(X_2)=\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/lecture_14.html#correlation",
    "href": "lectures/lecture_14.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#learning-outcomes",
    "href": "lectures/lecture_12.html#learning-outcomes",
    "title": "Joint Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMarginal Distributions\nConditional Distributions\nIndependence\nExpectations\nCovariance"
  },
  {
    "objectID": "lectures/lecture_12.html#marginal-density-functions",
    "href": "lectures/lecture_12.html#marginal-density-functions",
    "title": "Joint Distribution Functions",
    "section": "Marginal Density Functions",
    "text": "Marginal Density Functions\nA Marginal Density Function is density function of one random variable from a random vector."
  },
  {
    "objectID": "lectures/lecture_12.html#marginal-discrete-probability-mass-function",
    "href": "lectures/lecture_12.html#marginal-discrete-probability-mass-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Discrete Probability Mass Function",
    "text": "Marginal Discrete Probability Mass Function\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe marginal distribution of \\(X_1\\) is defined as\n\\[\np_{X_1}(x_1) = \\sum_{x_2}p_{X_1,X_2}(x_1,x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#marginal-continuous-density-function",
    "href": "lectures/lecture_12.html#marginal-continuous-density-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Continuous Density Function",
    "text": "Marginal Continuous Density Function\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The marginal distribution of \\(X_1\\) is defined as\n\\[\nf_{X_1}(x_1) = \\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#example",
    "href": "lectures/lecture_12.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf_{X,Y}(x,y) \\left\\{\\begin{array}{cc}\n2x & 0\\le y \\le 1;\\ 0 \\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(f_X(x)\\)"
  },
  {
    "objectID": "lectures/lecture_12.html#conditional-distributions-1",
    "href": "lectures/lecture_12.html#conditional-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable."
  },
  {
    "objectID": "lectures/lecture_12.html#discrete-conditional-distributions",
    "href": "lectures/lecture_12.html#discrete-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Discrete Conditional Distributions",
    "text": "Discrete Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#continuous-conditional-distributions",
    "href": "lectures/lecture_12.html#continuous-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Continuous Conditional Distributions",
    "text": "Continuous Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2=_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#example-1",
    "href": "lectures/lecture_12.html#example-1",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\nLet the joint density function of \\(X_1\\) and \\(X_2\\) be defined as\n\\[\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\nFind the conditional density function of \\(X_2|X_1=x_1\\)."
  },
  {
    "objectID": "lectures/lecture_12.html#independent-random-variables",
    "href": "lectures/lecture_12.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/lecture_12.html#discrete-independent-random-variables",
    "href": "lectures/lecture_12.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#continuous-independent-random-variables",
    "href": "lectures/lecture_12.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#matrix-algebra",
    "href": "lectures/lecture_12.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#example-2",
    "href": "lectures/lecture_12.html#example-2",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\nShow that \\(X\\perp Y\\).\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/lecture_12.html#expectations-1",
    "href": "lectures/lecture_12.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/lecture_12.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/lecture_12.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/lecture_12.html#conditional-expectations",
    "href": "lectures/lecture_12.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#conditional-expectations-1",
    "href": "lectures/lecture_12.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#covariance-1",
    "href": "lectures/lecture_12.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(\\mu_1\\) and \\(\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#correlation",
    "href": "lectures/lecture_12.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_10.html#learning-outcomes",
    "href": "lectures/lecture_10.html#learning-outcomes",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExpectations\nVariance\nProperties"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value",
    "href": "lectures/lecture_10.html#expected-value",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value for a continuous distribution is defined as\n\\[\nE(X)=\\int x f(x)dx\n\\]\nThe expectation of a function \\(g(X)\\) is defined as\n\\[\nE\\{g(X)\\}=\\int g(x)f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-properties",
    "href": "lectures/lecture_10.html#expected-value-properties",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value Properties",
    "text": "Expected Value Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(X)\\}=cE\\{g(X)\\}\\)\n\\(E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance",
    "href": "lectures/lecture_10.html#variance",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\nThe variance of continuous variable is defined as\n\\[\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-properties",
    "href": "lectures/lecture_10.html#variance-properties",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance Properties",
    "text": "Variance Properties\n\\(Y=aX+b\\)\n\n\\(Var(Y) = Var(aX+b) = Var(aX) + Var(b) = a^2Var(X)\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-1",
    "href": "lectures/lecture_10.html#expected-value-1",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-1",
    "href": "lectures/lecture_10.html#variance-1",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-2",
    "href": "lectures/lecture_10.html#expected-value-2",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-2",
    "href": "lectures/lecture_10.html#variance-2",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-3",
    "href": "lectures/lecture_10.html#expected-value-3",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\mathrm{Beta}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-3",
    "href": "lectures/lecture_10.html#variance-3",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\mathrm{Beta}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#expected-value-4",
    "href": "lectures/lecture_10.html#expected-value-4",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Expected Value",
    "text": "Expected Value\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/lecture_10.html#variance-4",
    "href": "lectures/lecture_10.html#variance-4",
    "title": "Expected Value of Continuous Random Variables",
    "section": "Variance",
    "text": "Variance\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/2b.html#learning-outcomes",
    "href": "lectures/2b.html#learning-outcomes",
    "title": "Introduction to Probability",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDescribe disjoint events\nDescribe a conditional probability\nDefine independent events\nLaw of Total Probability\nBaye’s Theorem"
  },
  {
    "objectID": "lectures/2b.html#disjoint-events-1",
    "href": "lectures/2b.html#disjoint-events-1",
    "title": "Introduction to Probability",
    "section": "Disjoint Events",
    "text": "Disjoint Events\nTwo events A and B are considered disjoint if \\(P(A\\cap B)=0\\). In general terms, only one event can occur, not both."
  },
  {
    "objectID": "lectures/2b.html#diagram",
    "href": "lectures/2b.html#diagram",
    "title": "Introduction to Probability",
    "section": "Diagram",
    "text": "Diagram"
  },
  {
    "objectID": "lectures/2b.html#conditional-probability-1",
    "href": "lectures/2b.html#conditional-probability-1",
    "title": "Introduction to Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nLet there be 2 events A and B. Given that B has occurred, what is the probability that A occurs? The conditional probability requires there to be at least 2 events and one event must have occurred. Additionally, the events cannot be disjoint. Conditional probabilities are denoted as \\(P(A|B)\\), the probability of A given B has occurred.\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\]"
  },
  {
    "objectID": "lectures/2b.html#diagram-1",
    "href": "lectures/2b.html#diagram-1",
    "title": "Introduction to Probability",
    "section": "Diagram",
    "text": "Diagram"
  },
  {
    "objectID": "lectures/2b.html#example",
    "href": "lectures/2b.html#example",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\n\n\n\nUses Eye Glasses\n\n\n\nNeeds Glasses\nYes\nNo\n\n\nYes\n44\n14\n\n\nNo\n2\n40\n\n\n\n\nFind the probability of needing glasses\nFind the probability of not using glasses\nFind the probability of not using glasses and needing glasses\nFind the probability of not using glasses, given they need glasses"
  },
  {
    "objectID": "lectures/2b.html#work",
    "href": "lectures/2b.html#work",
    "title": "Introduction to Probability",
    "section": "Work",
    "text": "Work"
  },
  {
    "objectID": "lectures/2b.html#independent-events-1",
    "href": "lectures/2b.html#independent-events-1",
    "title": "Introduction to Probability",
    "section": "Independent Events",
    "text": "Independent Events\nEvents A and B are considered independent if \\(P(A\\cap B)=P(A)P(B)\\). In other words, The occurrence of one event will not have an effect on the occurrence of the other event."
  },
  {
    "objectID": "lectures/2b.html#example-1",
    "href": "lectures/2b.html#example-1",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\n\n\n\nUses Eye Glasses\n\n\n\n\n\nNeeds Glasses\nYes\nNo\n\n\nYes\n44\n14\n\n\nNo\n2\n40"
  },
  {
    "objectID": "lectures/2b.html#law-of-total-probability-1",
    "href": "lectures/2b.html#law-of-total-probability-1",
    "title": "Introduction to Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\nThe law of total probability allows you to compute the probability of an event A given that the sets {\\(B_1\\), … , \\(B_n\\)} partitions event A. The law of total probability is given as\n\\[\nP(A)= \\sum^n_{i=1}P(A\\cap B_i)\n\\]\n\n\\[\nP(A)=\\sum^n_{i=1}P(A|B_i)P(B_i)\n\\]"
  },
  {
    "objectID": "lectures/2b.html#diagrams",
    "href": "lectures/2b.html#diagrams",
    "title": "Introduction to Probability",
    "section": "Diagrams",
    "text": "Diagrams"
  },
  {
    "objectID": "lectures/2b.html#example-2",
    "href": "lectures/2b.html#example-2",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nThe probability of an individual having a disease, given that they test positive for the disease, is 0.82. The probability of an individual having a disease, given they tested negative, is 0.14. The probability of testing positive is 0.6. What is the prevalence of a disease (probability of having a disease)?"
  },
  {
    "objectID": "lectures/2b.html#bayes-theorem-1",
    "href": "lectures/2b.html#bayes-theorem-1",
    "title": "Introduction to Probability",
    "section": "Baye’s Theorem",
    "text": "Baye’s Theorem\nBaye’s theorem computes the probability of an event \\(B_i\\) given event A\n\\[\nP(B_i|A) = \\frac{P(A\\cap B_i)}{P(A)}=\\frac{P(A|B_i)P(B_i)}{\\sum^n_{i=1}P(A|B_i)P(B_i)}\n\\]"
  },
  {
    "objectID": "lectures/2b.html#example-3",
    "href": "lectures/2b.html#example-3",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nThe probability of an having a disease, given that they test positive for the disease, is 0.82. The probability of and individual having a disease, given they tested negative, is 0.14. The probability of testing positive is 0.6. What is the probability of resulting in a false negative?"
  },
  {
    "objectID": "lectures/1b.html#r-programming",
    "href": "lectures/1b.html#r-programming",
    "title": "Introduction to R/RStudio",
    "section": "R Programming",
    "text": "R Programming\nR is a statistical programming package that allows you to conduct different types of analysis.\nR"
  },
  {
    "objectID": "lectures/1b.html#rstudio",
    "href": "lectures/1b.html#rstudio",
    "title": "Introduction to R/RStudio",
    "section": "RStudio",
    "text": "RStudio\nA piece of software that organizes how you conduct statistical analysis in R.\nRStudio"
  },
  {
    "objectID": "lectures/1b.html#posit-cloud",
    "href": "lectures/1b.html#posit-cloud",
    "title": "Introduction to R/RStudio",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nA web version of RStudio.\nPosit Cloud"
  },
  {
    "objectID": "lectures/1b.html#r-packages",
    "href": "lectures/1b.html#r-packages",
    "title": "Introduction to R/RStudio",
    "section": "R Packages",
    "text": "R Packages\n\nTidyverse"
  },
  {
    "objectID": "lectures/1b.html#r-as-a-calculator",
    "href": "lectures/1b.html#r-as-a-calculator",
    "title": "Introduction to R/RStudio",
    "section": "R as a calculator",
    "text": "R as a calculator\nR can evaluate different expressions in the console tab.\nTry the following:\n\n\\(4(4+2)/34\\)\n\\(6^3\\)\n\\(3-1\\)\n\\(4+4/3+45(32*34-54)\\)"
  },
  {
    "objectID": "lectures/1b.html#r-functions",
    "href": "lectures/1b.html#r-functions",
    "title": "Introduction to R/RStudio",
    "section": "R Functions",
    "text": "R Functions\nR functions performs tasks to specific data values.\nEvaluate the following values in R:\n\n\\(\\sqrt{3}\\)\n\\(e^3\\)\n\\(\\ln(53)\\)\n\\(\\log(324)\\)\n\\(\\sin(3)\\)\n\\(\\sin(3\\pi)\\)"
  },
  {
    "objectID": "lectures/1b.html#types-of-data",
    "href": "lectures/1b.html#types-of-data",
    "title": "Introduction to R/RStudio",
    "section": "Types of Data",
    "text": "Types of Data\n\nNumeric\nCharacter\nLogical\nMissing\n\nEvaluate the following code:"
  },
  {
    "objectID": "lectures/1b.html#types-of-objects",
    "href": "lectures/1b.html#types-of-objects",
    "title": "Introduction to R/RStudio",
    "section": "Types of Objects",
    "text": "Types of Objects\nIn R, an object contains a set of data. The most common types are vectors and matrix.\nRun this code and print out the objects in the console:"
  },
  {
    "objectID": "lectures/1b.html#data-frames",
    "href": "lectures/1b.html#data-frames",
    "title": "Introduction to R/RStudio",
    "section": "Data Frames",
    "text": "Data Frames\nData frames can be thought of as R’s version of a data set.\nPlay around with mtcars:\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "lectures/1b.html#lists",
    "href": "lectures/1b.html#lists",
    "title": "Introduction to R/RStudio",
    "section": "Lists",
    "text": "Lists\nList can be thought as an extended vector, but each element is a different R object.\nTry playing with this R object:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Probability and Statistics!",
    "section": "",
    "text": "Brief Introduction\n\n\n\n\n\nWELCOME TO THE COURSE! This is the home page of the course where I will provide a recap on what was covered in the week. Here I will post any documents or videos for your reference. If you have any questions, please email me at isaac.qs@csuci.edu.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\nThis week is designed to be an introduction week. We will briefly discuss topics related to statistics and inference. Then we will look at installing R and RStudio as well as the basics of using R.\n\n\n\n\n\nAug 24, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "Homework",
    "section": "",
    "text": "Below are the different homework assignments for the course. Make sure to upload your assignment as as single PDF on Canvas.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ec.html",
    "href": "ec.html",
    "title": "Extra Credit",
    "section": "",
    "text": "Extra Credit is designed to expand on different topics that related to Probability and Statistics, but are not necessarily required for the course. Additionally, these opportunities provide students relief when unexpected situations occur during the semester. While it is not required, I encourage everyone to attempt each opportunity.\nBelow is more information on each assignment.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/1a.html#introductions",
    "href": "lectures/1a.html#introductions",
    "title": "Weclome to Math 352",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "lectures/1a.html#introductions-1",
    "href": "lectures/1a.html#introductions-1",
    "title": "Weclome to Math 352",
    "section": "Introductions",
    "text": "Introductions\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "lectures/1a.html#goals-for-the-course",
    "href": "lectures/1a.html#goals-for-the-course",
    "title": "Weclome to Math 352",
    "section": "Goals for the Course",
    "text": "Goals for the Course\n\nIntroduction to Probability\nIntroduction to Statistics\nIntroduction to Regression"
  },
  {
    "objectID": "lectures/1a.html#oh-traditional",
    "href": "lectures/1a.html#oh-traditional",
    "title": "Weclome to Math 352",
    "section": "OH: Traditional",
    "text": "OH: Traditional\nBTE 2840 MW 5-6 PM"
  },
  {
    "objectID": "lectures/1a.html#oh-casual",
    "href": "lectures/1a.html#oh-casual",
    "title": "Weclome to Math 352",
    "section": "OH: Casual",
    "text": "OH: Casual\nBell Tower Rooftop Terrace Th 1:30 - 3 PM"
  },
  {
    "objectID": "lectures/1a.html#oh-r-programming",
    "href": "lectures/1a.html#oh-r-programming",
    "title": "Weclome to Math 352",
    "section": "OH: R Programming",
    "text": "OH: R Programming\nBTE 2810 F 10AM - 12PM"
  },
  {
    "objectID": "lectures/1a.html#syllabus-1",
    "href": "lectures/1a.html#syllabus-1",
    "title": "Weclome to Math 352",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus"
  },
  {
    "objectID": "lectures/1a.html#homework-0",
    "href": "lectures/1a.html#homework-0",
    "title": "Weclome to Math 352",
    "section": "Homework 0",
    "text": "Homework 0\nHomework 0 is posted!\nIt is due on February 3, 2023!\nSubmit your assignment on Canvas!"
  },
  {
    "objectID": "lectures/1a.html#plot-a-thon",
    "href": "lectures/1a.html#plot-a-thon",
    "title": "Weclome to Math 352",
    "section": "Plot-a-thon",
    "text": "Plot-a-thon"
  },
  {
    "objectID": "lectures/1a.html#learning-objectives",
    "href": "lectures/1a.html#learning-objectives",
    "title": "Weclome to Math 352",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPopulation\nSample\nInference\nAssociations"
  },
  {
    "objectID": "lectures/1a.html#population",
    "href": "lectures/1a.html#population",
    "title": "Weclome to Math 352",
    "section": "Population",
    "text": "Population\n\nA set of all measurements of interest to the sample collector."
  },
  {
    "objectID": "lectures/1a.html#sample",
    "href": "lectures/1a.html#sample",
    "title": "Weclome to Math 352",
    "section": "Sample",
    "text": "Sample\n\nA sample is any subset of measurements selected from the population."
  },
  {
    "objectID": "lectures/1a.html#inference",
    "href": "lectures/1a.html#inference",
    "title": "Weclome to Math 352",
    "section": "Inference",
    "text": "Inference\n\nParameter: a measurement describing the population\nStatistic: a measurement describing the sample"
  },
  {
    "objectID": "lectures/1a.html#associations",
    "href": "lectures/1a.html#associations",
    "title": "Weclome to Math 352",
    "section": "Associations",
    "text": "Associations\nAn association describes the relationship between two characteristics of a population."
  },
  {
    "objectID": "lectures/2a.html#learning-objectives",
    "href": "lectures/2a.html#learning-objectives",
    "title": "Introduction to Probability",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine sample space and experiment\nDefine probabilities\nDefine random variable and distribution function"
  },
  {
    "objectID": "lectures/2a.html#sample-space",
    "href": "lectures/2a.html#sample-space",
    "title": "Introduction to Probability",
    "section": "Sample Space",
    "text": "Sample Space"
  },
  {
    "objectID": "lectures/2a.html#event",
    "href": "lectures/2a.html#event",
    "title": "Introduction to Probability",
    "section": "Event",
    "text": "Event\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/2a.html#set-rules",
    "href": "lectures/2a.html#set-rules",
    "title": "Introduction to Probability",
    "section": "Set Rules",
    "text": "Set Rules\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/2a.html#set-rules-1",
    "href": "lectures/2a.html#set-rules-1",
    "title": "Introduction to Probability",
    "section": "Set Rules",
    "text": "Set Rules\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/2a.html#enumerating-outcomes",
    "href": "lectures/2a.html#enumerating-outcomes",
    "title": "Introduction to Probability",
    "section": "Enumerating outcomes",
    "text": "Enumerating outcomes\n\\(S=\\{a_1,a_2,a_3,a_4, a_5\\}\\)"
  },
  {
    "objectID": "lectures/2a.html#probability-rules",
    "href": "lectures/2a.html#probability-rules",
    "title": "Introduction to Probability",
    "section": "Probability Rules",
    "text": "Probability Rules"
  },
  {
    "objectID": "lectures/2a.html#probability-rules-1",
    "href": "lectures/2a.html#probability-rules-1",
    "title": "Introduction to Probability",
    "section": "Probability Rules",
    "text": "Probability Rules"
  },
  {
    "objectID": "lectures/2a.html#random-variable-1",
    "href": "lectures/2a.html#random-variable-1",
    "title": "Introduction to Probability",
    "section": "Random Variable",
    "text": "Random Variable"
  },
  {
    "objectID": "lectures/2a.html#probability-mass-function",
    "href": "lectures/2a.html#probability-mass-function",
    "title": "Introduction to Probability",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function"
  },
  {
    "objectID": "lectures/2a.html#cumulative-density-function",
    "href": "lectures/2a.html#cumulative-density-function",
    "title": "Introduction to Probability",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function"
  },
  {
    "objectID": "lectures/2a.html#example-1",
    "href": "lectures/2a.html#example-1",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nSuppose we want to understand the efficacy of a test for a certain disease. Consider the following table:\n\n\n\n\n\nDisease\nPresence\nTotal\n\n\n\n\n\n\nYes\nNo\n\n\n\nTest Result\nYes\n42\n6\n\n\n\n\nNo\n17\n35\n\n\n\n\n\n\n\n100"
  },
  {
    "objectID": "lectures/2a.html#example-2",
    "href": "lectures/2a.html#example-2",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\nFind the probability that an individual has a disease\nFind the probability that an individual tests negative for a disease\nFind the probability that and tests positive for a disease or they don’t have the disease\nFind the probability that the test gives an accurate result\nFind the probability that the test gives and inaccurate result"
  },
  {
    "objectID": "lectures/lecture_1.html#introductions",
    "href": "lectures/lecture_1.html#introductions",
    "title": "Weclome to Math 352",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "lectures/lecture_1.html#introductions-1",
    "href": "lectures/lecture_1.html#introductions-1",
    "title": "Weclome to Math 352",
    "section": "Introductions",
    "text": "Introductions\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "lectures/lecture_1.html#goals-for-the-course",
    "href": "lectures/lecture_1.html#goals-for-the-course",
    "title": "Weclome to Math 352",
    "section": "Goals for the Course",
    "text": "Goals for the Course\n\nIntroduction to Probability\nIntroduction to Statistics\nIntroduction to Regression"
  },
  {
    "objectID": "lectures/lecture_1.html#oh-traditional",
    "href": "lectures/lecture_1.html#oh-traditional",
    "title": "Weclome to Math 352",
    "section": "OH: Traditional",
    "text": "OH: Traditional\nBTE 2840 MW 5-6 PM"
  },
  {
    "objectID": "lectures/lecture_1.html#oh-casual",
    "href": "lectures/lecture_1.html#oh-casual",
    "title": "Weclome to Math 352",
    "section": "OH: Casual",
    "text": "OH: Casual\nBell Tower Rooftop Terrace Th 1:30 - 3 PM"
  },
  {
    "objectID": "lectures/lecture_1.html#oh-r-programming",
    "href": "lectures/lecture_1.html#oh-r-programming",
    "title": "Weclome to Math 352",
    "section": "OH: R Programming",
    "text": "OH: R Programming\nBTE 2810 F 10AM - 12PM"
  },
  {
    "objectID": "lectures/lecture_1.html#syllabus-1",
    "href": "lectures/lecture_1.html#syllabus-1",
    "title": "Weclome to Math 352",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus"
  },
  {
    "objectID": "lectures/lecture_1.html#homework-0",
    "href": "lectures/lecture_1.html#homework-0",
    "title": "Weclome to Math 352",
    "section": "Homework 0",
    "text": "Homework 0\nHomework 0 is posted!\nIt is due on February 3, 2023!\nSubmit your assignment on Canvas!"
  },
  {
    "objectID": "lectures/lecture_1.html#plot-a-thon",
    "href": "lectures/lecture_1.html#plot-a-thon",
    "title": "Weclome to Math 352",
    "section": "Plot-a-thon",
    "text": "Plot-a-thon"
  },
  {
    "objectID": "lectures/lecture_1.html#learning-objectives",
    "href": "lectures/lecture_1.html#learning-objectives",
    "title": "Weclome to Math 352",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nPopulation\nSample\nInference\nAssociations"
  },
  {
    "objectID": "lectures/lecture_1.html#population",
    "href": "lectures/lecture_1.html#population",
    "title": "Weclome to Math 352",
    "section": "Population",
    "text": "Population\n\nA set of all measurements of interest to the sample collector."
  },
  {
    "objectID": "lectures/lecture_1.html#sample",
    "href": "lectures/lecture_1.html#sample",
    "title": "Weclome to Math 352",
    "section": "Sample",
    "text": "Sample\n\nA sample is any subset of measurements selected from the population."
  },
  {
    "objectID": "lectures/lecture_1.html#inference",
    "href": "lectures/lecture_1.html#inference",
    "title": "Weclome to Math 352",
    "section": "Inference",
    "text": "Inference\n\nParameter: a measurement describing the population\nStatistic: a measurement describing the sample"
  },
  {
    "objectID": "lectures/lecture_1.html#associations",
    "href": "lectures/lecture_1.html#associations",
    "title": "Weclome to Math 352",
    "section": "Associations",
    "text": "Associations\nAn association describes the relationship between two characteristics of a population."
  },
  {
    "objectID": "lectures/lecture_11.html#learning-outcomes",
    "href": "lectures/lecture_11.html#learning-outcomes",
    "title": "MGF of Continuous Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMoment Generating Functions\nMGF Properties"
  },
  {
    "objectID": "lectures/lecture_11.html#moments",
    "href": "lectures/lecture_11.html#moments",
    "title": "MGF of Continuous Random Variables",
    "section": "Moments",
    "text": "Moments\nThe \\(k\\)th moment is defined as the expectation of the random variable, raised to the \\(k\\)th power, defined as \\(E(X^k)\\)."
  },
  {
    "objectID": "lectures/lecture_11.html#moment-generating-functions",
    "href": "lectures/lecture_11.html#moment-generating-functions",
    "title": "MGF of Continuous Random Variables",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating functions is used to obtain the \\(k\\)th moment. The mgf is defined as\n\\[\nm(t) = E(e^{tX})\n\\]\nThe \\(k\\)th moment can be obtained by taking the \\(k\\)th derivative of the mgf, with respect to \\(t\\), and setting \\(t\\) equal to 0:\n\\[\nE(X^k)=\\frac{d^km(t)}{dt}\\Bigg|_{t=0}\n\\]"
  },
  {
    "objectID": "lectures/lecture_11.html#mgf-properties-linearity",
    "href": "lectures/lecture_11.html#mgf-properties-linearity",
    "title": "MGF of Continuous Random Variables",
    "section": "MGF Properties: Linearity",
    "text": "MGF Properties: Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "lectures/lecture_11.html#mgf-properties-uniqueness",
    "href": "lectures/lecture_11.html#mgf-properties-uniqueness",
    "title": "MGF of Continuous Random Variables",
    "section": "MGF Properties: Uniqueness",
    "text": "MGF Properties: Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "lectures/lecture_11.html#uniform-distribution-mgf",
    "href": "lectures/lecture_11.html#uniform-distribution-mgf",
    "title": "MGF of Continuous Random Variables",
    "section": "Uniform Distribution MGF",
    "text": "Uniform Distribution MGF\n\\(X\\sim\\mathrm{U(a,b)}\\)\n\\(a&lt;x&lt;b\\)\n\\(f_X(x) = \\frac{1}{b-a}\\)"
  },
  {
    "objectID": "lectures/lecture_11.html#normal-distribution-mgf",
    "href": "lectures/lecture_11.html#normal-distribution-mgf",
    "title": "MGF of Continuous Random Variables",
    "section": "Normal Distribution MGF",
    "text": "Normal Distribution MGF\n\\(X\\sim\\mathrm{N}(\\mu, \\sigma^2)\\)\n\\(-\\infty &lt; x &lt; \\infty\\)\n\\(f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)"
  },
  {
    "objectID": "lectures/lecture_11.html#gamma-distribution-mgf",
    "href": "lectures/lecture_11.html#gamma-distribution-mgf",
    "title": "MGF of Continuous Random Variables",
    "section": "Gamma Distribution MGF",
    "text": "Gamma Distribution MGF\n\\(X\\sim\\mathrm{Gamma}(\\alpha, \\beta)\\)\n\\(0&lt;x&lt;1\\)\n\\(f_X(x)=\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}x^{\\alpha-1}e^{-x/\\beta}\\)"
  },
  {
    "objectID": "lectures/lecture_11.html#chi2-distribution-mgf",
    "href": "lectures/lecture_11.html#chi2-distribution-mgf",
    "title": "MGF of Continuous Random Variables",
    "section": "\\(\\chi^2\\)-Distribution MGF",
    "text": "\\(\\chi^2\\)-Distribution MGF\n\\(X\\sim\\chi^2_k\\)\n\\(x&gt;0\\)\n\\(f_X(x)=\\frac{x^{k/2-1}\\exp\\{-x/2\\}}{2^{k/2}\\Gamma(k/2)}\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#learning-outcomes",
    "href": "lectures/lecture_13.html#learning-outcomes",
    "title": "Joint Distribution Functions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nJoint Distributions\nMarginal Distributions\nConditional Distributions\nIndependence\nExpectations\nCovariance"
  },
  {
    "objectID": "lectures/lecture_13.html#partial-derivatives",
    "href": "lectures/lecture_13.html#partial-derivatives",
    "title": "Joint Distribution Functions",
    "section": "Partial Derivatives",
    "text": "Partial Derivatives\nFor a function \\(f(x,y)\\), the partial derivative with respect to \\(x\\) is taken by differentiating \\(f(x,y)\\) with respect to \\(x\\) while treating \\(y\\) as a constant. For example:\n\\(f(x,y) = x^2 + \\ln(y)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#multiple-integration",
    "href": "lectures/lecture_13.html#multiple-integration",
    "title": "Joint Distribution Functions",
    "section": "Multiple Integration",
    "text": "Multiple Integration\nMultiple integration is when you integrate a multivariate function by multiple variables. This is done by integrating the function by an individual variable at a time. For example:\n\\(f(x,y)=x^2 + y^2\\) which can be integrated as:"
  },
  {
    "objectID": "lectures/lecture_13.html#joint-distributions-1",
    "href": "lectures/lecture_13.html#joint-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Joint Distributions",
    "text": "Joint Distributions\nA joint distribution is a process where more than one random variable is generated; for example, collecting biomedical data, such as multiple biomarkers, are considered to follow a joint distribution. In mathematical terms, instead of dealing with a random variable, we are dealing with a random vector. Observing a particular random vector will have a probability attached to it."
  },
  {
    "objectID": "lectures/lecture_13.html#bivariate-discrete-distributions",
    "href": "lectures/lecture_13.html#bivariate-discrete-distributions",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Discrete Distributions",
    "text": "Bivariate Discrete Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, the joint distribution function of \\((X_1, X_2)\\) is defined as\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe properties of a bivariate discrete distribution are\n\n\\(p_{X_1,X_2}(x_1,x_2)\\ge 0\\) for all \\(x_1,\\ x_2\\)\n\\(\\sum_{x_1}\\sum_{x2}p(x_1,x_2)=1\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#bivariate-continuous-distribution",
    "href": "lectures/lecture_13.html#bivariate-continuous-distribution",
    "title": "Joint Distribution Functions",
    "section": "Bivariate Continuous Distribution",
    "text": "Bivariate Continuous Distribution\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, the joint distribution function of \\((X_1, X_2)\\) is defined as\n\\[\nF_{X_1,X_2}(x_1, x_2) = P(X_1\\le x_1, X_2 \\le x_2).\n\\]\nThe properties of a bivariate continuous distribution are\n\n\\(f_{X_1,X_2}(x_1,x_2)=\\frac{\\partial^2F(x_1,x_2)}{\\partial x_1\\partial x_2}\\)\n\\(f_{X_1,X_2}(x_1, x_2)\\ge 0\\)\n\\(\\int_{x_1}\\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2dx_1=1\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#example",
    "href": "lectures/lecture_13.html#example",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf(x,y) \\left\\{\\begin{array}{cc}\n3x & 0\\le y\\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(P(0\\le X\\le 0.5,0.25\\le Y)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#marginal-density-functions",
    "href": "lectures/lecture_13.html#marginal-density-functions",
    "title": "Joint Distribution Functions",
    "section": "Marginal Density Functions",
    "text": "Marginal Density Functions\nA Marginal Density Function is density function of one random variable from a random vector."
  },
  {
    "objectID": "lectures/lecture_13.html#marginal-discrete-probability-mass-function",
    "href": "lectures/lecture_13.html#marginal-discrete-probability-mass-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Discrete Probability Mass Function",
    "text": "Marginal Discrete Probability Mass Function\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe marginal distribution of \\(X_1\\) is defined as\n\\[\np_{X_1}(x_1) = \\sum_{x_2}p_{X_1,X_2}(x_1,x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#marginal-continuous-density-function",
    "href": "lectures/lecture_13.html#marginal-continuous-density-function",
    "title": "Joint Distribution Functions",
    "section": "Marginal Continuous Density Function",
    "text": "Marginal Continuous Density Function\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The marginal distribution of \\(X_1\\) is defined as\n\\[\nf_{X_1}(x_1) = \\int_{x_2}f_{X_1,X_2}(x_1,x_2)dx_2\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#example-1",
    "href": "lectures/lecture_13.html#example-1",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\nf_{X,Y}(x,y) \\left\\{\\begin{array}{cc}\n2x & 0\\le y \\le 1;\\ 0 \\le x\\le 1\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\\right.\n\\]\nFind \\(f_X(x)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#conditional-distributions-1",
    "href": "lectures/lecture_13.html#conditional-distributions-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable."
  },
  {
    "objectID": "lectures/lecture_13.html#discrete-conditional-distributions",
    "href": "lectures/lecture_13.html#discrete-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Discrete Conditional Distributions",
    "text": "Discrete Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint distribution function of\n\\[\np_{X_1,X_2}(x_1, x_2) = P(X_1=x_1, X_2 = x_2).\n\\]\nThe conditional distribution of \\(X_1|X_2=x_2\\) is defined as\n\\[\np_{X_1|X_2}(x_1) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#continuous-conditional-distributions",
    "href": "lectures/lecture_13.html#continuous-conditional-distributions",
    "title": "Joint Distribution Functions",
    "section": "Continuous Conditional Distributions",
    "text": "Continuous Conditional Distributions\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). The conditional distribution of \\(X_1|X_2=_2\\) is defined as\n\\[\nf_{X_1|X_2}(x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#example-2",
    "href": "lectures/lecture_13.html#example-2",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\nLet the joint density function of \\(X_1\\) and \\(X_2\\) be defined as\n\\[\nf_{X_1,X_2}(x_1,x_2)=\\left\\{\\begin{array}{cc}\n30x_1x_2² & x_1 -1 \\le x_2 \\le 1-x_1; 0\\le x_1\\le 1\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\nFind the conditional density function of \\(X_2|X_1=x_1\\)."
  },
  {
    "objectID": "lectures/lecture_13.html#independent-random-variables",
    "href": "lectures/lecture_13.html#independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable."
  },
  {
    "objectID": "lectures/lecture_13.html#discrete-independent-random-variables",
    "href": "lectures/lecture_13.html#discrete-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Discrete Independent Random Variables",
    "text": "Discrete Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 discrete random variables, with a joint density function of \\(p_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\np_{X_1,X_2}(x_1,x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#continuous-independent-random-variables",
    "href": "lectures/lecture_13.html#continuous-independent-random-variables",
    "title": "Joint Distribution Functions",
    "section": "Continuous Independent Random Variables",
    "text": "Continuous Independent Random Variables\nLet \\(X_1\\) and \\(X_2\\) be 2 continuous random variables, with a joint density function of \\(f_{X_1,X_2}(x_1,x_2)\\). \\(X_1\\) is independent of \\(X_2\\) if and only if\n\\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#matrix-algebra",
    "href": "lectures/lecture_13.html#matrix-algebra",
    "title": "Joint Distribution Functions",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\\[\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n\\]\n\\[\n\\det(A) = a_1a_2\n\\]\n\\[\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#example-3",
    "href": "lectures/lecture_13.html#example-3",
    "title": "Joint Distribution Functions",
    "section": "Example",
    "text": "Example\n\\[\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n\\]\nShow that \\(X\\perp Y\\).\n\\[\nf_{X,Y}(x,y)=\\det(2\\pi\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{w}-\\boldsymbol\\mu)^T\\Sigma^{-1}(\\boldsymbol w-\\boldsymbol\\mu)\\right\\}\n\\]\nwhere \\(\\Sigma=\\left(\\begin{array}{cc}\\sigma_y^2 & 0\\\\0 & \\sigma_y^2\\end{array}\\right)\\), \\(\\boldsymbol \\mu = \\left(\\begin{array}{cc}\\mu_x\\\\ \\mu_y \\end{array}\\right)\\), and \\(\\boldsymbol w = \\left(\\begin{array}{cc} x\\\\ y \\end{array}\\right)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#expectations-1",
    "href": "lectures/lecture_13.html#expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Expectations",
    "text": "Expectations\nLet \\(X_1, X_2, \\ldots,X_n\\) be a set of random variables, the expectation of a function \\(g(X_1,\\ldots, X_n)\\) is defined as\n\\[\nE\\{g(X_1,\\ldots, X_n)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(X_1,\\ldots, X_n)p(x_1,\\ldots,x_n)\n\\]\nor\n\\[\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol X)dx_n \\cdots dx_1\n\\]\n\n\\(\\boldsymbol X = (X_1,\\cdots, X_n)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#expected-value-and-variance-of-linear-functions",
    "href": "lectures/lecture_13.html#expected-value-and-variance-of-linear-functions",
    "title": "Joint Distribution Functions",
    "section": "Expected Value and Variance of Linear Functions",
    "text": "Expected Value and Variance of Linear Functions\nLet \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\) be random variables with \\(E(X_i)=\\mu_i\\) and \\(E(Y_j)=\\tau_j\\). Furthermore, let \\(U = \\sum^n_{i=1}a_iX_i\\) and \\(V=\\sum^m_{j=1}b_jY_j\\) where \\(\\{a_i\\}^n_{i=1}\\) and \\(\\{b_j\\}_{j=1}^m\\) are constants. We have the following properties:\n\n\\(E(U)=\\sum_{i=1}^na_i\\mu_i\\)\n\\(Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i&lt;j}{\\sum\\sum}a_ia_jCov(X_i,X_j)\\)\n\\(Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#conditional-expectations",
    "href": "lectures/lecture_13.html#conditional-expectations",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nLet \\(X_1\\) and \\(X_2\\) be two random variables, the conditional expectation of \\(g(X_1)\\), given \\(X_2=x_2\\), is defined as\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n\\]\nor\n\\[\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#conditional-expectations-1",
    "href": "lectures/lecture_13.html#conditional-expectations-1",
    "title": "Joint Distribution Functions",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nFurthermore,\n\\[\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]\nand\n\\[\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#covariance-1",
    "href": "lectures/lecture_13.html#covariance-1",
    "title": "Joint Distribution Functions",
    "section": "Covariance",
    "text": "Covariance\nLet \\(X_1\\) and \\(X_2\\) be 2 random variables with mean \\(E(X_1)=\\mu_1\\) and \\(E(X_2)=\\mu_2\\), respectively. The covariance of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\begin{eqnarray*}\nCov(X_1,X_2) & = & E\\{(X_1-\\mu_1)(X_2-\\mu_2)\\}\\\\\n& =& E(X_1X_2)-\\mu_1\\mu_2\n\\end{eqnarray*}\n\\]\nIf \\(X_1\\) and \\(X_2\\) are independent random variables, then\n\\[\nCov(X_1,X_2)=0\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#correlation",
    "href": "lectures/lecture_13.html#correlation",
    "title": "Joint Distribution Functions",
    "section": "Correlation",
    "text": "Correlation\nThe correlation of \\(X_1\\) and \\(X_2\\) is defined as\n\\[\n\\rho = Cor(X_1,X_2) = \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1)Var(X_2)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_15.html#learning-outcomes",
    "href": "lectures/lecture_15.html#learning-outcomes",
    "title": "Functions of Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nFunctions of Random Variables\nFinding PDFs using the distribution function\nFinding the PDF of a function of random variables\nUsing Moment Generating Functions"
  },
  {
    "objectID": "lectures/lecture_15.html#function-of-random-variables-1",
    "href": "lectures/lecture_15.html#function-of-random-variables-1",
    "title": "Functions of Random Variables",
    "section": "Function of Random Variables",
    "text": "Function of Random Variables"
  },
  {
    "objectID": "lectures/lecture_15.html#using-the-distribution-function",
    "href": "lectures/lecture_15.html#using-the-distribution-function",
    "title": "Functions of Random Variables",
    "section": "Using the Distribution Function",
    "text": "Using the Distribution Function\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), the density function for the random variable \\(Y=g(X)\\) can be found with the following steps\n\n\nFind the region of \\(Y\\) in the space of \\(X\\), find \\(g^{-1}(y)\\)\nFind the region of \\(Y\\le y\\)\nFind \\(F_Y(y)=P(Y\\le y)\\) using the probability density function of \\(X\\) over region \\(Y\\le y\\)\nFind \\(f_Y(y)\\) by differentiating \\(F_Y(y)\\)"
  },
  {
    "objectID": "lectures/lecture_15.html#example-1",
    "href": "lectures/lecture_15.html#example-1",
    "title": "Functions of Random Variables",
    "section": "Example 1",
    "text": "Example 1\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n2x & 0\\le x \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=3X-1\\)?"
  },
  {
    "objectID": "lectures/lecture_15.html#using-the-pdf",
    "href": "lectures/lecture_15.html#using-the-pdf",
    "title": "Functions of Random Variables",
    "section": "Using the PDF",
    "text": "Using the PDF\nLet there be a random variable \\(X\\) with a known distribution function \\(F_X(x)\\), if the random variable \\(Y=g(X)\\) is either increasing or decreasing, than the probability density function can be found as\n\\[\nf_Y(y) = f_X\\{g^{-1}(y)\\}\\left|\\frac{dg^{-1}(y)}{dy}\\right|\n\\]"
  },
  {
    "objectID": "lectures/lecture_15.html#example-2",
    "href": "lectures/lecture_15.html#example-2",
    "title": "Functions of Random Variables",
    "section": "Example 2",
    "text": "Example 2\nLet \\(X\\) have the following probability density function:\n\\[\nf_X(x)=\\left\\{\\begin{array}{cc}\n\\frac{3}{2}x^2 + x & 0\\le y \\le 1 \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\nFind the probability density function of \\(Y=5-(X/2)\\)?"
  },
  {
    "objectID": "lectures/lecture_15.html#mgf-properties-linearity",
    "href": "lectures/lecture_15.html#mgf-properties-linearity",
    "title": "Functions of Random Variables",
    "section": "MGF Properties: Linearity",
    "text": "MGF Properties: Linearity\nLet \\(X\\) follow a distribution \\(f\\), with the an MGF \\(M_X(t)\\), the MGF of \\(Y=aX+b\\) is given as\n\\[\nM_Y(t) = e^{tb}M_X(at)\n\\]"
  },
  {
    "objectID": "lectures/lecture_15.html#mgf-properties-uniqueness",
    "href": "lectures/lecture_15.html#mgf-properties-uniqueness",
    "title": "Functions of Random Variables",
    "section": "MGF Properties: Uniqueness",
    "text": "MGF Properties: Uniqueness\nLet \\(X\\) and \\(Y\\) have the following distributions \\(F_X(x)\\) and \\(F_Y(y)\\) and MGFs \\(M_X(t)\\) and \\(M_Y(t)\\), respectively. \\(X\\) and \\(Y\\) have the same distribution \\(F_X(x)=F_Y(y)\\) if and only if \\(M_X(t)=M_Y(t)\\)."
  },
  {
    "objectID": "lectures/lecture_15.html#using-the-mgf",
    "href": "lectures/lecture_15.html#using-the-mgf",
    "title": "Functions of Random Variables",
    "section": "Using the MGF",
    "text": "Using the MGF\nUsing the uniqueness property of Moment Generating Functions, for a random variable \\(X\\) with a known distribution function \\(F_X(x)\\) and random variable \\(Y=g(X)\\), the distribution of \\(Y\\) can be found by:\n\nFind the moment generating function of \\(Y\\), \\(M_Y(t)\\).\nCompare \\(M_Y(t)\\), with known moment generating functions. If \\(M_Y(t)=M_V(t)\\), for all values \\(t\\), them \\(Y\\) and \\(V\\) have identical distributions."
  },
  {
    "objectID": "lectures/lecture_15.html#example-3",
    "href": "lectures/lecture_15.html#example-3",
    "title": "Functions of Random Variables",
    "section": "Example 3",
    "text": "Example 3\nLet \\(X\\) follow a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Find the distribution of \\(Z=\\frac{X-\\mu}{\\sigma}\\)."
  },
  {
    "objectID": "lectures/lecture_15.html#example-4",
    "href": "lectures/lecture_15.html#example-4",
    "title": "Functions of Random Variables",
    "section": "Example 4",
    "text": "Example 4\nLet \\(Z\\) follow a standard normal distribution with mean \\(0\\) and variance \\(1\\). Find the distribution of \\(Y=Z^2\\)"
  },
  {
    "objectID": "lectures/lecture_17.html#learning-outcomes",
    "href": "lectures/lecture_17.html#learning-outcomes",
    "title": "Maximum Likelihood Estimators",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMaximum Likelihood Estimators\nLog-Likelihood Functions"
  },
  {
    "objectID": "lectures/lecture_17.html#estimators",
    "href": "lectures/lecture_17.html#estimators",
    "title": "Maximum Likelihood Estimators",
    "section": "Estimators",
    "text": "Estimators\nAn estimator is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample."
  },
  {
    "objectID": "lectures/lecture_17.html#data",
    "href": "lectures/lecture_17.html#data",
    "title": "Maximum Likelihood Estimators",
    "section": "Data",
    "text": "Data\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)\\) where \\(F(\\cdot)\\) is a known distribution function and \\(\\boldsymbol\\theta\\) is a vector of parameters. Let \\(\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}\\), be the sample collected."
  },
  {
    "objectID": "lectures/lecture_17.html#likelihood-function",
    "href": "lectures/lecture_17.html#likelihood-function",
    "title": "Maximum Likelihood Estimators",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nUsing the joint pdf or pmf of the sample \\(\\boldsymbol X\\), the likelihood function is a function of \\(\\boldsymbol \\theta\\), given the observed data \\(\\boldsymbol X =\\boldsymbol x\\), defined as\n\\[\nL(\\boldsymbol \\theta|\\boldsymbol x)=f(\\boldsymbol x|\\boldsymbol \\theta)\n\\]\nIf the data is iid, then\n\\[\nf(\\boldsymbol x|\\boldsymbol \\theta) = \\prod^n_{i=1}f(x_i|\\boldsymbol\\theta)\n\\]"
  },
  {
    "objectID": "lectures/lecture_17.html#log-likelihood-function",
    "href": "lectures/lecture_17.html#log-likelihood-function",
    "title": "Maximum Likelihood Estimators",
    "section": "Log-Likelihood Function",
    "text": "Log-Likelihood Function\nIf \\(\\ln\\{L(\\boldsymbol \\theta)\\}\\) is monotone of \\(\\boldsymbol \\theta\\), then maximizing \\(\\ell(\\boldsymbol\\theta) = \\ln\\{L(\\boldsymbol \\theta)\\}\\) will yield the maximum likelihood estimators."
  },
  {
    "objectID": "lectures/lecture_17.html#maximum-log-likelihood-estimator",
    "href": "lectures/lecture_17.html#maximum-log-likelihood-estimator",
    "title": "Maximum Likelihood Estimators",
    "section": "Maximum log-Likelihood Estimator",
    "text": "Maximum log-Likelihood Estimator\nThe maximum likelihood estimator are the estimates of \\(\\boldsymbol \\theta\\) that maximize \\(\\ell(\\boldsymbol\\theta)\\)."
  },
  {
    "objectID": "lectures/lecture_17.html#poisson-distribution",
    "href": "lectures/lecture_17.html#poisson-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Pois}(\\lambda)\\), show that the MLE of \\(\\lambda\\) is \\(\\bar x\\)."
  },
  {
    "objectID": "lectures/lecture_17.html#normal-distribution",
    "href": "lectures/lecture_17.html#normal-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). Show that the MLE’s of \\(\\mu\\) and \\(\\sigma^2\\) are \\(\\bar x\\) and \\(\\frac{n-1}{n}s^2\\), respectively."
  },
  {
    "objectID": "lectures/lecture_17.html#exponential-distribution",
    "href": "lectures/lecture_17.html#exponential-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nLet \\(X_1,\\ldots,X_n\\overset{iid}{\\sim}Exp(\\lambda)\\). Find the MLE of \\(\\lambda\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#learning-outcomes",
    "href": "lectures/lecture_19.html#learning-outcomes",
    "title": "Linear Regression",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nScatter Plot\nLinear Regression\nOrdinary Least Squares\nUnbiasedness"
  },
  {
    "objectID": "lectures/lecture_19.html#scatter-plot-1",
    "href": "lectures/lecture_19.html#scatter-plot-1",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_19.html#scatter-plot-2",
    "href": "lectures/lecture_19.html#scatter-plot-2",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_19.html#linear-regression-1",
    "href": "lectures/lecture_19.html#linear-regression-1",
    "title": "Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is used to model the association between a set of predictor variables (x’s) and an outcome variable (y). Linear regression will fit a line that best describes the data points."
  },
  {
    "objectID": "lectures/lecture_19.html#simple-linear-regression",
    "href": "lectures/lecture_19.html#simple-linear-regression",
    "title": "Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nSimple linear regression will model the association between one predictor variable and an outcome:\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\\(\\beta_0\\): Intercept term\n\\(\\beta_1\\): Slope term\n\\(\\epsilon\\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#fitting-a-line",
    "href": "lectures/lecture_19.html#fitting-a-line",
    "title": "Linear Regression",
    "section": "Fitting a Line",
    "text": "Fitting a Line"
  },
  {
    "objectID": "lectures/lecture_19.html#interpretation",
    "href": "lectures/lecture_19.html#interpretation",
    "title": "Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\n\\hat y = 136.73 + 0.015 x\n\\]"
  },
  {
    "objectID": "lectures/lecture_19.html#ordinary-least-squares-1",
    "href": "lectures/lecture_19.html#ordinary-least-squares-1",
    "title": "Linear Regression",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nFor a data pair \\((X_i,Y_i)_{i=1}^n\\), the ordinary least squares estimator will find the estimates of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize the following function:\n\\[\n\\sum^n_{i=1}\\{y_i-(\\beta_0+\\beta_1x_i)\\}^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_19.html#estimating-betas",
    "href": "lectures/lecture_19.html#estimating-betas",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta\\)’s",
    "text": "Estimating \\(\\beta\\)’s"
  },
  {
    "objectID": "lectures/lecture_19.html#estimating-beta_1",
    "href": "lectures/lecture_19.html#estimating-beta_1",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta_1\\)",
    "text": "Estimating \\(\\beta_1\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#estimating-beta_0",
    "href": "lectures/lecture_19.html#estimating-beta_0",
    "title": "Linear Regression",
    "section": "Estimating \\(\\beta_0\\)",
    "text": "Estimating \\(\\beta_0\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#estimates",
    "href": "lectures/lecture_19.html#estimates",
    "title": "Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\n\\] \\[\n\\hat\\beta_1 = \\frac{\\sum^n_{i=1}(y_i-\\bar y)(x_i-\\bar x)}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\] \\[\n\\hat\\sigma^2 = \\frac{1}{n-2}\\sum^n_{i=1}(y_i-\\hat y_i)^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_19.html#unbiasedness-of-betas-1",
    "href": "lectures/lecture_19.html#unbiasedness-of-betas-1",
    "title": "Linear Regression",
    "section": "Unbiasedness of \\(\\beta\\)’s",
    "text": "Unbiasedness of \\(\\beta\\)’s\nBoth \\(\\beta_0\\) and \\(\\beta_1\\) are unbiased estimators."
  },
  {
    "objectID": "lectures/lecture_19.html#ebeta_0",
    "href": "lectures/lecture_19.html#ebeta_0",
    "title": "Linear Regression",
    "section": "\\(E(\\beta_0)\\)",
    "text": "\\(E(\\beta_0)\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#ebeta_1",
    "href": "lectures/lecture_19.html#ebeta_1",
    "title": "Linear Regression",
    "section": "\\(E(\\beta_1)\\)",
    "text": "\\(E(\\beta_1)\\)"
  },
  {
    "objectID": "lectures/lecture_20.html#learning-objectives",
    "href": "lectures/lecture_20.html#learning-objectives",
    "title": "Linear Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nMatrix Formulation\nMultiple Linear Regression\nModel Assumptions"
  },
  {
    "objectID": "lectures/lecture_20.html#matrix-version-of-model",
    "href": "lectures/lecture_20.html#matrix-version-of-model",
    "title": "Linear Regression",
    "section": "Matrix Version of Model",
    "text": "Matrix Version of Model\n\\[\nY_i = \\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta + \\epsilon_i\n\\]\n\n\\(Y_i\\): Outcome Variable\n\\(\\boldsymbol X_i=(1, X_i)^\\mathrm T\\): Predictors\n\\(\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T\\): Coefficients\n\\(\\epsilon_i\\): error term"
  },
  {
    "objectID": "lectures/lecture_20.html#data-matrix-formulation",
    "href": "lectures/lecture_20.html#data-matrix-formulation",
    "title": "Linear Regression",
    "section": "Data Matrix Formulation",
    "text": "Data Matrix Formulation\nFor \\(n\\) data points\n\\[\n\\boldsymbol Y = \\boldsymbol X^\\mathrm T\\boldsymbol \\beta + \\boldsymbol \\epsilon\n\\]\n\n\\(\\boldsymbol Y = (Y_1, \\cdots, Y_n)^\\mathrm T\\): Outcome Variable\n\\(\\boldsymbol X=(\\boldsymbol X_1, \\cdots, \\boldsymbol X_n)^\\mathrm T\\): Predictors\n\\(\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T\\): Coefficients\n\\(\\boldsymbol \\epsilon = (\\epsilon_1, \\cdots, \\epsilon_n)^\\mathrm T\\): Error terms"
  },
  {
    "objectID": "lectures/lecture_20.html#least-squares-formula",
    "href": "lectures/lecture_20.html#least-squares-formula",
    "title": "Linear Regression",
    "section": "Least Squares Formula",
    "text": "Least Squares Formula\n\\[\n(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)^\\mathrm T(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#estimates",
    "href": "lectures/lecture_20.html#estimates",
    "title": "Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat{\\boldsymbol \\beta} = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1}\\boldsymbol X ^\\mathrm T\\boldsymbol Y\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#mlr",
    "href": "lectures/lecture_20.html#mlr",
    "title": "Linear Regression",
    "section": "MLR",
    "text": "MLR\nMultivariable linear regression models are used when more than one explanatory variable is used to explain the outcome of interest."
  },
  {
    "objectID": "lectures/lecture_20.html#continuous-variable",
    "href": "lectures/lecture_20.html#continuous-variable",
    "title": "Linear Regression",
    "section": "Continuous Variable",
    "text": "Continuous Variable\nTo fit an additional continuous random variable to the model, we will only need to add it to the model:\n\\[\nY = \\beta_0 +\\beta_1 X_1 + \\beta_2 X_2\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#categorical-variable",
    "href": "lectures/lecture_20.html#categorical-variable",
    "title": "Linear Regression",
    "section": "Categorical Variable",
    "text": "Categorical Variable\nA categorical variable can be included in a model, but a reference category must be specified."
  },
  {
    "objectID": "lectures/lecture_20.html#fitting-a-model-with-categorical-variables",
    "href": "lectures/lecture_20.html#fitting-a-model-with-categorical-variables",
    "title": "Linear Regression",
    "section": "Fitting a model with categorical variables",
    "text": "Fitting a model with categorical variables\nTo fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use \\(C-1\\) dummy variables where \\(C\\) indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables."
  },
  {
    "objectID": "lectures/lecture_20.html#example",
    "href": "lectures/lecture_20.html#example",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nIf we have 4 categories, we will need 3 dummy variables:\n\n\n\n\nCat 1\nCat 2\nCat 3\nCat 4\n\n\n\n\nDummy 1\n1\n0\n0\n0\n\n\nDummy 2\n0\n1\n0\n0\n\n\nDummy 3\n0\n0\n1\n0\n\n\n\nWhich one is the reference category?"
  },
  {
    "objectID": "lectures/lecture_20.html#matrix-notation",
    "href": "lectures/lecture_20.html#matrix-notation",
    "title": "Linear Regression",
    "section": "Matrix Notation",
    "text": "Matrix Notation\n\\[\nY = \\boldsymbol \\beta^T\\boldsymbol X\n\\]\n\n\\(\\boldsymbol \\beta\\): a column vector of regression coefficients\n\\(\\boldsymbol X\\): a column vector of predictor variables"
  },
  {
    "objectID": "lectures/lecture_20.html#model",
    "href": "lectures/lecture_20.html#model",
    "title": "Linear Regression",
    "section": "Model",
    "text": "Model\n\\[\nY = \\boldsymbol \\beta^T\\boldsymbol X\n\\]\n\n\\(\\epsilon \\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "lectures/lecture_20.html#model-scatter-plot",
    "href": "lectures/lecture_20.html#model-scatter-plot",
    "title": "Linear Regression",
    "section": "Model Scatter Plot",
    "text": "Model Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_20.html#model-assumptions-1",
    "href": "lectures/lecture_20.html#model-assumptions-1",
    "title": "Linear Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nErrors are normally distributed\nConstant Variance\nLinearity\nIndependence\nNo outliers"
  },
  {
    "objectID": "lectures/lecture_20.html#errors-normally-distributed",
    "href": "lectures/lecture_20.html#errors-normally-distributed",
    "title": "Linear Regression",
    "section": "Errors Normally Distributed",
    "text": "Errors Normally Distributed"
  },
  {
    "objectID": "lectures/lecture_20.html#constant-variance",
    "href": "lectures/lecture_20.html#constant-variance",
    "title": "Linear Regression",
    "section": "Constant Variance",
    "text": "Constant Variance"
  },
  {
    "objectID": "lectures/lecture_20.html#linearity",
    "href": "lectures/lecture_20.html#linearity",
    "title": "Linear Regression",
    "section": "Linearity",
    "text": "Linearity"
  },
  {
    "objectID": "lectures/lecture_20.html#linearity-1",
    "href": "lectures/lecture_20.html#linearity-1",
    "title": "Linear Regression",
    "section": "Linearity",
    "text": "Linearity"
  },
  {
    "objectID": "lectures/lecture_20.html#no-outliers",
    "href": "lectures/lecture_20.html#no-outliers",
    "title": "Linear Regression",
    "section": "No Outliers",
    "text": "No Outliers"
  },
  {
    "objectID": "lectures/lecture_20.html#residual-analysis",
    "href": "lectures/lecture_20.html#residual-analysis",
    "title": "Linear Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nA residual analysis is used to assess the validity of the assumptions."
  },
  {
    "objectID": "lectures/lecture_22.html#learning-outcomes",
    "href": "lectures/lecture_22.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nEstimation Procedures\n\nRegression Coefficients\nDispersion Parameter\n\nNewton-Raphson Algorithm"
  },
  {
    "objectID": "lectures/lecture_22.html#estimating-boldsymbolbeta",
    "href": "lectures/lecture_22.html#estimating-boldsymbolbeta",
    "title": "Generalized Linear Models",
    "section": "Estimating \\(\\boldsymbol\\beta\\)",
    "text": "Estimating \\(\\boldsymbol\\beta\\)\nTo obtain the estimates of \\(\\boldsymbol \\beta\\) we can use the maximum log-likelihood approach to obtain \\(\\hat{\\boldsymbol\\beta}\\).\n\\[\nL(\\boldsymbol \\beta) =  \\prod^n_{i=1}f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#maximum-likelihood-approach",
    "href": "lectures/lecture_22.html#maximum-likelihood-approach",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood Approach",
    "text": "Maximum Likelihood Approach\n\\[\n\\ell(\\boldsymbol \\beta) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#numerical-approaches",
    "href": "lectures/lecture_22.html#numerical-approaches",
    "title": "Generalized Linear Models",
    "section": "Numerical Approaches",
    "text": "Numerical Approaches\n\nNewton-Rhapson Algorithm\nFisher-Scoring Algorithm\nNelder-Mead\nBFGS"
  },
  {
    "objectID": "lectures/lecture_22.html#estimating-phi-1",
    "href": "lectures/lecture_22.html#estimating-phi-1",
    "title": "Generalized Linear Models",
    "section": "Estimating \\(\\phi\\)",
    "text": "Estimating \\(\\phi\\)\nDepending on the random variable, the dispersion parameter will need to be estimated to conduct inference procedures. There are 4 methods to estimate the dispersion parameter:\n\nMaximum Likelihood\nMaximum (Modified) Profile Likelihood Approach\nMean Deviance Estimator\nPearson Estimator"
  },
  {
    "objectID": "lectures/lecture_22.html#maximum-likelihood-approach-1",
    "href": "lectures/lecture_22.html#maximum-likelihood-approach-1",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood Approach",
    "text": "Maximum Likelihood Approach\n\\[\n\\ell(\\phi) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#maximum-modified-profile-likelihood-approach",
    "href": "lectures/lecture_22.html#maximum-modified-profile-likelihood-approach",
    "title": "Generalized Linear Models",
    "section": "Maximum (Modified) Profile Likelihood Approach",
    "text": "Maximum (Modified) Profile Likelihood Approach\n\\[\n\\ell_p(\\phi) = \\frac{p}{2}\\log \\phi + \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\hat{\\boldsymbol \\beta},\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#mean-deviance-estimator",
    "href": "lectures/lecture_22.html#mean-deviance-estimator",
    "title": "Generalized Linear Models",
    "section": "Mean Deviance Estimator",
    "text": "Mean Deviance Estimator\n\\[\n\\tilde \\phi = \\frac{D(y,\\hat\\mu)}{n-p}\n\\]\n\n\\(D(y,\\hat\\mu)=2\\sum^n_{i=1}\\left\\{t(y,y) - t(y,\\mu) \\right\\}\\)\n\\(t(y,\\mu)=y\\theta-\\kappa(\\theta)\\)\n\\(p\\): number of regression coefficients"
  },
  {
    "objectID": "lectures/lecture_22.html#pearson-estimator",
    "href": "lectures/lecture_22.html#pearson-estimator",
    "title": "Generalized Linear Models",
    "section": "Pearson Estimator",
    "text": "Pearson Estimator\n\\[\n\\bar \\phi = \\frac{\\Lambda^2}{n-p}\n\\]\n\n\\(\\Lambda^2=\\sum^n_{i=1}\\frac{y_i-\\hat\\mu_i}{V(\\hat\\mu_i)}\\)\n\\(\\hat \\mu_i = g^{-1}(\\hat\\beta_0 + \\sum^n_{j=1}{X_{ij}\\hat\\beta_j})\\)\n\\(V(\\hat\\mu_i)=\\frac{d^2\\kappa(\\hat\\theta_i)}{d\\theta_i^2}\\)"
  },
  {
    "objectID": "lectures/lecture_22.html#numerical-algorithm",
    "href": "lectures/lecture_22.html#numerical-algorithm",
    "title": "Generalized Linear Models",
    "section": "Numerical Algorithm",
    "text": "Numerical Algorithm\nIn Mathematics and Statistics, numerical algorithms are used to approximate the value of different functions:\n\nRoot Finding:\n\nNewton’s Method\n\nDerivatives\n\nSecant Step-size\n\nIntegrals\n\nReimman Sums\n\nMaximization\n\nNewton-Raphson"
  },
  {
    "objectID": "lectures/lecture_22.html#optimization",
    "href": "lectures/lecture_22.html#optimization",
    "title": "Generalized Linear Models",
    "section": "Optimization",
    "text": "Optimization\nOptimization is the techniques used to find the values that maximizes the a function:\n\\[\nx_0 = \\mathrm{argmax}_{x}f(x)\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#newton-raphson",
    "href": "lectures/lecture_22.html#newton-raphson",
    "title": "Generalized Linear Models",
    "section": "Newton-Raphson",
    "text": "Newton-Raphson\nThe Newton-Raphson algorithm is used to estimate the parameters using an iterative algorithm. Given initial estimates, it will update the estimates of the parameters using the Newton step. It will continue iterating and updating the steps until the function converges to the maximum value."
  },
  {
    "objectID": "lectures/lecture_22.html#newton-raphson-1",
    "href": "lectures/lecture_22.html#newton-raphson-1",
    "title": "Generalized Linear Models",
    "section": "Newton-Raphson",
    "text": "Newton-Raphson\n\\[\n\\beta_j^{(it+1)} = \\beta_j^{(it)} - \\frac{G_{\\beta_j}^{(it)}}{H_{\\beta_j}^{(it)}}\n\\]\n\n\\(\\beta_j^{(it)}\\): current estimate of \\(\\beta_j\\)\n\\(G_{\\beta_j}^{(it)}=d\\ell(\\boldsymbol \\beta)/d\\beta_j|_{\\beta_j=\\beta_j^{(it)}}\\)\n\\(H_{\\beta_j}^{(it)}=d^2\\ell(\\boldsymbol \\beta)/d\\beta_j^2|_{\\beta_j=\\beta_j^{(it)}}\\)\n\\(\\beta_j^{(it+1)}\\): Updated estimate of \\(\\beta_j\\)"
  },
  {
    "objectID": "lectures/lecture_22.html#logistic-regression",
    "href": "lectures/lecture_22.html#logistic-regression",
    "title": "Generalized Linear Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet \\((Y_i,X_i)_{i=1}^n\\) be a data set where \\(Y_i\\overset{iid}{\\sim}Bernoulli(p)\\). Find the first and second derivative for \\(\\beta_1\\), when a GLM is fitted to the model."
  },
  {
    "objectID": "lectures/lecture_22.html#poisson-regression",
    "href": "lectures/lecture_22.html#poisson-regression",
    "title": "Generalized Linear Models",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nLet \\((Y_i,X_i)_{i=1}^n\\) be a data set where \\(Y_i\\overset{iid}{\\sim}Pois(\\lambda)\\). Find the first and second derivative for \\(\\beta_0\\), when a GLM is fitted to the model."
  },
  {
    "objectID": "lectures/lecture_25.html#learning-objectives",
    "href": "lectures/lecture_25.html#learning-objectives",
    "title": "Interpreting Output",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nR\nPython\nJulia\nSPSS\nStata\nSAS\nExcel\nMinitab"
  },
  {
    "objectID": "lectures/lecture_25.html#r",
    "href": "lectures/lecture_25.html#r",
    "title": "Interpreting Output",
    "section": "R",
    "text": "R"
  },
  {
    "objectID": "lectures/lecture_25.html#python",
    "href": "lectures/lecture_25.html#python",
    "title": "Interpreting Output",
    "section": "Python",
    "text": "Python"
  },
  {
    "objectID": "lectures/lecture_25.html#julia",
    "href": "lectures/lecture_25.html#julia",
    "title": "Interpreting Output",
    "section": "Julia",
    "text": "Julia"
  },
  {
    "objectID": "lectures/lecture_25.html#sas",
    "href": "lectures/lecture_25.html#sas",
    "title": "Interpreting Output",
    "section": "SAS",
    "text": "SAS"
  },
  {
    "objectID": "lectures/lecture_25.html#stata",
    "href": "lectures/lecture_25.html#stata",
    "title": "Interpreting Output",
    "section": "Stata",
    "text": "Stata"
  },
  {
    "objectID": "lectures/lecture_25.html#spss",
    "href": "lectures/lecture_25.html#spss",
    "title": "Interpreting Output",
    "section": "SPSS",
    "text": "SPSS"
  },
  {
    "objectID": "lectures/lecture_25.html#excel",
    "href": "lectures/lecture_25.html#excel",
    "title": "Interpreting Output",
    "section": "Excel",
    "text": "Excel"
  },
  {
    "objectID": "lectures/lecture_25.html#minitab",
    "href": "lectures/lecture_25.html#minitab",
    "title": "Interpreting Output",
    "section": "Minitab",
    "text": "Minitab"
  },
  {
    "objectID": "lectures/lecture_25.html#output",
    "href": "lectures/lecture_25.html#output",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1058.80  -259.27   -26.88   247.33  1288.69 \n#&gt; \n#&gt; Coefficients:\n#&gt;                    Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -5780.831    305.815  -18.90   &lt;2e-16 ***\n#&gt; flipper_length_mm    49.686      1.518   32.72   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 394.3 on 340 degrees of freedom\n#&gt;   (2 observations deleted due to missingness)\n#&gt; Multiple R-squared:  0.759,  Adjusted R-squared:  0.7583 \n#&gt; F-statistic:  1071 on 1 and 340 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/lecture_25.html#output-1",
    "href": "lectures/lecture_25.html#output-1",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-2",
    "href": "lectures/lecture_25.html#output-2",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-3",
    "href": "lectures/lecture_25.html#output-3",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-4",
    "href": "lectures/lecture_25.html#output-4",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-5",
    "href": "lectures/lecture_25.html#output-5",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-6",
    "href": "lectures/lecture_25.html#output-6",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_25.html#output-7",
    "href": "lectures/lecture_25.html#output-7",
    "title": "Interpreting Output",
    "section": "Output",
    "text": "Output"
  },
  {
    "objectID": "lectures/lecture_4.html#learning-outcomes",
    "href": "lectures/lecture_4.html#learning-outcomes",
    "title": "Introduction to Probability",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDescribe disjoint events\nDescribe a conditional probability\nDefine independent events\nLaw of Total Probability\nBaye’s Theorem"
  },
  {
    "objectID": "lectures/lecture_4.html#disjoint-events-1",
    "href": "lectures/lecture_4.html#disjoint-events-1",
    "title": "Introduction to Probability",
    "section": "Disjoint Events",
    "text": "Disjoint Events\nTwo events A and B are considered disjoint if \\(P(A\\cap B)=0\\). In general terms, only one event can occur, not both."
  },
  {
    "objectID": "lectures/lecture_4.html#diagram",
    "href": "lectures/lecture_4.html#diagram",
    "title": "Introduction to Probability",
    "section": "Diagram",
    "text": "Diagram"
  },
  {
    "objectID": "lectures/lecture_4.html#conditional-probability-1",
    "href": "lectures/lecture_4.html#conditional-probability-1",
    "title": "Introduction to Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nLet there be 2 events A and B. Given that B has occurred, what is the probability that A occurs? The conditional probability requires there to be at least 2 events and one event must have occurred. Additionally, the events cannot be disjoint. Conditional probabilities are denoted as \\(P(A|B)\\), the probability of A given B has occurred.\n\\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_4.html#diagram-1",
    "href": "lectures/lecture_4.html#diagram-1",
    "title": "Introduction to Probability",
    "section": "Diagram",
    "text": "Diagram"
  },
  {
    "objectID": "lectures/lecture_4.html#example",
    "href": "lectures/lecture_4.html#example",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\n\n\n\nUses Eye Glasses\n\n\n\nNeeds Glasses\nYes\nNo\n\n\nYes\n44\n14\n\n\nNo\n2\n40\n\n\n\n\nFind the probability of needing glasses\nFind the probability of not using glasses\nFind the probability of not using glasses and needing glasses\nFind the probability of not using glasses, given they need glasses"
  },
  {
    "objectID": "lectures/lecture_4.html#work",
    "href": "lectures/lecture_4.html#work",
    "title": "Introduction to Probability",
    "section": "Work",
    "text": "Work"
  },
  {
    "objectID": "lectures/lecture_4.html#independent-events-1",
    "href": "lectures/lecture_4.html#independent-events-1",
    "title": "Introduction to Probability",
    "section": "Independent Events",
    "text": "Independent Events\nEvents A and B are considered independent if \\(P(A\\cap B)=P(A)P(B)\\). In other words, The occurrence of one event will not have an effect on the occurrence of the other event."
  },
  {
    "objectID": "lectures/lecture_4.html#example-1",
    "href": "lectures/lecture_4.html#example-1",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\n\n\n\n\nUses Eye Glasses\n\n\n\n\n\nNeeds Glasses\nYes\nNo\n\n\nYes\n44\n14\n\n\nNo\n2\n40"
  },
  {
    "objectID": "lectures/lecture_4.html#law-of-total-probability-1",
    "href": "lectures/lecture_4.html#law-of-total-probability-1",
    "title": "Introduction to Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\nThe law of total probability allows you to compute the probability of an event A given that the sets {\\(B_1\\), … , \\(B_n\\)} partitions event A. The law of total probability is given as\n\\[\nP(A)= \\sum^n_{i=1}P(A\\cap B_i)\n\\]\n\n\\[\nP(A)=\\sum^n_{i=1}P(A|B_i)P(B_i)\n\\]"
  },
  {
    "objectID": "lectures/lecture_4.html#diagrams",
    "href": "lectures/lecture_4.html#diagrams",
    "title": "Introduction to Probability",
    "section": "Diagrams",
    "text": "Diagrams"
  },
  {
    "objectID": "lectures/lecture_4.html#example-2",
    "href": "lectures/lecture_4.html#example-2",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nThe probability of an individual having a disease, given that they test positive for the disease, is 0.82. The probability of an individual having a disease, given they tested negative, is 0.14. The probability of testing positive is 0.6. What is the prevalence of a disease (probability of having a disease)?"
  },
  {
    "objectID": "lectures/lecture_4.html#bayes-theorem-1",
    "href": "lectures/lecture_4.html#bayes-theorem-1",
    "title": "Introduction to Probability",
    "section": "Baye’s Theorem",
    "text": "Baye’s Theorem\nBaye’s theorem computes the probability of an event \\(B_i\\) given event A\n\\[\nP(B_i|A) = \\frac{P(A\\cap B_i)}{P(A)}=\\frac{P(A|B_i)P(B_i)}{\\sum^n_{i=1}P(A|B_i)P(B_i)}\n\\]"
  },
  {
    "objectID": "lectures/lecture_4.html#example-3",
    "href": "lectures/lecture_4.html#example-3",
    "title": "Introduction to Probability",
    "section": "Example",
    "text": "Example\nThe probability of an having a disease, given that they test positive for the disease, is 0.82. The probability of and individual having a disease, given they tested negative, is 0.14. The probability of testing positive is 0.6. What is the probability of resulting in a false negative?"
  },
  {
    "objectID": "lectures/lecture_6.html#learning-outcomes",
    "href": "lectures/lecture_6.html#learning-outcomes",
    "title": "Expectation of a Random Variable",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExpected Values\nVariances\nProperties of Expected Values"
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value",
    "href": "lectures/lecture_6.html#expected-value",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of \\(Y\\) is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]\nwhere \\(P(y)\\) is the PMF of \\(Y\\)."
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value-1",
    "href": "lectures/lecture_6.html#expected-value-1",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value of a function of a random variable \\(Y\\) is provided as\n\\[\nE\\{g(Y)\\} = \\sum_y g(y)P(y)\n\\]"
  },
  {
    "objectID": "lectures/lecture_6.html#variance",
    "href": "lectures/lecture_6.html#variance",
    "title": "Expectation of a Random Variable",
    "section": "Variance",
    "text": "Variance\nThe variance is the expected squared difference between the random variable and expected value.\n\\[\nVar(Y)= E[\\{Y-E(Y)\\}^2]=\\sum_y\\{y-E(Y)\\}^2P(y)\n\\]\n\\[\nVar(Y) = E(Y^2) - E(Y)^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value-2",
    "href": "lectures/lecture_6.html#expected-value-2",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/lecture_6.html#variance-1",
    "href": "lectures/lecture_6.html#variance-1",
    "title": "Expectation of a Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value-3",
    "href": "lectures/lecture_6.html#expected-value-3",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/lecture_6.html#variance-2",
    "href": "lectures/lecture_6.html#variance-2",
    "title": "Expectation of a Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/lecture_6.html#expected-value-4",
    "href": "lectures/lecture_6.html#expected-value-4",
    "title": "Expectation of a Random Variable",
    "section": "Expected Value",
    "text": "Expected Value"
  },
  {
    "objectID": "lectures/lecture_6.html#variance-3",
    "href": "lectures/lecture_6.html#variance-3",
    "title": "Expectation of a Random Variable",
    "section": "Variance",
    "text": "Variance"
  },
  {
    "objectID": "lectures/lecture_6.html#properties",
    "href": "lectures/lecture_6.html#properties",
    "title": "Expectation of a Random Variable",
    "section": "Properties",
    "text": "Properties\n\n\\(E(c)=c\\), where \\(c\\) is constant\n\\(E\\{cg(Y)\\}=cE\\{g(Y)\\}\\)\n\\(E\\{g_1(Y)+\\cdots+g_n(Y)\\}=E\\{g_1(Y)\\}+\\cdots+E\\{g_n(Y)\\}\\)"
  },
  {
    "objectID": "lectures/lecture_9.html#learning-outcomes",
    "href": "lectures/lecture_9.html#learning-outcomes",
    "title": "Continuous Random Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nContinuous Random Variables\n\nProbability Density Functions\nCumulative Density/Distribution Function\n\nCommon Distributions"
  },
  {
    "objectID": "lectures/lecture_9.html#continuous-random-variable",
    "href": "lectures/lecture_9.html#continuous-random-variable",
    "title": "Continuous Random Variables",
    "section": "Continuous Random Variable",
    "text": "Continuous Random Variable\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist.\n\n\n\n\n\n\n\n\n\nDistribution\nParameters\nPDF\n\n\n\n\nUniform\n\\(a\\) and \\(b\\)\n\\(\\frac{1}{b-a}\\)\n\n\nNormal\n\\(\\mu\\) and \\(\\sigma^2\\)\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\)\n\n\nExponential\n\\(\\lambda\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\nGamma\n\\(\\alpha\\) and \\(\\beta\\)\n\\(\\frac{x^{\\alpha-1}e^{-x/\\beta}}{\\beta^\\alpha\\Gamma(\\alpha)}\\)\n\n\nBeta\n\\(\\alpha\\) and \\(\\beta\\)\n\\(\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\int^1_0x^{\\alpha-1}(1-x)^{\\beta-1}dx}\\)"
  },
  {
    "objectID": "lectures/lecture_9.html#cumulative-density-function",
    "href": "lectures/lecture_9.html#cumulative-density-function",
    "title": "Continuous Random Variables",
    "section": "Cumulative Density Function",
    "text": "Cumulative Density Function\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function"
  },
  {
    "objectID": "lectures/lecture_9.html#probability-density-function",
    "href": "lectures/lecture_9.html#probability-density-function",
    "title": "Continuous Random Variables",
    "section": "Probability Density Function",
    "text": "Probability Density Function\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)"
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-to-cdf",
    "href": "lectures/lecture_9.html#pdf-to-cdf",
    "title": "Continuous Random Variables",
    "section": "PDF to CDF",
    "text": "PDF to CDF"
  },
  {
    "objectID": "lectures/lecture_9.html#obtaining-probability",
    "href": "lectures/lecture_9.html#obtaining-probability",
    "title": "Continuous Random Variables",
    "section": "Obtaining Probability",
    "text": "Obtaining Probability"
  },
  {
    "objectID": "lectures/lecture_9.html#uniform-distribution-1",
    "href": "lectures/lecture_9.html#uniform-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nA random variable is said to follow uniform distribution if the density function is constant between two parameters."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf",
    "href": "lectures/lecture_9.html#pdf",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/lecture_9.html#cdf",
    "href": "lectures/lecture_9.html#cdf",
    "title": "Continuous Random Variables",
    "section": "CDF",
    "text": "CDF"
  },
  {
    "objectID": "lectures/lecture_9.html#exponential-distribution-1",
    "href": "lectures/lecture_9.html#exponential-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nAn exponential distribution is used to model positive continuous random variables, commonly used for time."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-1",
    "href": "lectures/lecture_9.html#pdf-1",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/lecture_9.html#cdf-1",
    "href": "lectures/lecture_9.html#cdf-1",
    "title": "Continuous Random Variables",
    "section": "CDF",
    "text": "CDF"
  },
  {
    "objectID": "lectures/lecture_9.html#normal-distribution-1",
    "href": "lectures/lecture_9.html#normal-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-2",
    "href": "lectures/lecture_9.html#pdf-2",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/lecture_9.html#gamma-distribution-1",
    "href": "lectures/lecture_9.html#gamma-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\nA gamma distribution is the general form of distribution for a \\(\\chi^2\\)-distribution and exponential distribution."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-3",
    "href": "lectures/lecture_9.html#pdf-3",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "lectures/lecture_9.html#beta-distribution-1",
    "href": "lectures/lecture_9.html#beta-distribution-1",
    "title": "Continuous Random Variables",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nA beta distribution models a continuous random variable that only has support between the 0 and 1."
  },
  {
    "objectID": "lectures/lecture_9.html#pdf-4",
    "href": "lectures/lecture_9.html#pdf-4",
    "title": "Continuous Random Variables",
    "section": "PDF",
    "text": "PDF"
  },
  {
    "objectID": "r.html",
    "href": "r.html",
    "title": "R Labs",
    "section": "",
    "text": "Below are the R Labs for the course. Make sure you complete and submit your QMD file by the deadline on Canvas.\n\n\n\n\n\n\n\n\nNo matching items"
  }
]