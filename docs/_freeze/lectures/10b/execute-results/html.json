{
  "hash": "73efe10c8302675da7578d45db5ae451",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum Likelihood Estimators\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: true \n    chalkboard:\n      src: chalkboard.json\n      storage: \"chalkboard_pres\"\n      theme: whiteboard\n      chalk-width: 4\neditor: source\n---\n\n\n\n## Learning Outcomes\n\n-   Maximum Likelihood Estimators\n\n-   Properties\n\n# Background Information\n\n## Estimators\n\nAn *estimator* is an operation computing the value of an estimate, that targets the parameter, using measurements from a sample.\n\n## Data\n\nLet $X_1,\\ldots,X_n\\overset{iid}{\\sim}F(\\boldsymbol \\theta)$ where $F(\\cdot)$ is a known distribution function and $\\boldsymbol\\theta$ is a vector of parameters. Let $\\boldsymbol X = (X_1,\\ldots, X_n)^\\mathrm{T}$, be the sample collected.\n\n# MLE Properties\n\n## Unbiased Estimators\n\nLet $X_1,\\ldots,X_n$ be a random sample from a distribution with parameter $\\theta$. Let $\\hat \\theta$ be an estimator for a parameter $\\theta$. Then $\\hat \\theta$ is an unbiased estimator if $E(\\hat \\theta) = \\theta$. Otherwise, $\\hat\\theta$ is considered biased.\n\n## Consistent Estimators\n\nLet $X_1,\\ldots,X_n$ be a random sample from a distribution with parameter $\\theta$. The estimator $\\hat \\theta$ is a consistent estimator of the $\\theta$ if\n\n1.  $E\\{(\\hat\\theta-\\theta)^2\\}\\rightarrow0$ as $n\\rightarrow \\infty$\n2.  $P(|\\hat\\theta-\\theta|\\ge \\epsilon)\\rightarrow0$ as $n\\rightarrow \\infty$ for every $\\epsilon>0$\n\n## Invariance Property\n\nIf $\\hat \\theta$ is an ML estimator of $\\theta$, then for any one-to-one function $g$, the ML estimator for $g(\\theta)$ is $g(\\hat\\theta)$.\n\n## Large Sample Theory\n\nLet $X_1,\\ldots,X_n$ be a random sample from a distribution with parameter $\\theta$. Let $\\hat \\theta$ be the MLE estimator for a parameter $\\theta$. As $n\\rightarrow\\infty$, then $\\hat \\theta$ has a normal distribution with mean $\\theta$ and variance $1/nI(\\theta)$, where\n\n$$\nI(\\theta)=E\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\{f(X;\\theta)\\}\\right]\n$$\n\n# Example\n\n## Exponential Distribution\n\nLet $X_1,\\ldots,X_n\\overset{iid}{\\sim}Exp(\\lambda)$. Find the sampling distribution of the MLE of $\\lambda$\n\n## Poisson Distribution\n\nLet $X_1,\\ldots,X_n\\overset{iid}{\\sim}\\mathrm{Pois}(\\lambda)$, Find the sampling distribution of the MLE.\n\n## Normal Distribution\n\nLet $X_1,\\ldots,X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. Are the MLE's of $\\mu$ and $\\sigma^2$ unbiased?",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}