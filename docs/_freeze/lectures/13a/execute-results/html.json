{
  "hash": "dfc01fb12b957d8b1666dc4286b8adba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Generalized Linear Models\"\nsubtitle: \"Estimation\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: false\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \neditor: source\n---\n\n## Learning Outcomes\n\n-   Estimation Procedures\n\n    -   Regression Coefficients\n\n    -   Dispersion Parameter\n\n-   Newton-Raphson Algorithm\n\n# Estimating: $\\boldsymbol \\beta$\n\n## Estimating $\\boldsymbol\\beta$\n\nTo obtain the estimates of $\\boldsymbol \\beta$ we can use the maximum log-likelihood approach to obtain $\\hat{\\boldsymbol\\beta}$.\n\n$$\nL(\\boldsymbol \\beta) =  \\prod^n_{i=1}f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\n$$\n\n## Maximum Likelihood Approach\n\n$$\n\\ell(\\boldsymbol \\beta) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n$$\n\n## Numerical Approaches\n\n-   Newton-Rhapson Algorithm\n\n-   Fisher-Scoring Algorithm\n\n-   Nelder-Mead\n\n-   BFGS\n\n# Estimating: $\\phi$\n\n## Estimating $\\phi$\n\nDepending on the random variable, the dispersion parameter will need to be estimated to conduct inference procedures. There are 4 methods to estimate the dispersion parameter:\n\n-   Maximum Likelihood\n\n-   Maximum (Modified) Profile Likelihood Approach\n\n-   Mean Deviance Estimator\n\n-   Pearson Estimator\n\n## Maximum Likelihood Approach\n\n$$\n\\ell(\\phi) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n$$\n\n## Maximum (Modified) Profile Likelihood Approach\n\n$$\n\\ell_p(\\phi) = \\frac{p}{2}\\log \\phi + \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\hat{\\boldsymbol \\beta},\\phi\\right)\\right\\}\n$$\n\n## Mean Deviance Estimator\n\n$$\n\\tilde \\phi = \\frac{D(y,\\hat\\mu)}{n-p}\n$$\n\n-   $D(y,\\hat\\mu)=2\\sum^n_{i=1}\\left\\{t(y,y) - t(y,\\mu) \\right\\}$\n\n-   $t(y,\\mu)=y\\theta-\\kappa(\\theta)$\n\n-   $p$: number of regression coefficients\n\n## Pearson Estimator\n\n$$\n\\bar \\phi = \\frac{\\Lambda^2}{n-p}\n$$\n\n-   $\\Lambda^2=\\sum^n_{i=1}\\frac{y_i-\\hat\\mu_i}{V(\\hat\\mu_i)}$\n\n-   $\\hat \\mu_i = g^{-1}(\\hat\\beta_0 + \\sum^n_{j=1}{X_{ij}\\hat\\beta_j})$\n\n-   $V(\\hat\\mu_i)=\\frac{d^2\\kappa(\\hat\\theta_i)}{d\\theta_i^2}$\n\n# Newton-Raphson Algorithm\n\n## Numerical Algorithm\n\nIn Mathematics and Statistics, numerical algorithms are used to approximate the value of different functions:\n\n-   Root Finding:\n\n    -   Newton's Method\n\n-   Derivatives\n\n    -   Secant Step-size\n\n-   Integrals\n\n    -   Reimman Sums\n\n-   Maximization\n\n    -   Newton-Raphson\n\n## Optimization\n\nOptimization is the techniques used to find the values that maximizes the a function:\n\n$$\nx_0 = \\mathrm{argmax}_{x}f(x)\n$$\n\n## Newton-Raphson\n\nThe Newton-Raphson algorithm is used to estimate the parameters using an iterative algorithm. Given initial estimates, it will update the estimates of the parameters using the Newton step. It will continue iterating and updating the steps until the function converges to the maximum value.\n\n## Newton-Raphson\n\n$$\n\\beta_j^{(it+1)} = \\beta_j^{(it)} - \\frac{G_{\\beta_j}^{(it)}}{H_{\\beta_j}^{(it)}}\n$$\n\n-   $\\beta_j^{(it)}$: current estimate of $\\beta_j$\n\n-   $G_{\\beta_j}^{(it)}=d\\ell(\\boldsymbol \\beta)/d\\beta_j|_{\\beta_j=\\beta_j^{(it)}}$\n\n-   $H_{\\beta_j}^{(it)}=d^2\\ell(\\boldsymbol \\beta)/d\\beta_j^2|_{\\beta_j=\\beta_j^{(it)}}$\n\n-   $\\beta_j^{(it+1)}$: Updated estimate of $\\beta_j$\n\n# Example\n\n## Logistic Regression\n\nLet $(Y_i,X_i)_{i=1}^n$ be a data set where $Y_i\\overset{iid}{\\sim}Bernoulli(p)$. Find the first and second derivative for $\\beta_1$, when a GLM is fitted to the model.\n\n## Poisson Regression\n\nLet $(Y_i,X_i)_{i=1}^n$ be a data set where $Y_i\\overset{iid}{\\sim}Pois(\\lambda)$. Find the first and second derivative for $\\beta_0$, when a GLM is fitted to the model.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}