{
  "hash": "1e90a9dbcc0f5d343ebd441cf50c85b2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 11 and Week 12\"\ndate: 11-08-22\ndescription: These two weeks, we begin our discussion on hypothesis testing.\neditor: source\nformat:\n  html:\n    toc: false\ndraft: true\n---\n\n## Learning Outcomes\n\n-   Introduction to Hypothesis Testing\n-   Hypothesis Test Assumptions\n\n## Reading\n\nOpen Intro Statistics Book: Chapter 7\n\n## Homework\n\nHW 6 will be posted at the end of the week.\n\n## Important Concepts\n\n### Statistical Hypothesis Testing\n\nA claim about the value of a certain parameter, a set of parameters, or a distribution.\n\n#### Null Hypothesis $H_0$\n\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value.\n\n#### Alternative Hypothesis $H_a$\n\nThe alternative hypothesis contradicts the null hypothesis.\n\n#### Examples of Null and Alternative Hypothesis Tests\n\n| Null Hypothesis    | Alternative Hypothesis |\n|--------------------|------------------------|\n| $H_0: \\mu=\\mu_0$   | $H_a: \\mu\\ne\\mu_0$     |\n| $H_0: \\mu\\le\\mu_0$ | $H_a: \\mu>\\mu_0$       |\n| $H_0: \\mu\\ge\\mu_0$ | $H_0: \\mu<\\mu_0$       |\n\n#### Testing the Hypothesis\n\nA hypothesis test is a statistical procedure to determine whether the null hypothesis is true or not. If we find that the null hypothesis is true, we claim: **Fail to reject the null hypothesis**. If we find the the null hypothesis to be false, we claim : **Reject the null hypothesis.**\n\nTo conduct a hypothesis test, we compute a test statistic based on the distribution of the null hypothesis. Then we determine if the test statistic is in the rejection region of the distribution of the null hypothesis. If it is in the rejection region, we reject the null hypothesis. Otherwise we fail to reject the null hypothesis.\n\nThe rejection region is determined by our $\\alpha$ value. $\\alpha$ determines the probability you are willing to be wrong if you reject the null hypothesis. This probability is something you want to set yourself at the beginning of a study. In general, the majority of researched set this to be $\\alpha=0.05$.\n\n#### Commonly Used Tests\n\n| Test | \\# of Samples | Null Hypothesis | Distribution | DF | Test Statistics | Notes |\n|----------|----------|----------|----------|--------------|----------|----------|\n| t-test | 1 | $\\mu=\\mu_0$ | $t_{DF}$ | $DF=n-1$ | $\\frac{\\bar X-\\mu_0}{\\sqrt{s^2/n}}$ | $n<30$ |\n| z-test | 1 | $\\mu=\\mu_0$ | $N(0,1)$ | None | $\\frac{\\bar X-\\mu_0}{\\sqrt{\\sigma^2/n}}$ | $n>30$ $\\sigma^2$ known |\n| z-test | 1 | $\\mu=\\mu_0$ | $N(0,1)$ | None | $\\frac{\\bar X-\\mu_0}{\\sqrt{s^2/n}}$ | $n>30$ $\\sigma^2$ unknown |\n| paired t-test | 2 | $\\mu_1-\\mu_2=D$ | $t_{DF}$ | $DF=n-1$ | $\\frac{\\bar X_d-D}{\\sqrt{s_d^2/n}}$ | $n$ is the number of pairs |\n| independent t-test | 2 | $\\mu_1-\\mu_2=D$ | $t_{DF}$ | $DF=n_1+n_2-2$ | $\\frac{\\bar X_1-\\bar X_2-D}{\\sqrt{s_p^2(1/n_1+1/n_2)}}$ | $s_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}$ |\n| independent t-test | 2 | $\\mu_1-\\mu_2=D$ | $t_{DF}$ | $DF=\\frac{(s^2_1/n_1+s^2_2/n_2)^2}{(s^2_1/n_1)^2/(n_1-1)+(s^2_2/n_2)^2/(n_2-1)}$ | $\\frac{\\bar X_1-\\bar X_2-D}{\\sqrt{s_1^2/n_1+s^2_2/n_2}}$ |  |\n\n#### One-Side vs Two-Side Hypothesis Tests\n\nNotice how there are 3 types of null and alternative hypothesis, The first type of hypothesis ($H_a:\\mu\\ne\\mu_0$) is considered a 2-sided hypothesis because the rejection region is located in 2 regions. The remaining two hypotheses are considered 1-sided because the rejection region is located on one side of the distribution.\n\n| Null Hypothesis    | Alternative Hypothesis | Side    |\n|--------------------|------------------------|---------|\n| $H_0: \\mu=\\mu_0$   | $H_a: \\mu\\ne\\mu_0$     | 2-sided |\n| $H_0: \\mu\\le\\mu_0$ | $H_a: \\mu>\\mu_0$       | 1-sided |\n| $H_0: \\mu\\ge\\mu_0$ | $H_0: \\mu<\\mu_0$       | 1-sided |\n\n#### Hypothesis Test: Critical Value Approach\n\nDepending on the type of test and alternative hypothesis, you may be asked to conduct the test using a critical value. This approach will require you to compute the test statistic (denoted as $T(x)$) and and compare it with a critical value(s). Depending on you alternative hypothesis and $\\alpha$, you will reject the null hypothesis based on the following table:\n\n| Alternative Hypothesis | Rejection Region | Critical value |\n|------------------|--------------------|----------------------------------|\n| $\\mu>\\mu_0$ | $T(x)>CV$ | $P(X>CV)=\\alpha$ |\n| $\\mu<\\mu_0$ | $T(x)<CV$ | $P(X<CV)=\\alpha$ |\n| $\\mu\\ne\\mu_0$ | $T(X)<CV_1$ or $T(X)>CV_2$ | $P(X<CV_1)=\\alpha/2$ and $P(X>CV_2)=\\alpha/2$ |\n\n#### Hypothesis Test: P-value Approach\n\nThe p-value approach is one of the most common methods to report significant results. It is easier to interpret the p-value because it provides the probability of observing our test statistics, or something more extreme, given that the null hypothesis is true. Depending on the type of test, your p-value may be constructed as:\n\n| Alternative Hypothesis | p-value                 |\n|------------------------|-------------------------|\n| $\\mu>\\mu_0$            | $P(X>T(x))=p$           |\n| $\\mu<\\mu_0$            | $P(X<T(x))=p$           |\n| $\\mu\\ne\\mu_0$          | $2\\times P(X>|T(X)|)=p$ |\n\nIf $p < \\alpha$, then you reject $H_0$; otherwise, you will fail to reject $H_0$.\n\n#### Hypothesis Test: Confidence Interval Approach\n\nThe confidence interval approach can evaluate a hypothesis test where the alternative hypothesis is $\\mu\\ne\\mu_0$. For this approach you will construct a $(1-\\alpha)100\\%$ confidence interval as\n\n$$\nPE \\pm CV *SE\n$$\n\nwhere PE is the point estimate, CV is the critical value based on $\\alpha$ and SE is the standard error. This will result in a lower and upper bound denoted as: $(LB, UB)$. The confidence intervals provides a range values to capture the parameter $\\mu$ such that if you repeat this process $n$ times, $(1-\\alpha)100\\%$ of $n$ will capture the true value of $\\mu$. If $\\mu_0 \\in (LB,UB)$, then you fail to reject $H_0$. If $\\mu_0\\notin (LB,UB)$, then you reject $H_0$.\n\n### Hypothesis Test Assumptions\n\n#### 1-Sample Hypothesis Testing (t-test or z-test)\n\n1.  Independence\n2.  Data comes from a normal distribution **OR** n\\>30\n    1.  QQ plot\n\n    2.  Shapiro-Wilks Test\n\n#### Paired t-test\n\n1.  Independence\n2.  Differences comes from a normal distribution **OR** n\\>30\n    1.  QQ plot\n\n    2.  Shapiro-Wilks Test\n\n#### 2-sample independent t-test\n\n1.  Independence\n2.  Data comes from a normal distribution **OR** $n_1,n_2>30$\n    1.  QQ plot\n\n    2.  Shapiro-Wilks Test\n3.  Equal Variances\n    1.  F-test\n\n    2.  Bartlett's Test\n\n    3.  Levene's Test\n\n#### QQ Plot\n\nA qq (quantile-quantile) plot will plot the sample quantiles from the data versus the theoretical quantiles from an normal distribution. If the data comes from a normal distribution, then the points should line up to $y=x$ line. The following code runs a qq plot from data generated from a standard normal distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(1000)\nqqnorm(x)\nqqline(x)\n```\n\n::: {.cell-output-display}\n![](week_1112_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n#### Shapiro-Wilks Test\n\nThe Shapiro-Wilks Test is a hypothesis test to determine if the data comes from a normal distribution. It tests the following hypothesis\n\n| $H_0$ | $H_1$ |\n|---------------------------------|---------------------------------------|\n| The data comes from a normal distribution. | The data does not come from a normal distribution. |\n\nThis test will result in a p-value that can be used to conduct the hypothesis test. The following code runs a Shapiro-Wilks test on data generated from a standard normal distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(1000)\nshapiro.test(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  x\nW = 0.9977, p-value = 0.1785\n```\n\n\n:::\n:::\n\n\n#### F-test\n\nAn F-test is hypothesis test to see if the variances are equal or not. It has the following Hypothesis:\n\n| $H_0$                             | $H_1$                               |\n|-----------------------------------|-------------------------------------|\n| $\\frac{\\sigma^2_1}{\\sigma^2_2}=1$ | $\\frac{\\sigma^2_1}{\\sigma^2_2}\\ne1$ |\n\nThis test will result in a p-value that can be used to conduct the hypothesis test. The following code runs an F-test on data generated from a standard normal distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(1000)\ny <- rnorm(1000, sd = 0.5)\nvar.test(x,y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tF test to compare two variances\n\ndata:  x and y\nF = 3.5392, num df = 999, denom df = 999, p-value < 2.2e-16\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 3.126171 4.006799\nsample estimates:\nratio of variances \n            3.5392 \n```\n\n\n:::\n:::\n\n\n#### Bartlett's and Levene's Test\n\nBoth Bartlett's and Levene's test the following hypothesis for equal variances:\n\n| $H_0$ | $H_1$ |\n|----------------------------------|--------------------------------------|\n| The variance are equal to each other. | The variances are not equal to each other. |\n\nThis test will result in a p-value that can be used to conduct the hypothesis test. The following code runs a Bartlett's and Levene's test on data generated from a standard normal distribution:\n\nÂ \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(1000)\ny <- rnorm(1000, sd = 0.5)\n\nz <- c(x,y)\ng <- rep(c(0,1),each=1000)\n\nbartlett.test(z, g)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  z and g\nBartlett's K-squared = 432.31, df = 1, p-value < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(car) # Levene's test is found in the car package, you may need to install it\nleveneTest(z, g)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(>F)    \ngroup    1  346.11 < 2.2e-16 ***\n      1998                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Resources\n\n| Playlist | Playlists | Notes |\n|--------------|--------------------------------------------|--------------|\n| Hypothesis Testing | [Videos](https://www.youtube.com/playlist?list=PLQmJJJJVDVnM9NFOQEzgyHjcl02LhFQYp) |  |\n| Hypothesis Testing Examples | [Videos](https://www.youtube.com/playlist?list=PLQmJJJJVDVnPizJhMcCjGn0i2TzXMw3WA) |  |\n| Hypothesis Test Assumptions | [Videos](https://www.youtube.com/playlist?list=PLQmJJJJVDVnPdIRn4NLNaynUzOvBOCAF6) |  |\n\nThursdays Slides and Videos: [Slides 002](https://m352.inqs.info/files/Lecture_15/index.html#/title-slide), [Slides 003](https://m352.inqs.info/files/Lecture_15b/index.html#/title-slide), [Video 002](https://youtu.be/B8bVnyccPHc), and [Video 003](https://youtu.be/V4-f9-I5Xek).",
    "supporting": [
      "week_1112_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}