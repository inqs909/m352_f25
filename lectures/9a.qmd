---
title: "Sampling Distributions"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: true 
    chalkboard:
      src: chalkboard.json
      theme: whiteboard
      chalk-width: 4
editor: source
---

```{r}
#| include: false

knitr::opts_chunk$set(echo = T)

```

## Learning Outcomes

-   Distribution of Mean

-   Distribution of Sample Variance

-   Likelihood Function


## MGF Independence Property

If $X$ and $Y$ are **independent**, then the MGF of $Z = X + Y$ equals the product of their MGFs:

$$
M_{Z}(t) = M_X(t)M_Y(t).
$$

# Sampling Distributions

## Distribution of Means

Suppose $X_1, \dots, X_n \sim \text{N}(\mu, \sigma^2)$

## Distribution of Sample Variance

Suppose $X_1, \dots, X_n \sim \text{N}(\mu, \sigma^2)$

# Likelihood Function

## Likelihood Function

The likelihood function tells us how plausible a set of parameter values is, given the observed data.


## Setup

Given a random sample $X_1, X_2, \dots, X_n \overset{iid}{\sim} F(x;\theta)$, where $\theta$ are the parameters that shape the distribution function.

Then, the **joint density function** of the sample is:

$$
f(x_1, x_2, \dots, x_n; \theta) = \prod_{i=1}^n f(x_i; \theta)
$$

This expression, viewed as a function of $\theta$ (for fixed data $x_i$), is the **likelihood function**:

$$
L(\theta \mid x_1, x_2, \dots, x_n) = \prod_{i=1}^n f(x_i; \theta)
$$


## Likelihood vs. Probability

| Concept | Interpretation |
|----------|----------------|
| **Probability** | $P(X = x \mid \theta)$: $x$ is random, $\theta$ fixed |
| **Likelihood** | $L(\theta \mid x)$: $x$ fixed, $\theta$ variable |

Although they share the same mathematical form, their interpretations differ.

## Log-Likelihood Function

For convenience, we often work with the **log-likelihood**:

$$
\ell(\theta) = \log L(\theta)
$$



## Example 1: Bernoulli Random Variables

Suppose $X_1, \dots, X_n \sim \text{Bernoulli}(p)$.

$$
f(x_i; p) = p^{x_i}(1 - p)^{1 - x_i}
$$


## Example 2: Poisson Random Variables

Suppose $X_1, \dots, X_n \sim \text{Pois}(\lambda)$

$$
f(x_i; \lambda) = \frac{e^{-\lambda}\lambda^{x_i}}{x_i!}
$$

## Example 3: Normal Random Variables

Suppose $X_1, \dots, X_n \sim \text{N}(\mu, \sigma^2)$

$$
f(x_i; \lambda) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}
$$

# Optimization

## Optimization

For a given function $f(x)$, optimization is the process of finding the value of $x$ where $f(x)$ is either the maximum or minimum.

## Finding X

To find $x$,

$$
f^\prime (x) = 0
$$

## Maximum or Minimum
::: {.column}

### Maximum

$$
f^{\prime\prime} (X) <0  
$$

:::
::: {.column}

### Minimum

$$
f^{\prime\prime} (X) <0  
$$

:::


## Example

$$
f(x) = x^2 + 5x + 3
$$


## Example

$$
f(x) = -4(3x - 9)^2 + 12
$$
